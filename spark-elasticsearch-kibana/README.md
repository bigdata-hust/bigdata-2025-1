# ğŸš€ Há»‡ thá»‘ng Spark-Elasticsearch-Kibana cho Yelp Data

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c yÃªu cáº§u

```
your-project/
â”œâ”€â”€ processed_data/              â† Dá»® LIá»†U Cá»¦A Báº N (thÆ° má»¥c nÃ y cáº§n tá»“n táº¡i)
â”‚   â”œâ”€â”€ business.csv
â”‚   â”œâ”€â”€ user.csv
â”‚   â””â”€â”€ review_combined_1.csv
â”‚
â””â”€â”€ spark-elasticsearch-kibana/  â† CODE (sau khi giáº£i nÃ©n zip)
    â”œâ”€â”€ main.py                  â­ FILE CHáº Y CHÃNH
    â”œâ”€â”€ csv_data_loader.py       ğŸ“Š Load CSV data
    â”œâ”€â”€ yelp_analytics.py        ğŸ“ˆ CÃ¡c hÃ m phÃ¢n tÃ­ch
    â”œâ”€â”€ spark_elasticsearch_integration.py  ğŸ”— Káº¿t ná»‘i ES
    â”œâ”€â”€ docker-compose.yml       ğŸ³ ES & Kibana setup
    â”œâ”€â”€ requirements.txt         ğŸ“¦ Dependencies
    â”œâ”€â”€ env.example              âš™ï¸ Cáº¥u hÃ¬nh máº«u
    â””â”€â”€ README.md               ğŸ“– File nÃ y
```

**âš ï¸ QUAN TRá»ŒNG**: ThÆ° má»¥c `processed_data` pháº£i náº±m **NGANG HÃ€NG** vá»›i thÆ° má»¥c `spark-elasticsearch-kibana`

---

## ğŸ¯ Chá»©c nÄƒng chÃ­nh

### âœ… Há»‡ thá»‘ng nÃ y sáº½:

1. **Äá»c dá»¯ liá»‡u CSV** tá»« thÆ° má»¥c `processed_data`
2. **PhÃ¢n tÃ­ch 7 loáº¡i insights** tá»« Yelp data:
   - Top sáº£n pháº©m bÃ¡n cháº¡y
   - Cá»­a hÃ ng Ä‘a dáº¡ng nháº¥t
   - ÄÃ¡nh giÃ¡ tá»‘t nháº¥t
   - Review tÃ­ch cá»±c nháº¥t
   - Thá»i gian cao Ä‘iá»ƒm
   - Top categories
   - Thá»‘ng kÃª tá»•ng há»£p

3. **Export káº¿t quáº£** vÃ o Elasticsearch (7 indices)
4. **Trá»±c quan hÃ³a** trÃªn Kibana dashboards

---

## âš¡ Quick Start (5 phÃºt)

### BÆ°á»›c 1: Chuáº©n bá»‹ mÃ´i trÆ°á»ng

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c code
cd spark-elasticsearch-kibana

# CÃ i Ä‘áº·t Python packages
pip install -r requirements.txt
```

### BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng Elasticsearch & Kibana

```bash
# Khá»Ÿi Ä‘á»™ng Docker containers
docker-compose up -d

# Äá»£i ~30 giÃ¢y Ä‘á»ƒ services sáºµn sÃ ng

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker-compose ps

# Test káº¿t ná»‘i
curl http://localhost:9200
```

**URLs sau khi khá»Ÿi Ä‘á»™ng:**
- Elasticsearch: http://localhost:9200
- Kibana: http://localhost:5601

### BÆ°á»›c 3: Kiá»ƒm tra dá»¯ liá»‡u

```bash
# Äáº£m báº£o cÃ¡c file CSV tá»“n táº¡i
ls -lh ../processed_data/

# Output mong Ä‘á»£i:
# business.csv
# user.csv
# review_combined_1.csv
```

### BÆ°á»›c 4: Cháº¡y Pipeline

```bash
# Cháº¡y file chÃ­nh
python main.py
```

Pipeline sáº½ tá»± Ä‘á»™ng:
- âœ… Load CSV data
- âœ… Validate dá»¯ liá»‡u
- âœ… Cháº¡y 7 analyses
- âœ… Export sang Elasticsearch
- âœ… Hiá»ƒn thá»‹ káº¿t quáº£

**Thá»i gian dá»± kiáº¿n**: 5-15 phÃºt (tÃ¹y kÃ­ch thÆ°á»›c data)

### BÆ°á»›c 5: Táº¡o Dashboard trong Kibana

1. Má»Ÿ browser: http://localhost:5601
2. VÃ o **Stack Management** â†’ **Index Patterns**
3. Click **Create index pattern**
4. Táº¡o pattern: `yelp-top-selling*` (time field: `timestamp`)
5. Láº·p láº¡i cho 6 indices cÃ²n láº¡i
6. VÃ o **Visualize** â†’ Táº¡o visualizations
7. VÃ o **Dashboard** â†’ Táº¡o dashboard

---

## ğŸ“Š CÃ¡c Index Ä‘Æ°á»£c táº¡o

| Index Name | MÃ´ táº£ | Documents |
|-----------|-------|-----------|
| `yelp-top-selling` | Top 10 sáº£n pháº©m bÃ¡n cháº¡y gáº§n Ä‘Ã¢y | ~10 |
| `yelp-diverse-stores` | Top 10 cá»­a hÃ ng Ä‘a dáº¡ng | ~10 |
| `yelp-best-rated` | Top 10 Ä‘Ã¡nh giÃ¡ tá»‘t nháº¥t | ~10 |
| `yelp-positive-reviews` | Top 10 review tÃ­ch cá»±c | ~10 |
| `yelp-peak-hours` | Thá»‘ng kÃª theo thá»i gian | Variable |
| `yelp-top-categories` | Top 20 categories | ~20 |
| `yelp-store-stats` | Táº¥t cáº£ businesses | All |

---

## ğŸ”§ Cáº¥u hÃ¬nh

### Environment Variables (Optional)

```bash
# Copy file máº«u
cp env.example .env

# Edit cáº¥u hÃ¬nh
nano .env
```

**CÃ¡c biáº¿n quan trá»ng:**
```bash
# Elasticsearch
ES_HOST=localhost
ES_PORT=9200

# Data path
DATA_PATH=../processed_data/

# Spark resources
SPARK_DRIVER_MEMORY=4g
SPARK_EXECUTOR_MEMORY=4g
```

### Äiá»u chá»‰nh Memory (náº¿u cáº§n)

**Docker (cho Elasticsearch):**
```yaml
# docker-compose.yml
elasticsearch:
  environment:
    - "ES_JAVA_OPTS=-Xms2g -Xmx2g"  # Giáº£m náº¿u mÃ¡y yáº¿u
```

**Spark:**
```python
# Trong main.py, tÃ¬m SparkESSession.create_session()
# ThÃªm configs:
.config("spark.driver.memory", "8g")
.config("spark.executor.memory", "8g")
```

---

## ğŸ¨ VÃ­ dá»¥ Visualizations trong Kibana

### 1. Top Selling Products (Data Table)
```
Index: yelp-top-selling
Columns: name, city, recent_review_count, avg_rating
Sort: recent_review_count DESC
```

### 2. Review Trends (Line Chart)
```
Index: yelp-peak-hours
X-axis: date_string (Date)
Y-axis: review_count (Sum)
```

### 3. Top Categories (Bar Chart)
```
Index: yelp-top-categories
X-axis: category
Y-axis: total_reviews
Top: 15 categories
```

### 4. Geographic Distribution (Map)
```
Index: yelp-store-stats
Geo field: location (latitude, longitude)
Size by: actual_review_count
Color by: actual_avg_stars
```

---

## ğŸ†˜ Troubleshooting

### Lá»—i 1: "FileNotFoundError: ../processed_data/business.csv"

**NguyÃªn nhÃ¢n**: ThÆ° má»¥c data khÃ´ng Ä‘Ãºng vá»‹ trÃ­

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra cáº¥u trÃºc thÆ° má»¥c
pwd
ls ..

# NÃªn tháº¥y:
# processed_data/
# spark-elasticsearch-kibana/

# Náº¿u khÃ´ng Ä‘Ãºng, di chuyá»ƒn thÆ° má»¥c hoáº·c cáº­p nháº­t DATA_PATH
```

### Lá»—i 2: "Connection refused to Elasticsearch"

**NguyÃªn nhÃ¢n**: Elasticsearch chÆ°a cháº¡y

**Giáº£i phÃ¡p**:
```bash
docker-compose up -d
docker-compose ps  # Kiá»ƒm tra status

# Náº¿u váº«n lá»—i
docker-compose logs elasticsearch
```

### Lá»—i 3: "Spark out of memory"

**Giáº£i phÃ¡p**:
```bash
# Giáº£m partitions trong main.py
# TÃ¬m dÃ²ng: .repartition(200, ...)
# Äá»•i thÃ nh: .repartition(50, ...)

# Hoáº·c tÄƒng Spark memory
# Edit main.py, thÃªm configs
```

### Lá»—i 4: "Schema mismatch in CSV"

**NguyÃªn nhÃ¢n**: CSV format khÃ´ng Ä‘Ãºng

**Giáº£i phÃ¡p**:
```python
# Kiá»ƒm tra CSV headers
head -1 ../processed_data/business.csv

# Äáº£m báº£o cÃ³ cÃ¡c cá»™t:
# business_id, name, city, state, categories, stars, review_count, is_open, latitude, longitude
```

### Lá»—i 5: "Date parsing failed"

**Giáº£i phÃ¡p**: ÄÃ£ cÃ³ auto-detect format trong code, nhÆ°ng náº¿u váº«n lá»—i:
```python
# Edit csv_data_loader.py
# ThÃªm format date cá»§a báº¡n vÃ o coalesce()
```

---

## ğŸ“ˆ Performance Tips

### TÄƒng tá»‘c Pipeline:

1. **Reduce data scope**
```python
# Trong main.py, thÃªm filter
review_df = review_df.filter(col("review_date") >= "2023-01-01")
```

2. **Adjust partitions**
```python
# Giáº£m cho small data
.repartition(50, "business_id")

# TÄƒng cho big data
.repartition(400, "business_id")
```

3. **Cache frequently used data**
```python
# ÄÃ£ cÃ³ trong code, nhÆ°ng cÃ³ thá»ƒ thÃªm:
business_df.cache()
business_df.count()  # Trigger cache
```

4. **Limit results for testing**
```python
# Trong analysis_config
'analysis_1': {'days': 30, 'top_n': 5}  # Giáº£m tá»« 90 days, 10 results
```

---

## ğŸ“ Project Structure Chi tiáº¿t

```
spark-elasticsearch-kibana/
â”‚
â”œâ”€â”€ main.py                              â­ ENTRY POINT
â”‚   â”œâ”€â”€ Khá»Ÿi táº¡o Spark
â”‚   â”œâ”€â”€ Cáº¥u hÃ¬nh Elasticsearch
â”‚   â”œâ”€â”€ Load CSV data
â”‚   â”œâ”€â”€ Cháº¡y analyses
â”‚   â”œâ”€â”€ Export to ES
â”‚   â””â”€â”€ Hiá»ƒn thá»‹ káº¿t quáº£
â”‚
â”œâ”€â”€ csv_data_loader.py                   ğŸ“Š DATA LOADING
â”‚   â”œâ”€â”€ CSVDataLoader
â”‚   â”‚   â”œâ”€â”€ load_business_data()
â”‚   â”‚   â”œâ”€â”€ load_review_data()
â”‚   â”‚   â”œâ”€â”€ load_user_data()
â”‚   â”‚   â””â”€â”€ validate_data()
â”‚   â””â”€â”€ YelpAnalyticsPipeline
â”‚       â”œâ”€â”€ load_all_data()
â”‚       â””â”€â”€ get_dataframes()
â”‚
â”œâ”€â”€ yelp_analytics.py                    ğŸ“ˆ ANALYTICS
â”‚   â””â”€â”€ YelpAnalytics
â”‚       â”œâ”€â”€ top_selling_products_recent()
â”‚       â”œâ”€â”€ top_stores_by_product_count()
â”‚       â”œâ”€â”€ top_rated_products()
â”‚       â”œâ”€â”€ top_stores_by_positive_reviews()
â”‚       â”œâ”€â”€ get_peak_hours()
â”‚       â”œâ”€â”€ get_top_categories()
â”‚       â””â”€â”€ get_store_stats()
â”‚
â”œâ”€â”€ spark_elasticsearch_integration.py   ğŸ”— ES INTEGRATION
â”‚   â”œâ”€â”€ ElasticsearchConfig
â”‚   â”œâ”€â”€ SparkESSession
â”‚   â”œâ”€â”€ SparkToElasticsearch
â”‚   â”œâ”€â”€ ElasticsearchMappings
â”‚   â””â”€â”€ YelpElasticsearchPipeline
â”‚
â”œâ”€â”€ docker-compose.yml                   ğŸ³ SERVICES
â”‚   â”œâ”€â”€ Elasticsearch 8.11.0
â”‚   â””â”€â”€ Kibana 8.11.0
â”‚
â”œâ”€â”€ requirements.txt                     ğŸ“¦ DEPENDENCIES
â”‚   â”œâ”€â”€ pyspark>=3.4.0
â”‚   â”œâ”€â”€ elasticsearch>=8.11.0
â”‚   â””â”€â”€ pandas>=1.5.0
â”‚
â””â”€â”€ env.example                          âš™ï¸ CONFIG TEMPLATE
```

---

## ğŸ”„ Workflow

```
CSV FILES (processed_data/)
    â†“
LOAD & VALIDATE (csv_data_loader.py)
    â†“
ANALYZE (yelp_analytics.py)
    â”œâ”€â”€ Analysis 1: Top Selling
    â”œâ”€â”€ Analysis 2: Diverse Stores
    â”œâ”€â”€ Analysis 3: Best Rated
    â”œâ”€â”€ Analysis 4: Positive Reviews
    â”œâ”€â”€ Analysis 5: Peak Hours
    â”œâ”€â”€ Analysis 6: Top Categories
    â””â”€â”€ Analysis 7: Store Stats
    â†“
TRANSFORM & EXPORT (spark_elasticsearch_integration.py)
    â†“
ELASTICSEARCH (7 indices)
    â†“
KIBANA (Visualizations & Dashboards)
```

---

## âœ… Verification Checklist

Sau khi cháº¡y xong, kiá»ƒm tra:

- [ ] ThÆ° má»¥c `processed_data` Ä‘Ãºng vá»‹ trÃ­
- [ ] 3 file CSV tá»“n táº¡i vÃ  cÃ³ data
- [ ] Docker containers Ä‘ang cháº¡y
- [ ] Elasticsearch accessible (curl localhost:9200)
- [ ] Kibana accessible (curl localhost:5601)
- [ ] Pipeline cháº¡y khÃ´ng lá»—i
- [ ] 7 indices Ä‘Ã£ Ä‘Æ°á»£c táº¡o
- [ ] CÃ³ thá»ƒ query data tá»« ES
- [ ] Index patterns táº¡o Ä‘Æ°á»£c trong Kibana
- [ ] Data hiá»ƒn thá»‹ trong Discover

**Kiá»ƒm tra indices:**
```bash
curl http://localhost:9200/_cat/indices?v | grep yelp
```

**Äáº¿m documents:**
```bash
curl http://localhost:9200/yelp-top-selling/_count
```

---

## ğŸ¯ Next Steps

### 1. TÃ¹y chá»‰nh Analyses
```python
# Edit main.py, section analysis_config
analysis_config = {
    'analysis_1': {'days': 180, 'top_n': 20},  # 6 thÃ¡ng, top 20
    ...
}
```

### 2. ThÃªm Analyses má»›i
```python
# Trong yelp_analytics.py
@staticmethod
def custom_analysis(business_df, review_df):
    # Your analysis logic
    return result_df

# Trong main.py, thÃªm vÃ o run_analysis()
results['custom'] = analytics.custom_analysis(business_df, review_df)
```

### 3. Schedule Pipeline
```bash
# Crontab (Linux)
0 2 * * * cd /path/to/spark-elasticsearch-kibana && python main.py

# Task Scheduler (Windows)
# Táº¡o task cháº¡y main.py hÃ ng ngÃ y
```

### 4. Export to other formats
```python
# ThÃªm vÃ o main.py sau analyses
results['top_selling'].write.csv("output/top_selling.csv")
results['top_selling'].write.parquet("output/top_selling.parquet")
```

---

## ğŸ“š TÃ i nguyÃªn bá»• sung

- **Elasticsearch Guide**: https://www.elastic.co/guide/en/elasticsearch/reference/current/
- **Kibana Guide**: https://www.elastic.co/guide/en/kibana/current/
- **PySpark Docs**: https://spark.apache.org/docs/latest/api/python/
- **ES-Hadoop**: https://www.elastic.co/guide/en/elasticsearch/hadoop/current/

---

## ğŸ†˜ Support

Náº¿u gáº·p váº¥n Ä‘á»:

1. **Check logs**
```bash
# Elasticsearch logs
docker-compose logs elasticsearch

# Kibana logs
docker-compose logs kibana

# Python errors
# Xem trong terminal output
```

2. **Common commands**
```bash
# Restart services
docker-compose restart

# Stop services
docker-compose down

# View all containers
docker ps -a

# Check Spark UI
# http://localhost:4040 (khi pipeline Ä‘ang cháº¡y)
```

3. **Debug mode**
```python
# ThÃªm vÃ o Ä‘áº§u main.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

---

## ğŸ“ Notes

- **Data Format**: CSV vá»›i header, encoding UTF-8
- **Spark Version**: 3.4.0+
- **Elasticsearch**: 8.11.0
- **Python**: 3.8+
- **Memory**: Khuyáº¿n nghá»‹ 8GB+ RAM
- **Disk**: ~2GB cho Docker images

---

## ğŸ‰ Káº¿t luáº­n

Báº¡n Ä‘Ã£ cÃ³ má»™t há»‡ thá»‘ng hoÃ n chá»‰nh Ä‘á»ƒ:

âœ… Load dá»¯ liá»‡u Yelp tá»« CSV  
âœ… PhÃ¢n tÃ­ch insights tá»± Ä‘á»™ng  
âœ… Export sang Elasticsearch  
âœ… Visualize trÃªn Kibana  
âœ… Scale vá»›i big data  

**ChÃºc báº¡n thÃ nh cÃ´ng! ğŸš€ğŸ“Š**

---

*Version: 1.0.0*  
*Last Updated: 2025-11-02*
