#!/usr/bin/env python3
"""
Main Pipeline Runner cho Spark-Elasticsearch-Kibana
Phi√™n b·∫£n t·ªëi ∆∞u cho CSV data
"""

import sys
import os
from datetime import datetime
import time

# Import c√°c module c·∫ßn thi·∫øt
from spark_elasticsearch_integration import (
    ElasticsearchConfig,
    SparkESSession,
    SparkToElasticsearch,
    ElasticsearchMappings,
    YelpElasticsearchPipeline
)

from csv_data_loader import YelpAnalyticsPipeline
from yelp_analytics import YelpAnalytics


def print_header(title):
    """In header ƒë·∫πp"""
    print("\n" + "="*80)
    print(" " * ((80 - len(title)) // 2) + title)
    print("="*80 + "\n")


def check_elasticsearch_connection(es_config):
    """Ki·ªÉm tra k·∫øt n·ªëi Elasticsearch"""
    try:
        es_client = es_config.get_es_client()
        info = es_client.info()
        print(f"‚úì ƒê√£ k·∫øt n·ªëi Elasticsearch version {info['version']['number']}")
        return True
    except Exception as e:
        print(f"‚úó Kh√¥ng th·ªÉ k·∫øt n·ªëi Elasticsearch: {str(e)}")
        print("\nüí° H√£y ch·∫°y: docker-compose up -d")
        return False


def run_analysis(spark_pipeline, config):
    """
    Ch·∫°y c√°c ph√¢n t√≠ch Yelp
    
    Args:
        spark_pipeline: YelpAnalyticsPipeline instance
        config: Dict c·∫•u h√¨nh cho c√°c analyses
    
    Returns:
        Dict ch·ª©a results
    """
    business_df, review_df, user_df = spark_pipeline.get_dataframes()
    analytics = YelpAnalytics()
    results = {}
    
    print_header("CH·∫†Y C√ÅC PH√ÇN T√çCH YELP")
    
    # Analysis 1: Top Selling Products
    try:
        print("1Ô∏è‚É£  Analysis 1: Top Selling Products...")
        results['top_selling'] = analytics.top_selling_products_recent(
            review_df, business_df,
            days=config.get('analysis_1', {}).get('days', 90),
            top_n=config.get('analysis_1', {}).get('top_n', 10)
        )
        print("   ‚úì Completed")
    except Exception as e:
        print(f"   ‚úó Error: {str(e)}")
    
    # Analysis 2: Diverse Stores
    try:
        print("2Ô∏è‚É£  Analysis 2: Most Diverse Stores...")
        results['diverse_stores'] = analytics.top_stores_by_product_count(
            business_df,
            top_n=config.get('analysis_2', {}).get('top_n', 10)
        )
        print("   ‚úì Completed")
    except Exception as e:
        print(f"   ‚úó Error: {str(e)}")
    
    # Analysis 3: Best Rated Products
    try:
        print("3Ô∏è‚É£  Analysis 3: Best Rated Products...")
        results['best_rated'] = analytics.top_rated_products(
            business_df, review_df,
            min_reviews=config.get('analysis_3', {}).get('min_reviews', 50),
            top_n=config.get('analysis_3', {}).get('top_n', 10)
        )
        print("   ‚úì Completed")
    except Exception as e:
        print(f"   ‚úó Error: {str(e)}")
    
    # Analysis 4: Most Positive Reviews
    try:
        print("4Ô∏è‚É£  Analysis 4: Stores with Most Positive Reviews...")
        results['most_positive'] = analytics.top_stores_by_positive_reviews(
            business_df, review_df,
            positive_threshold=config.get('analysis_4', {}).get('positive_threshold', 4),
            top_n=config.get('analysis_4', {}).get('top_n', 10)
        )
        print("   ‚úì Completed")
    except Exception as e:
        print(f"   ‚úó Error: {str(e)}")
    
    # Analysis 5: Peak Hours
    try:
        print("5Ô∏è‚É£  Analysis 5: Peak Review Hours...")
        results['peak_hours'] = analytics.get_peak_hours(review_df)
        print("   ‚úì Completed")
    except Exception as e:
        print(f"   ‚úó Error: {str(e)}")
    
    # Analysis 6: Top Categories
    try:
        print("6Ô∏è‚É£  Analysis 6: Top Categories...")
        results['top_categories'] = analytics.get_top_categories(
            business_df, review_df,
            top_n=config.get('analysis_6', {}).get('top_n', 20)
        )
        print("   ‚úì Completed")
    except Exception as e:
        print(f"   ‚úó Error: {str(e)}")
    
    # Analysis 7: Store Statistics
    try:
        print("7Ô∏è‚É£  Analysis 7: Store Statistics...")
        results['store_stats'] = analytics.get_store_stats(business_df, review_df)
        print("   ‚úì Completed")
    except Exception as e:
        print(f"   ‚úó Error: {str(e)}")
    
    print(f"\n‚úì Ho√†n th√†nh {len(results)}/7 analyses")
    return results


def main():
    """Main execution function"""
    
    print_header("YELP SPARK-ELASTICSEARCH-KIBANA PIPELINE")
    print(f"B·∫Øt ƒë·∫ßu l√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    # ========================================================================
    # B∆Ø·ªöC 1: KH·ªûI T·∫†O SPARK SESSION
    # ========================================================================
    print_header("B∆Ø·ªöC 1: KH·ªûI T·∫†O SPARK SESSION")
    
    try:
        spark = SparkESSession.create_session()
        print("‚úì Spark Session ƒë√£ s·∫µn s√†ng")
    except Exception as e:
        print(f"‚úó L·ªói kh·ªüi t·∫°o Spark: {str(e)}")
        return False
    
    # ========================================================================
    # B∆Ø·ªöC 2: C·∫§U H√åNH ELASTICSEARCH
    # ========================================================================
    print_header("B∆Ø·ªöC 2: C·∫§U H√åNH ELASTICSEARCH")
    
    # ƒê·ªçc config t·ª´ environment variables ho·∫∑c s·ª≠ d·ª•ng default
    es_host = os.getenv('ES_HOST', 'localhost')
    es_port = os.getenv('ES_PORT', '9200')
    es_username = os.getenv('ES_USERNAME', None)
    es_password = os.getenv('ES_PASSWORD', None)
    
    es_config = ElasticsearchConfig(
        hosts=[f'{es_host}:{es_port}'],
        username=es_username,
        password=es_password,
        use_ssl=False,
        verify_certs=False
    )
    
    if not check_elasticsearch_connection(es_config):
        print("\n‚ö†Ô∏è  Elasticsearch ch∆∞a s·∫µn s√†ng. H√£y kh·ªüi ƒë·ªông tr∆∞·ªõc:")
        print("   docker-compose up -d")
        print("\nB·∫°n c√≥ mu·ªën ti·∫øp t·ª•c kh√¥ng? (S·∫Ω ch·ªâ ch·∫°y ph√¢n t√≠ch, kh√¥ng export ES)")
        response = input("Ti·∫øp t·ª•c? (y/n): ").strip().lower()
        if response != 'y':
            return False
        skip_es_export = True
    else:
        skip_es_export = False
    
    # ========================================================================
    # B∆Ø·ªöC 3: LOAD D·ªÆ LI·ªÜU T·ª™ CSV
    # ========================================================================
    print_header("B∆Ø·ªöC 3: LOAD D·ªÆ LI·ªÜU T·ª™ CSV")
    
    data_path = os.getenv('DATA_PATH', '../processed_data/')
    print(f"Data path: {data_path}")
    
    spark_pipeline = YelpAnalyticsPipeline(spark, data_path)
    
    if not spark_pipeline.load_all_data(validate=True):
        print("‚úó Load data th·∫•t b·∫°i!")
        return False
    
    print("‚úì ƒê√£ load t·∫•t c·∫£ d·ªØ li·ªáu th√†nh c√¥ng")
    
    # ========================================================================
    # B∆Ø·ªöC 4: CH·∫†Y C√ÅC PH√ÇN T√çCH
    # ========================================================================
    
    # C·∫•u h√¨nh cho c√°c analyses
    analysis_config = {
        'analysis_1': {'days': 90, 'top_n': 10},
        'analysis_2': {'top_n': 10},
        'analysis_3': {'min_reviews': 50, 'top_n': 10},
        'analysis_4': {'positive_threshold': 4, 'top_n': 10},
        'analysis_6': {'top_n': 20}
    }
    
    results = run_analysis(spark_pipeline, analysis_config)
    
    if not results:
        print("‚úó Kh√¥ng c√≥ k·∫øt qu·∫£ ph√¢n t√≠ch n√†o!")
        return False
    
    # ========================================================================
    # B∆Ø·ªöC 5: EXPORT SANG ELASTICSEARCH
    # ========================================================================
    
    if not skip_es_export:
        print_header("B∆Ø·ªöC 5: EXPORT SANG ELASTICSEARCH")
        
        try:
            # T·∫°o YelpElasticsearchPipeline wrapper
            class ResultsWrapper:
                """Wrapper ƒë·ªÉ c√≥ interface t∆∞∆°ng th√≠ch"""
                def __init__(self, results):
                    self.results = results
            
            wrapper = ResultsWrapper(results)
            
            es_pipeline = YelpElasticsearchPipeline(spark, es_config, wrapper)
            es_pipeline.export_all_analyses()
            
            print("‚úì Export sang Elasticsearch ho√†n t·∫•t!")
            
        except Exception as e:
            print(f"‚úó L·ªói khi export: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # ========================================================================
    # B∆Ø·ªöC 6: HI·ªÇN TH·ªä K·∫æT QU·∫¢
    # ========================================================================
    print_header("B∆Ø·ªöC 6: K·∫æT QU·∫¢ PH√ÇN T√çCH")
    
    for key, df in results.items():
        print(f"\n{key}:")
        try:
            df.show(5, truncate=False)
        except:
            print(f"  ƒê√£ l∆∞u k·∫øt qu·∫£ (count: {df.count()})")
    
    # ========================================================================
    # HO√ÄN T·∫§T
    # ========================================================================
    
    elapsed_time = time.time() - start_time
    minutes = int(elapsed_time // 60)
    seconds = int(elapsed_time % 60)
    
    print_header("HO√ÄN T·∫§T!")
    print(f"T·ªïng th·ªùi gian: {minutes} ph√∫t {seconds} gi√¢y")
    print(f"K·∫øt th√∫c l√∫c: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if not skip_es_export:
        print("\nüìä B∆∞·ªõc ti·∫øp theo:")
        print("  1. M·ªü Kibana: http://localhost:5601")
        print("  2. V√†o Stack Management > Index Patterns")
        print("  3. T·∫°o index patterns cho c√°c index sau:")
        print("     - yelp-top-selling*")
        print("     - yelp-diverse-stores*")
        print("     - yelp-best-rated*")
        print("     - yelp-positive-reviews*")
        print("     - yelp-peak-hours*")
        print("     - yelp-top-categories*")
        print("     - yelp-store-stats*")
        print("  4. T·∫°o Visualizations v√† Dashboards")
    
    # Cleanup
    print("\nüõë D·ª´ng Spark Session...")
    spark.stop()
    
    return True


if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Pipeline b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚úó L·ªói kh√¥ng mong ƒë·ª£i: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
