"""
Script t·∫°o Jupyter Notebook - Sampling + Multi-file Strategy
X·ª≠ l√Ω 6M records m√† kh√¥ng tr√†n RAM
"""

import json

notebook_content = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Yelp Dataset Processing - Sampling + Multi-file Strategy\n",
                "## X·ª≠ l√Ω dataset l·ªõn kh√¥ng tr√†n RAM\n",
                "\n",
                "**Chi·∫øn l∆∞·ª£c:**\n",
                "- Sample 40% t·ª´ m·ªói dataset\n",
                "- X·ª≠ l√Ω theo batch 100k records\n",
                "- Output: Multiple small files\n",
                "- RAM usage: < 2GB\n",
                "\n",
                "**Expected output:**\n",
                "- ~2.4M reviews (from 6M)\n",
                "- ~24 train files + 24 test files\n",
                "- Each file ~80-100k records"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 0. C√†i ƒë·∫∑t th∆∞ vi·ªán"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install tqdm -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from datetime import datetime\n",
                "import os\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "import psutil\n",
                "import time\n",
                "import gc\n",
                "\n",
                "print(\"‚úÖ Libraries imported!\")\n",
                "print(f\"üìÖ Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. C·∫•u h√¨nh"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚öôÔ∏è C·∫§U H√åNH - THAY ƒê·ªîI ƒê∆Ø·ªúNG D·∫™N\n",
                "DATA_PATH = \"yelp_dataset/\"\n",
                "\n",
                "FILE_PATHS = {\n",
                "    'business': DATA_PATH + 'yelp_academic_dataset_business.json',\n",
                "    'review': DATA_PATH + 'yelp_academic_dataset_review.json',\n",
                "    'user': DATA_PATH + 'yelp_academic_dataset_user.json'\n",
                "}\n",
                "\n",
                "OUTPUT_DIR = \"processed_data/\"\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "\n",
                "# Processing settings\n",
                "BATCH_SIZE = 100000          # ƒê·ªçc 100k records/batch\n",
                "SAMPLE_RATE = 0.4            # L·∫•y 40% t·ª´ m·ªói batch\n",
                "COMBINE_BATCHES = 5          # G·ªôp 5 batch th√†nh 1 file output\n",
                "RECORDS_PER_FILE = 100000    # ~100k records/file output\n",
                "\n",
                "# Train/Test split\n",
                "TRAIN_RATIO = 0.8\n",
                "TEST_RATIO = 0.2\n",
                "RANDOM_STATE = 42\n",
                "\n",
                "print(f\"üìÇ Data path: {DATA_PATH}\")\n",
                "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
                "print(f\"üìä Batch size: {BATCH_SIZE:,}\")\n",
                "print(f\"üé≤ Sample rate: {SAMPLE_RATE:.0%}\")\n",
                "print(f\"üì¶ Records per output file: ~{RECORDS_PER_FILE:,}\")\n",
                "print(f\"‚úÇÔ∏è Train/Test: {TRAIN_RATIO:.0%}/{TEST_RATIO:.0%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_memory_mb():\n",
                "    \"\"\"Get current memory usage in MB\"\"\"\n",
                "    return psutil.Process().memory_info().rss / 1024 / 1024\n",
                "\n",
                "def count_lines(filepath):\n",
                "    \"\"\"Count total lines\"\"\"\n",
                "    print(f\"‚è≥ Counting lines in {filepath.split('/')[-1]}...\")\n",
                "    with open(filepath, 'r', encoding='utf-8') as f:\n",
                "        count = sum(1 for _ in f)\n",
                "    print(f\"   Total: {count:,} lines\")\n",
                "    return count\n",
                "\n",
                "def load_and_sample_batch(filepath, batch_size, sample_rate):\n",
                "    \"\"\"\n",
                "    Load JSON file in batches and sample immediately\n",
                "    Yields sampled DataFrames\n",
                "    \"\"\"\n",
                "    total_lines = count_lines(filepath)\n",
                "    filename = filepath.split('/')[-1]\n",
                "    \n",
                "    batch_data = []\n",
                "    processed = 0\n",
                "    errors = 0\n",
                "    \n",
                "    with open(filepath, 'r', encoding='utf-8') as f:\n",
                "        pbar = tqdm(total=total_lines, desc=f\"Processing {filename}\", unit=\" lines\")\n",
                "        \n",
                "        for line in f:\n",
                "            line = line.strip()\n",
                "            if not line:\n",
                "                pbar.update(1)\n",
                "                continue\n",
                "            \n",
                "            try:\n",
                "                obj = json.loads(line)\n",
                "                batch_data.append(obj)\n",
                "                processed += 1\n",
                "                \n",
                "                # When batch is full, sample and yield\n",
                "                if len(batch_data) >= batch_size:\n",
                "                    df_batch = pd.DataFrame(batch_data)\n",
                "                    \n",
                "                    # Random sample\n",
                "                    sample_size = int(len(df_batch) * sample_rate)\n",
                "                    df_sampled = df_batch.sample(n=sample_size, random_state=RANDOM_STATE)\n",
                "                    \n",
                "                    yield df_sampled\n",
                "                    \n",
                "                    # Clear memory\n",
                "                    batch_data = []\n",
                "                    del df_batch\n",
                "                    gc.collect()\n",
                "                    \n",
                "                    pbar.set_postfix({\n",
                "                        'RAM': f'{get_memory_mb():.0f}MB',\n",
                "                        'Sampled': f'{sample_size:,}'\n",
                "                    })\n",
                "            \n",
                "            except json.JSONDecodeError:\n",
                "                errors += 1\n",
                "            \n",
                "            pbar.update(1)\n",
                "        \n",
                "        # Process remaining data\n",
                "        if batch_data:\n",
                "            df_batch = pd.DataFrame(batch_data)\n",
                "            sample_size = int(len(df_batch) * sample_rate)\n",
                "            if sample_size > 0:\n",
                "                df_sampled = df_batch.sample(n=sample_size, random_state=RANDOM_STATE)\n",
                "                yield df_sampled\n",
                "        \n",
                "        pbar.close()\n",
                "    \n",
                "    print(f\"‚úÖ Processed {processed:,} records ({errors} errors)\")\n",
                "    print(f\"üíæ Peak RAM: {get_memory_mb():.0f} MB\\n\")\n",
                "\n",
                "print(\"‚úÖ Helper functions ready!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Process Business Data (40% Sample)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"üè¢ PROCESSING BUSINESS DATA (40% SAMPLE)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "output_file = OUTPUT_DIR + 'business.csv'\n",
                "first_write = True\n",
                "total_records = 0\n",
                "\n",
                "for sampled_df in load_and_sample_batch(\n",
                "    FILE_PATHS['business'], \n",
                "    BATCH_SIZE, \n",
                "    SAMPLE_RATE\n",
                "):\n",
                "    # Append to file\n",
                "    sampled_df.to_csv(output_file, mode='a', header=first_write, index=False)\n",
                "    first_write = False\n",
                "    total_records += len(sampled_df)\n",
                "    \n",
                "    del sampled_df\n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\n‚úÖ Business sampled: {total_records:,} records\")\n",
                "print(f\"üìÅ Saved to: {output_file}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Process User Data (40% Sample)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"üë• PROCESSING USER DATA (40% SAMPLE)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "output_file = OUTPUT_DIR + 'user.csv'\n",
                "first_write = True\n",
                "total_records = 0\n",
                "\n",
                "for sampled_df in load_and_sample_batch(\n",
                "    FILE_PATHS['user'], \n",
                "    BATCH_SIZE, \n",
                "    SAMPLE_RATE\n",
                "):\n",
                "    # Map yelping_since to since\n",
                "    if 'yelping_since' in sampled_df.columns:\n",
                "        sampled_df['since'] = sampled_df['yelping_since']\n",
                "    \n",
                "    # Select required columns\n",
                "    user_cols = ['user_id', 'name', 'review_count', 'since', 'useful', 'fans', 'average_stars']\n",
                "    available_cols = [col for col in user_cols if col in sampled_df.columns]\n",
                "    sampled_df = sampled_df[available_cols]\n",
                "    \n",
                "    # Append to file\n",
                "    sampled_df.to_csv(output_file, mode='a', header=first_write, index=False)\n",
                "    first_write = False\n",
                "    total_records += len(sampled_df)\n",
                "    \n",
                "    del sampled_df\n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\n‚úÖ User sampled: {total_records:,} records\")\n",
                "print(f\"üìÅ Saved to: {output_file}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Process Review Data (40% Sample + Multi-file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"üìù PROCESSING REVIEW DATA (40% SAMPLE + MULTI-FILE)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "accumulated_data = []\n",
                "file_counter = 1\n",
                "total_sampled = 0\n",
                "null_removed = 0\n",
                "\n",
                "for sampled_df in load_and_sample_batch(\n",
                "    FILE_PATHS['review'], \n",
                "    BATCH_SIZE, \n",
                "    SAMPLE_RATE\n",
                "):\n",
                "    # Create sentiment labels\n",
                "    sampled_df['label'] = sampled_df['stars'].apply(\n",
                "        lambda x: 0 if x <= 2 else (2 if x == 3 else 1)\n",
                "    )\n",
                "    \n",
                "    # Remove null text\n",
                "    before = len(sampled_df)\n",
                "    sampled_df = sampled_df.dropna(subset=['text'])\n",
                "    null_removed += (before - len(sampled_df))\n",
                "    \n",
                "    # Select columns\n",
                "    sampled_df = sampled_df[['text', 'label']]\n",
                "    sampled_df.columns = ['review', 'label']\n",
                "    \n",
                "    # Accumulate data\n",
                "    accumulated_data.append(sampled_df)\n",
                "    total_sampled += len(sampled_df)\n",
                "    \n",
                "    # Check if we have enough data for a file\n",
                "    total_accumulated = sum(len(df) for df in accumulated_data)\n",
                "    \n",
                "    if total_accumulated >= RECORDS_PER_FILE:\n",
                "        # Combine accumulated batches\n",
                "        combined_df = pd.concat(accumulated_data, ignore_index=True)\n",
                "        \n",
                "        # Remove duplicates within this file\n",
                "        before_dup = len(combined_df)\n",
                "        combined_df = combined_df.drop_duplicates(subset=['review'])\n",
                "        dup_removed = before_dup - len(combined_df)\n",
                "        \n",
                "        print(f\"\\nüì¶ Creating file #{file_counter}:\")\n",
                "        print(f\"   Records: {len(combined_df):,}\")\n",
                "        print(f\"   Duplicates removed: {dup_removed:,}\")\n",
                "        \n",
                "        # Save combined file\n",
                "        review_file = OUTPUT_DIR + f'review_combined_{file_counter}.csv'\n",
                "        combined_df.to_csv(review_file, index=False)\n",
                "        print(f\"   ‚úÖ Saved: {review_file}\")\n",
                "        \n",
                "        # Clear memory\n",
                "        del combined_df\n",
                "        accumulated_data = []\n",
                "        gc.collect()\n",
                "        \n",
                "        file_counter += 1\n",
                "    \n",
                "    del sampled_df\n",
                "    gc.collect()\n",
                "\n",
                "# Process remaining data\n",
                "if accumulated_data:\n",
                "    combined_df = pd.concat(accumulated_data, ignore_index=True)\n",
                "    before_dup = len(combined_df)\n",
                "    combined_df = combined_df.drop_duplicates(subset=['review'])\n",
                "    dup_removed = before_dup - len(combined_df)\n",
                "    \n",
                "    print(f\"\\nüì¶ Creating final file #{file_counter}:\")\n",
                "    print(f\"   Records: {len(combined_df):,}\")\n",
                "    print(f\"   Duplicates removed: {dup_removed:,}\")\n",
                "    \n",
                "    review_file = OUTPUT_DIR + f'review_combined_{file_counter}.csv'\n",
                "    combined_df.to_csv(review_file, index=False)\n",
                "    print(f\"   ‚úÖ Saved: {review_file}\")\n",
                "    \n",
                "    del combined_df\n",
                "    gc.collect()\n",
                "\n",
                "num_review_files = file_counter\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"‚úÖ Review processing complete!\")\n",
                "print(f\"   Total sampled: {total_sampled:,} records\")\n",
                "print(f\"   Null reviews removed: {null_removed:,}\")\n",
                "print(f\"   Output files: {num_review_files} files\")\n",
                "print(f\"{'='*80}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train/Test Split cho t·ª´ng file"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"‚úÇÔ∏è TRAIN/TEST SPLIT (STRATIFIED)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "total_train = 0\n",
                "total_test = 0\n",
                "\n",
                "for i in range(1, num_review_files + 1):\n",
                "    review_file = OUTPUT_DIR + f'review_combined_{i}.csv'\n",
                "    \n",
                "    print(f\"\\nüìÇ Processing file {i}/{num_review_files}...\")\n",
                "    \n",
                "    # Load file\n",
                "    df = pd.read_csv(review_file)\n",
                "    print(f\"   Loaded: {len(df):,} records\")\n",
                "    \n",
                "    # Split with stratify\n",
                "    train_df, test_df = train_test_split(\n",
                "        df,\n",
                "        test_size=TEST_RATIO,\n",
                "        random_state=RANDOM_STATE,\n",
                "        stratify=df['label']\n",
                "    )\n",
                "    \n",
                "    # Save train\n",
                "    train_file = OUTPUT_DIR + f'train_part{i}.csv'\n",
                "    train_df.to_csv(train_file, index=False)\n",
                "    print(f\"   ‚úÖ Train: {len(train_df):,} ‚Üí {train_file}\")\n",
                "    \n",
                "    # Save test\n",
                "    test_file = OUTPUT_DIR + f'test_part{i}.csv'\n",
                "    test_df.to_csv(test_file, index=False)\n",
                "    print(f\"   ‚úÖ Test: {len(test_df):,} ‚Üí {test_file}\")\n",
                "    \n",
                "    total_train += len(train_df)\n",
                "    total_test += len(test_df)\n",
                "    \n",
                "    # Clean memory\n",
                "    del df, train_df, test_df\n",
                "    gc.collect()\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"‚úÖ Split complete!\")\n",
                "print(f\"   Total train: {total_train:,} records\")\n",
                "print(f\"   Total test: {total_test:,} records\")\n",
                "print(f\"   Train files: {num_review_files}\")\n",
                "print(f\"   Test files: {num_review_files}\")\n",
                "print(f\"{'='*80}\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Summary Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"üìä FINAL SUMMARY REPORT\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# List all files\n",
                "all_files = os.listdir(OUTPUT_DIR)\n",
                "business_files = [f for f in all_files if f.startswith('business')]\n",
                "user_files = [f for f in all_files if f.startswith('user')]\n",
                "review_files = [f for f in all_files if f.startswith('review_combined')]\n",
                "train_files = [f for f in all_files if f.startswith('train_part')]\n",
                "test_files = [f for f in all_files if f.startswith('test_part')]\n",
                "\n",
                "print(f\"\\nüìÅ Output Files:\")\n",
                "print(f\"   Business: {len(business_files)} file(s)\")\n",
                "print(f\"   User: {len(user_files)} file(s)\")\n",
                "print(f\"   Review combined: {len(review_files)} file(s)\")\n",
                "print(f\"   Train: {len(train_files)} file(s)\")\n",
                "print(f\"   Test: {len(test_files)} file(s)\")\n",
                "\n",
                "# Calculate total size\n",
                "total_size = 0\n",
                "for f in all_files:\n",
                "    filepath = OUTPUT_DIR + f\n",
                "    if os.path.isfile(filepath):\n",
                "        total_size += os.path.getsize(filepath)\n",
                "\n",
                "print(f\"\\nüíæ Total output size: {total_size / 1024 / 1024:.1f} MB\")\n",
                "\n",
                "print(f\"\\nüìä Record Counts:\")\n",
                "print(f\"   Business: {pd.read_csv(OUTPUT_DIR + 'business.csv').shape[0]:,}\")\n",
                "print(f\"   User: {pd.read_csv(OUTPUT_DIR + 'user.csv').shape[0]:,}\")\n",
                "print(f\"   Train (total): {total_train:,}\")\n",
                "print(f\"   Test (total): {total_test:,}\")\n",
                "\n",
                "print(f\"\\n‚öôÔ∏è Processing Settings:\")\n",
                "print(f\"   Sample rate: {SAMPLE_RATE:.0%}\")\n",
                "print(f\"   Batch size: {BATCH_SIZE:,}\")\n",
                "print(f\"   Records per file: ~{RECORDS_PER_FILE:,}\")\n",
                "\n",
                "print(f\"\\nüíæ Peak RAM usage: {get_memory_mb():.0f} MB\")\n",
                "print(f\"üìÖ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"üéâ PROCESSING COMPLETE!\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüìö How to use:\")\n",
                "print(f\"   1. Train files: train_part1.csv, train_part2.csv, ...\")\n",
                "print(f\"   2. Test files: test_part1.csv, test_part2.csv, ...\")\n",
                "print(f\"   3. Load multiple files in training loop\")\n",
                "print(f\"   4. Labels: 0=Ti√™u c·ª±c, 1=T√≠ch c·ª±c, 2=Trung l·∫≠p\")\n",
                "\n",
                "# Save summary to file\n",
                "summary_file = OUTPUT_DIR + 'processing_summary.txt'\n",
                "with open(summary_file, 'w', encoding='utf-8') as f:\n",
                "    f.write(\"YELP DATASET PROCESSING SUMMARY\\n\")\n",
                "    f.write(\"=\"*80 + \"\\n\\n\")\n",
                "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
                "    f.write(f\"Business files: {len(business_files)}\\n\")\n",
                "    f.write(f\"User files: {len(user_files)}\\n\")\n",
                "    f.write(f\"Review combined files: {len(review_files)}\\n\")\n",
                "    f.write(f\"Train files: {len(train_files)}\\n\")\n",
                "    f.write(f\"Test files: {len(test_files)}\\n\\n\")\n",
                "    f.write(f\"Total train records: {total_train:,}\\n\")\n",
                "    f.write(f\"Total test records: {total_test:,}\\n\\n\")\n",
                "    f.write(f\"Sample rate: {SAMPLE_RATE:.0%}\\n\")\n",
                "    f.write(f\"Total size: {total_size / 1024 / 1024:.1f} MB\\n\")\n",
                "\n",
                "print(f\"\\n‚úÖ Summary saved to: {summary_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Quick Preview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview first train file\n",
                "print(\"üìã PREVIEW: train_part1.csv\\n\")\n",
                "df_preview = pd.read_csv(OUTPUT_DIR + 'train_part1.csv')\n",
                "print(df_preview.head(10))\n",
                "\n",
                "print(f\"\\nüìä Label distribution in train_part1:\")\n",
                "print(df_preview['label'].value_counts().sort_index())"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Ghi file
output_filename = "yelp_processing_final.ipynb"

with open(output_filename, 'w', encoding='utf-8') as f:
    json.dump(notebook_content, f, indent=1, ensure_ascii=False)

print(f"‚úÖ ƒê√£ t·∫°o file: {output_filename}")
print(f"\n‚öôÔ∏è C·∫•u h√¨nh:")
print(f"   - Sample rate: 40%")
print(f"   - Batch size: 100,000 records")
print(f"   - Output file size: ~100,000 records/file")
print(f"   - Expected: ~24 train files + 24 test files")
print(f"   - Total data: ~2.4M records")
print(f"   - RAM usage: < 2GB")
print(f"\nüöÄ Ch·∫°y:")
print(f"   python create_notebook.py")
print(f"   jupyter notebook {output_filename}")