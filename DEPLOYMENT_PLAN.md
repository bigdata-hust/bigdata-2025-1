# ğŸ“‹ PHÆ¯Æ NG ÃN TRIá»‚N KHAI NHÃNH HAI - LOCAL EXECUTION

## ğŸ” PHÃ‚N TÃCH KIáº¾N TRÃšC HIá»†N Táº I

### Luá»“ng dá»¯ liá»‡u cá»§a nhÃ¡nh HAI:
```
Data Files (JSON)
    â†“
Kafka Producer â†’ Kafka Topics (business, review, user)
    â†“
PySpark Streaming Consumer â†’ 7 Analytics Functions
    â†“
Output: HDFS + Elasticsearch + MongoDB
    â†“
Visualization: Kibana + Web Dashboard
```

### Äiá»ƒm máº¡nh:
âœ… Real-time streaming architecture
âœ… Production-ready vá»›i fault tolerance
âœ… Scalable vá»›i HDFS distributed storage
âœ… Full observability vá»›i Kibana

### ThÃ¡ch thá»©c khi cháº¡y local:
âŒ YÃªu cáº§u 7+ Docker containers (Kafka, Zookeeper, HDFS namenode/datanode, Elasticsearch, Kibana, Producer, Spark)
âŒ Tá»‘n ~8-16GB RAM
âŒ Setup phá»©c táº¡p (~30 phÃºt)
âŒ Code hiá»‡n táº¡i dÃ¹ng `readStream` â†’ khÃ´ng thá»ƒ cháº¡y trá»±c tiáº¿p vá»›i file local

---

## ğŸ¯ 2 PHÆ¯Æ NG ÃN TRIá»‚N KHAI

---

## â­ PHÆ¯Æ NG ÃN 1: SIMPLIFIED BATCH MODE (KHUYáº¾N NGHá»Š)

### MÃ´ táº£:
Chuyá»ƒn Ä‘á»•i code tá»« **Streaming** sang **Batch processing** Ä‘á»ƒ cháº¡y nhanh trÃªn local vá»›i CSV/JSON files.

### Kiáº¿n trÃºc Ä‘Æ¡n giáº£n hÃ³a:
```
Local Data Files (CSV/JSON)
    â†“
PySpark Batch Processing â†’ 7 Analytics
    â†“
Output Console + Elasticsearch (optional)
    â†“
Kibana Visualization (optional)
```

### YÃªu cáº§u tÃ i nguyÃªn:
- **RAM**: 4-8GB
- **Services**:
  - Báº¯t buá»™c: Python + PySpark
  - TÃ¹y chá»n: Elasticsearch + Kibana (chá»‰ khi cáº§n visualize)

### Thay Ä‘á»•i code cáº§n thiáº¿t:

#### 1. Sá»­a file `load_data.py`:
```python
# BEFORE (Streaming):
business_df = spark.readStream.format("kafka")...

# AFTER (Batch):
business_df = spark.read.json("data/business.json", schema=...)
```

#### 2. Sá»­a file `pipeline_orchestration.py`:
```python
# BEFORE: writeStream
df.writeStream.format("parquet")...

# AFTER: write
df.write.mode("overwrite").parquet("output/...")
```

#### 3. Bá» watermark vÃ  streaming configs:
```python
# REMOVE:
.withWatermark('business_ts', '10 minutes')
.awaitAnyTermination()
```

### CÃ¡c bÆ°á»›c thá»±c hiá»‡n:

#### BÆ°á»›c 1: Chuáº©n bá»‹ mÃ´i trÆ°á»ng
```bash
# Clone hoáº·c Ä‘Ã£ cÃ³ repository
cd bigdata-2025-1

# Táº¡o virtual environment
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c: venv\Scripts\activate  # Windows

# CÃ i Ä‘áº·t dependencies
pip install pyspark==4.0.1 requests
```

#### BÆ°á»›c 2: Chuáº©n bá»‹ dá»¯ liá»‡u
```bash
# Táº¡o thÆ° má»¥c data
mkdir -p data

# Copy hoáº·c download data files vÃ o thÆ° má»¥c:
# - data/business.json
# - data/review.json
# - data/user.json (optional)
```

#### BÆ°á»›c 3: Táº¡o phiÃªn báº£n Batch cá»§a code
```bash
# Táº¡o thÆ° má»¥c má»›i cho batch version
mkdir -p Spark/batch_mode

# Copy cÃ¡c file vÃ  modify:
# - batch_load_data.py (version batch cá»§a load_data.py)
# - batch_pipeline.py (version batch cá»§a pipeline_orchestration.py)
# - batch_main.py (version batch cá»§a main.py)
# - analytics_yelp.py (giá»¯ nguyÃªn)
# - configuration.py (bá» streaming configs)
```

#### BÆ°á»›c 4: Cháº¡y pipeline
```bash
cd Spark/batch_mode
python batch_main.py
```

#### BÆ°á»›c 5: Xem káº¿t quáº£
```bash
# Káº¿t quáº£ sáº½ hiá»ƒn thá»‹ trÃªn console
# Hoáº·c Ä‘Æ°á»£c lÆ°u vÃ o output/
ls -la output/
```

### Æ¯u Ä‘iá»ƒm:
âœ… ÄÆ¡n giáº£n, chá»‰ cáº§n Python + PySpark
âœ… Cháº¡y nhanh (~5-10 phÃºt vá»›i small dataset)
âœ… KhÃ´ng cáº§n Docker
âœ… Dá»… debug
âœ… PhÃ¹ há»£p cho development & testing

### NhÆ°á»£c Ä‘iá»ƒm:
âŒ Máº¥t tÃ­nh nÄƒng real-time
âŒ KhÃ´ng giá»‘ng production
âŒ Cáº§n modify code (nhÆ°ng khÃ´ng lá»›n)

---

## ğŸš€ PHÆ¯Æ NG ÃN 2: FULL STACK DOCKER (PRODUCTION-LIKE)

### MÃ´ táº£:
Cháº¡y Ä‘Ãºng nhÆ° production vá»›i táº¥t cáº£ services trong Docker.

### Kiáº¿n trÃºc:
```
Docker Compose Stack:
â”œâ”€â”€ Zookeeper
â”œâ”€â”€ Kafka
â”œâ”€â”€ Kafka Producer (streaming data)
â”œâ”€â”€ HDFS Namenode
â”œâ”€â”€ HDFS Datanode
â”œâ”€â”€ Elasticsearch
â”œâ”€â”€ Kibana
â””â”€â”€ Spark Container (processing)
```

### YÃªu cáº§u tÃ i nguyÃªn:
- **RAM**: 12-16GB
- **Disk**: ~5GB cho Docker images
- **CPU**: 4+ cores khuyáº¿n nghá»‹

### YÃªu cáº§u cáº§n cÃ³:
âœ… Docker & Docker Compose Ä‘Ã£ cÃ i
âœ… Data files (business.json, review.json, user.json)
âœ… Äá»§ RAM

### CÃ¡c bÆ°á»›c thá»±c hiá»‡n:

#### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u
```bash
cd bigdata-2025-1

# Táº¡o thÆ° má»¥c data náº¿u chÆ°a cÃ³
mkdir -p data

# Copy data files vÃ o:
# - data/business.json
# - data/review.json
# - data/user.json
```

#### BÆ°á»›c 2: Kiá»ƒm tra docker-compose.yml
```bash
# File Ä‘Ã£ cÃ³ sáºµn trong nhÃ¡nh HAI
cat docker-compose.yml

# Äáº£m báº£o volume mapping Ä‘Ãºng:
# - ./data:/app/data  (cho kafka-producer)
```

#### BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng stack
```bash
# Start táº¥t cáº£ services
docker-compose up -d

# Äá»£i ~2-3 phÃºt cho services khá»Ÿi Ä‘á»™ng
# Kiá»ƒm tra status
docker-compose ps

# Expected output: 8 services running
```

#### BÆ°á»›c 4: Verify services
```bash
# Kiá»ƒm tra Kafka
docker exec -it $(docker-compose ps -q kafka) \
  kafka-topics.sh --list --bootstrap-server localhost:9092

# Kiá»ƒm tra Elasticsearch
curl http://localhost:9200

# Kiá»ƒm tra Kibana
curl http://localhost:5601

# Kiá»ƒm tra HDFS
curl http://localhost:9870
```

#### BÆ°á»›c 5: Monitor Kafka Producer
```bash
# Xem logs cá»§a producer (data Ä‘Æ°á»£c stream vÃ o Kafka)
docker-compose logs -f kafka-producer

# Sáº½ tháº¥y: "ğŸ“¦ Sent 100 records to topic 'business'"
```

#### BÆ°á»›c 6: Cháº¡y Spark pipeline
```bash
# Pipeline tá»± Ä‘á»™ng cháº¡y trong spark container
docker-compose logs -f spark

# Hoáº·c trigger manually:
docker exec -it $(docker-compose ps -q spark) \
  spark-submit /app/Spark/main.py
```

#### BÆ°á»›c 7: Xem káº¿t quáº£ trong Elasticsearch
```bash
# Liá»‡t kÃª cÃ¡c indices
curl http://localhost:9200/_cat/indices?v

# Expected: top_selling, diverse_stores, best_rated, most_positive,
#           peak_hours, top_categories, store_stats

# Xem data trong má»™t index
curl http://localhost:9200/top_selling/_search?size=10&pretty
```

#### BÆ°á»›c 8: Visualize trong Kibana
```bash
# Má»Ÿ browser
open http://localhost:5601

# Táº¡o Index Patterns:
# - Go to: Stack Management â†’ Index Patterns
# - Create pattern: top_selling*
# - Repeat cho 6 indices khÃ¡c

# Táº¡o Visualizations & Dashboard
```

#### BÆ°á»›c 9: Kiá»ƒm tra HDFS
```bash
# List files in HDFS
docker exec -it hdfs-namenode \
  hdfs dfs -ls /test_01/

# Expected: 7 directories (má»™t cho má»—i analysis)
```

### Æ¯u Ä‘iá»ƒm:
âœ… Giá»‘ng production 100%
âœ… Real-time streaming
âœ… Full observability
âœ… Scalable
âœ… KhÃ´ng cáº§n modify code

### NhÆ°á»£c Ä‘iá»ƒm:
âŒ Tá»‘n tÃ i nguyÃªn lá»›n
âŒ Setup phá»©c táº¡p
âŒ Debug khÃ³ hÆ¡n
âŒ Cháº¡y lÃ¢u (~30 phÃºt setup + run)

---

## ğŸ“Š SO SÃNH 2 PHÆ¯Æ NG ÃN

| TiÃªu chÃ­ | PhÆ°Æ¡ng Ã¡n 1 (Batch) | PhÆ°Æ¡ng Ã¡n 2 (Docker) |
|----------|---------------------|----------------------|
| **RAM cáº§n** | 4-8GB | 12-16GB |
| **Setup time** | 5 phÃºt | 30 phÃºt |
| **Äá»™ phá»©c táº¡p** | Tháº¥p â­ | Cao â­â­â­ |
| **Modify code** | CÃ³ (nhá») | KhÃ´ng |
| **Real-time** | âŒ | âœ… |
| **Giá»‘ng production** | 40% | 100% |
| **Debug** | Dá»… | KhÃ³ |
| **PhÃ¹ há»£p cho** | Dev, Test, Demo | Production, Full test |

---

## ğŸ¯ KHUYáº¾N NGHá»Š

### Chá»n PhÆ°Æ¡ng Ã¡n 1 (Batch) náº¿u:
- âœ… Chá»‰ muá»‘n xem káº¿t quáº£ 7 hÃ m phÃ¢n tÃ­ch nhanh
- âœ… MÃ¡y cÃ³ RAM háº¡n cháº¿ (< 12GB)
- âœ… Äang trong giai Ä‘oáº¡n development/testing
- âœ… Muá»‘n demo nhanh
- âœ… KhÃ´ng cáº§n real-time

### Chá»n PhÆ°Æ¡ng Ã¡n 2 (Docker) náº¿u:
- âœ… Muá»‘n test full production stack
- âœ… MÃ¡y Ä‘á»§ máº¡nh (12GB+ RAM)
- âœ… Cáº§n test real-time streaming
- âœ… Muá»‘n hiá»ƒu rÃµ toÃ n bá»™ kiáº¿n trÃºc
- âœ… CÃ³ thá»i gian setup

---

## ğŸ“ Cáº¤U TRÃšC 7 HÃ€M PHÃ‚N TÃCH

Cáº£ 2 phÆ°Æ¡ng Ã¡n Ä‘á»u cháº¡y Ä‘Æ°á»£c 7 hÃ m phÃ¢n tÃ­ch sau:

### 1. Top Selling Products (Recent)
- **Input**: review_df, business_df
- **Logic**: Äáº¿m reviews trong N ngÃ y gáº§n Ä‘Ã¢y
- **Output**: Top 10 businesses cÃ³ nhiá»u review nháº¥t

### 2. Diverse Stores
- **Input**: business_df
- **Logic**: Äáº¿m sá»‘ lÆ°á»£ng categories má»—i store
- **Output**: Top 10 stores cÃ³ nhiá»u categories nháº¥t

### 3. Best Rated Products
- **Input**: business_df, review_df
- **Logic**: TÃ­nh avg stars, filter min_reviews
- **Output**: Top 10 businesses rating cao nháº¥t

### 4. Most Positive Reviews
- **Input**: business_df, review_df
- **Logic**: Äáº¿m reviews >= 4 sao, tÃ­nh tá»· lá»‡
- **Output**: Top 10 stores cÃ³ nhiá»u positive reviews nháº¥t

### 5. Peak Hours
- **Input**: review_df
- **Logic**: Group by year, month, hour
- **Output**: Thá»‘ng kÃª sá»‘ lÆ°á»£ng reviews theo thá»i gian

### 6. Top Categories
- **Input**: business_df, review_df
- **Logic**: Explode categories, count reviews
- **Output**: Top 20 categories cÃ³ nhiá»u reviews nháº¥t

### 7. Store Statistics
- **Input**: business_df, review_df
- **Logic**: Aggregate all businesses vá»›i actual stats
- **Output**: Full statistics cá»§a táº¥t cáº£ stores

---

## ğŸ› ï¸ BÆ¯á»šC TIáº¾P THEO

### Náº¿u chá»n PhÆ°Æ¡ng Ã¡n 1:
1. TÃ´i sáº½ táº¡o cÃ¡c file batch version:
   - `Spark/batch_mode/batch_load_data.py`
   - `Spark/batch_mode/batch_pipeline.py`
   - `Spark/batch_mode/batch_main.py`
   - `Spark/batch_mode/batch_configuration.py`

2. Update code Ä‘á»ƒ Ä‘á»c tá»« local files

3. Táº¡o script cháº¡y nhanh: `run_local.sh`

4. Táº¡o sample data (náº¿u chÆ°a cÃ³)

### Náº¿u chá»n PhÆ°Æ¡ng Ã¡n 2:
1. Verify docker-compose.yml

2. Check data files cÃ³ sáºµn chÆ°a

3. Táº¡o script helper: `setup_docker.sh`

4. Táº¡o monitoring dashboard

---

## ğŸ”§ TROUBLESHOOTING

### PhÆ°Æ¡ng Ã¡n 1:
```bash
# Lá»—i: Module not found
pip install pyspark==4.0.1 requests

# Lá»—i: Java not found
# Install Java 11+: https://adoptium.net/

# Lá»—i: Data file not found
# Äáº£m báº£o files á»Ÿ Ä‘Ãºng thÆ° má»¥c data/
```

### PhÆ°Æ¡ng Ã¡n 2:
```bash
# Lá»—i: Docker khÃ´ng khá»Ÿi Ä‘á»™ng
docker-compose down -v
docker-compose up -d

# Lá»—i: Out of memory
# TÄƒng Docker memory limit: Docker Desktop â†’ Settings â†’ Resources

# Lá»—i: Port conflict
# Äá»•i port trong docker-compose.yml

# Lá»—i: Kafka connection refused
# Äá»£i thÃªm 2-3 phÃºt cho Kafka khá»Ÿi Ä‘á»™ng hoÃ n toÃ n
```

---

## ğŸ“ Há»I ÄÃP

**Q: TÃ´i nÃªn chá»n phÆ°Æ¡ng Ã¡n nÃ o?**
A: Náº¿u má»¥c Ä‘Ã­ch chá»‰ lÃ  xem káº¿t quáº£ 7 phÃ¢n tÃ­ch â†’ Chá»n PhÆ°Æ¡ng Ã¡n 1. Náº¿u muá»‘n test full stack â†’ Chá»n PhÆ°Æ¡ng Ã¡n 2.

**Q: Data á»Ÿ Ä‘Ã¢u?**
A: Cáº§n cÃ³ 3 files JSON tá»« Yelp dataset. Náº¿u chÆ°a cÃ³, tÃ´i cÃ³ thá»ƒ táº¡o sample data nhá» Ä‘á»ƒ test.

**Q: CÃ³ thá»ƒ káº¿t há»£p 2 phÆ°Æ¡ng Ã¡n khÃ´ng?**
A: CÃ³! Cháº¡y PhÆ°Æ¡ng Ã¡n 1 trÆ°á»›c Ä‘á»ƒ verify logic, sau Ä‘Ã³ cháº¡y PhÆ°Æ¡ng Ã¡n 2 cho production test.

**Q: Thá»i gian cháº¡y bao lÃ¢u?**
A:
- PhÆ°Æ¡ng Ã¡n 1: 5-10 phÃºt (tÃ¹y kÃ­ch thÆ°á»›c data)
- PhÆ°Æ¡ng Ã¡n 2: 30-45 phÃºt (setup + run)

**Q: Káº¿t quáº£ hiá»ƒn thá»‹ á»Ÿ Ä‘Ã¢u?**
A:
- PhÆ°Æ¡ng Ã¡n 1: Console output + files trong `output/`
- PhÆ°Æ¡ng Ã¡n 2: Elasticsearch + Kibana dashboard

---

## âœ… CHECKLIST TRÆ¯á»šC KHI Báº®T Äáº¦U

### PhÆ°Æ¡ng Ã¡n 1:
- [ ] Python 3.8+ Ä‘Ã£ cÃ i
- [ ] Java 11+ Ä‘Ã£ cÃ i
- [ ] CÃ³ data files hoáº·c ready Ä‘á»ƒ táº¡o sample
- [ ] ~5GB disk space trá»‘ng

### PhÆ°Æ¡ng Ã¡n 2:
- [ ] Docker & Docker Compose Ä‘Ã£ cÃ i
- [ ] Docker cÃ³ Ã­t nháº¥t 12GB RAM allocation
- [ ] CÃ³ data files (business.json, review.json, user.json)
- [ ] Ports available: 2181, 9092, 9000, 9870, 9200, 5601
- [ ] ~10GB disk space trá»‘ng

---

**Báº¡n muá»‘n triá»ƒn khai phÆ°Æ¡ng Ã¡n nÃ o? TÃ´i sáº½ giÃºp báº¡n setup chi tiáº¿t tá»«ng bÆ°á»›c! ğŸš€**
