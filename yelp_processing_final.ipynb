{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Dataset Processing - Sampling + Multi-file Strategy\n",
    "## X·ª≠ l√Ω dataset l·ªõn kh√¥ng tr√†n RAM\n",
    "\n",
    "**Chi·∫øn l∆∞·ª£c:**\n",
    "- Sample 40% t·ª´ m·ªói dataset\n",
    "- X·ª≠ l√Ω theo batch 100k records\n",
    "- Output: Multiple small files\n",
    "- RAM usage: < 2GB\n",
    "\n",
    "**Expected output:**\n",
    "- ~2.4M reviews (from 6M)\n",
    "- ~24 train files + 24 test files\n",
    "- Each file ~80-100k records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported!\n",
      "üìÖ Start: 2025-10-12 15:37:39\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import psutil\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n",
    "print(f\"üìÖ Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. C·∫•u h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Data path: Yelp/yelp_dataset/\n",
      "üìÇ Output: processed_data/\n",
      "üìä Batch size: 100,000\n",
      "üé≤ Sample rate: 40%\n",
      "üì¶ Records per output file: ~100,000\n",
      "‚úÇÔ∏è Train/Test: 80%/20%\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è C·∫§U H√åNH - THAY ƒê·ªîI ƒê∆Ø·ªúNG D·∫™N\n",
    "DATA_PATH = \"Yelp/yelp_dataset/\"\n",
    "\n",
    "FILE_PATHS = {\n",
    "    'business': DATA_PATH + 'yelp_academic_dataset_business.json',\n",
    "    'review': DATA_PATH + 'yelp_academic_dataset_review.json',\n",
    "    'user': DATA_PATH + 'yelp_academic_dataset_user.json'\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = \"processed_data/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Processing settings\n",
    "BATCH_SIZE = 100000          # ƒê·ªçc 100k records/batch\n",
    "SAMPLE_RATE = 0.4            # L·∫•y 40% t·ª´ m·ªói batch\n",
    "COMBINE_BATCHES = 5          # G·ªôp 5 batch th√†nh 1 file output\n",
    "RECORDS_PER_FILE = 100000    # ~100k records/file output\n",
    "\n",
    "# Train/Test split\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"üìÇ Data path: {DATA_PATH}\")\n",
    "print(f\"üìÇ Output: {OUTPUT_DIR}\")\n",
    "print(f\"üìä Batch size: {BATCH_SIZE:,}\")\n",
    "print(f\"üé≤ Sample rate: {SAMPLE_RATE:.0%}\")\n",
    "print(f\"üì¶ Records per output file: ~{RECORDS_PER_FILE:,}\")\n",
    "print(f\"‚úÇÔ∏è Train/Test: {TRAIN_RATIO:.0%}/{TEST_RATIO:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions ready!\n"
     ]
    }
   ],
   "source": [
    "def get_memory_mb():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    return psutil.Process().memory_info().rss / 1024 / 1024\n",
    "\n",
    "def count_lines(filepath):\n",
    "    \"\"\"Count total lines\"\"\"\n",
    "    print(f\"‚è≥ Counting lines in {filepath.split('/')[-1]}...\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        count = sum(1 for _ in f)\n",
    "    print(f\"   Total: {count:,} lines\")\n",
    "    return count\n",
    "\n",
    "def load_and_sample_batch(filepath, batch_size, sample_rate):\n",
    "    \"\"\"\n",
    "    Load JSON file in batches and sample immediately\n",
    "    Yields sampled DataFrames\n",
    "    \"\"\"\n",
    "    total_lines = count_lines(filepath)\n",
    "    filename = filepath.split('/')[-1]\n",
    "    \n",
    "    batch_data = []\n",
    "    processed = 0\n",
    "    errors = 0\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        pbar = tqdm(total=total_lines, desc=f\"Processing {filename}\", unit=\" lines\")\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                batch_data.append(obj)\n",
    "                processed += 1\n",
    "                \n",
    "                # When batch is full, sample and yield\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    df_batch = pd.DataFrame(batch_data)\n",
    "                    \n",
    "                    # Random sample\n",
    "                    sample_size = int(len(df_batch) * sample_rate)\n",
    "                    df_sampled = df_batch.sample(n=sample_size, random_state=RANDOM_STATE)\n",
    "                    \n",
    "                    yield df_sampled\n",
    "                    \n",
    "                    # Clear memory\n",
    "                    batch_data = []\n",
    "                    del df_batch\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    pbar.set_postfix({\n",
    "                        'RAM': f'{get_memory_mb():.0f}MB',\n",
    "                        'Sampled': f'{sample_size:,}'\n",
    "                    })\n",
    "            \n",
    "            except json.JSONDecodeError:\n",
    "                errors += 1\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Process remaining data\n",
    "        if batch_data:\n",
    "            df_batch = pd.DataFrame(batch_data)\n",
    "            sample_size = int(len(df_batch) * sample_rate)\n",
    "            if sample_size > 0:\n",
    "                df_sampled = df_batch.sample(n=sample_size, random_state=RANDOM_STATE)\n",
    "                yield df_sampled\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "    print(f\"‚úÖ Processed {processed:,} records ({errors} errors)\")\n",
    "    print(f\"üíæ Peak RAM: {get_memory_mb():.0f} MB\\n\")\n",
    "\n",
    "print(\"‚úÖ Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Business Data (40% Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üè¢ PROCESSING BUSINESS DATA (40% SAMPLE)\n",
      "================================================================================\n",
      "‚è≥ Counting lines in yelp_academic_dataset_business.json...\n",
      "   Total: 150,348 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_business.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150348/150348 [00:03<00:00, 49442.92 lines/s, RAM=639MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 150,346 records (2 errors)\n",
      "üíæ Peak RAM: 607 MB\n",
      "\n",
      "\n",
      "‚úÖ Business sampled: 60,138 records\n",
      "üìÅ Saved to: processed_data/business.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üè¢ PROCESSING BUSINESS DATA (40% SAMPLE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = OUTPUT_DIR + 'business.csv'\n",
    "first_write = True\n",
    "total_records = 0\n",
    "\n",
    "for sampled_df in load_and_sample_batch(\n",
    "    FILE_PATHS['business'], \n",
    "    BATCH_SIZE, \n",
    "    SAMPLE_RATE\n",
    "):\n",
    "    # Append to file\n",
    "    sampled_df.to_csv(output_file, mode='a', header=first_write, index=False)\n",
    "    first_write = False\n",
    "    total_records += len(sampled_df)\n",
    "    \n",
    "    del sampled_df\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ Business sampled: {total_records:,} records\")\n",
    "print(f\"üìÅ Saved to: {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process User Data (40% Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üë• PROCESSING USER DATA (40% SAMPLE)\n",
      "================================================================================\n",
      "‚è≥ Counting lines in yelp_academic_dataset_user.json...\n",
      "   Total: 1,987,897 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_user.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1987897/1987897 [00:28<00:00, 68945.94 lines/s, RAM=708MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 1,987,897 records (0 errors)\n",
      "üíæ Peak RAM: 694 MB\n",
      "\n",
      "\n",
      "‚úÖ User sampled: 795,158 records\n",
      "üìÅ Saved to: processed_data/user.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üë• PROCESSING USER DATA (40% SAMPLE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = OUTPUT_DIR + 'user.csv'\n",
    "first_write = True\n",
    "total_records = 0\n",
    "\n",
    "for sampled_df in load_and_sample_batch(\n",
    "    FILE_PATHS['user'], \n",
    "    BATCH_SIZE, \n",
    "    SAMPLE_RATE\n",
    "):\n",
    "    # Map yelping_since to since\n",
    "    if 'yelping_since' in sampled_df.columns:\n",
    "        sampled_df['since'] = sampled_df['yelping_since']\n",
    "    \n",
    "    # Select required columns\n",
    "    user_cols = ['user_id', 'name', 'review_count', 'since', 'useful', 'fans', 'average_stars']\n",
    "    available_cols = [col for col in user_cols if col in sampled_df.columns]\n",
    "    sampled_df = sampled_df[available_cols]\n",
    "    \n",
    "    # Append to file\n",
    "    sampled_df.to_csv(output_file, mode='a', header=first_write, index=False)\n",
    "    first_write = False\n",
    "    total_records += len(sampled_df)\n",
    "    \n",
    "    del sampled_df\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n‚úÖ User sampled: {total_records:,} records\")\n",
    "print(f\"üìÅ Saved to: {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Review Data (40% Sample + Multi-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìù PROCESSING REVIEW DATA (40% SAMPLE + MULTI-FILE)\n",
      "================================================================================\n",
      "‚è≥ Counting lines in yelp_academic_dataset_review.json...\n",
      "   Total: 6,990,280 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:   4%|‚ñç         | 299571/6990280 [00:01<00:38, 174407.86 lines/s, RAM=420MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #1:\n",
      "   Records: 119,938\n",
      "   Duplicates removed: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:   4%|‚ñç         | 299999/6990280 [00:03<00:38, 174407.86 lines/s, RAM=450MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:   9%|‚ñä         | 597106/6990280 [00:05<00:38, 165939.93 lines/s, RAM=452MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #2:\n",
      "   Records: 119,948\n",
      "   Duplicates removed: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:   9%|‚ñâ         | 616959/6990280 [00:06<02:22, 44809.56 lines/s, RAM=464MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  13%|‚ñà‚ñé        | 899244/6990280 [00:08<00:34, 174346.49 lines/s, RAM=466MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #3:\n",
      "   Records: 119,970\n",
      "   Duplicates removed: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  13%|‚ñà‚ñé        | 920197/6990280 [00:09<02:04, 48806.45 lines/s, RAM=469MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  17%|‚ñà‚ñã        | 1181264/6990280 [00:11<00:38, 151559.07 lines/s, RAM=470MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #4:\n",
      "   Records: 119,946\n",
      "   Duplicates removed: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  17%|‚ñà‚ñã        | 1200678/6990280 [00:12<02:06, 45681.18 lines/s, RAM=473MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  21%|‚ñà‚ñà        | 1481782/6990280 [00:14<00:34, 159545.98 lines/s, RAM=473MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #5:\n",
      "   Records: 119,954\n",
      "   Duplicates removed: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  21%|‚ñà‚ñà‚ñè       | 1501861/6990280 [00:15<01:59, 46044.71 lines/s, RAM=476MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  26%|‚ñà‚ñà‚ñå       | 1786740/6990280 [00:17<00:32, 161191.20 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #6:\n",
      "   Records: 119,937\n",
      "   Duplicates removed: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  26%|‚ñà‚ñà‚ñå       | 1806996/6990280 [00:18<01:47, 48285.82 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  30%|‚ñà‚ñà‚ñâ       | 2090046/6990280 [00:20<00:29, 168835.20 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #7:\n",
      "   Records: 119,921\n",
      "   Duplicates removed: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  30%|‚ñà‚ñà‚ñà       | 2110297/6990280 [00:21<01:44, 46724.41 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  34%|‚ñà‚ñà‚ñà‚ñç      | 2390581/6990280 [00:23<00:26, 173783.16 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #8:\n",
      "   Records: 119,936\n",
      "   Duplicates removed: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  34%|‚ñà‚ñà‚ñà‚ñç      | 2411398/6990280 [00:24<01:32, 49704.10 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  39%|‚ñà‚ñà‚ñà‚ñä      | 2692208/6990280 [00:26<00:26, 165269.77 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #9:\n",
      "   Records: 119,936\n",
      "   Duplicates removed: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  39%|‚ñà‚ñà‚ñà‚ñâ      | 2712239/6990280 [00:27<01:32, 46122.16 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 2991897/6990280 [00:29<00:23, 170852.51 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #10:\n",
      "   Records: 119,954\n",
      "   Duplicates removed: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3012425/6990280 [00:30<01:22, 48252.88 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 3295833/6990280 [00:32<00:21, 168257.49 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #11:\n",
      "   Records: 119,932\n",
      "   Duplicates removed: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 3315975/6990280 [00:33<01:17, 47306.17 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3591495/6990280 [00:35<00:20, 168577.44 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #12:\n",
      "   Records: 119,953\n",
      "   Duplicates removed: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 3611644/6990280 [00:36<01:13, 46094.89 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3886774/6990280 [00:38<00:19, 162323.40 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #13:\n",
      "   Records: 119,932\n",
      "   Duplicates removed: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3906273/6990280 [00:40<01:07, 45748.72 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4185758/6990280 [00:41<00:16, 166140.64 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #14:\n",
      "   Records: 119,949\n",
      "   Duplicates removed: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4205819/6990280 [00:43<01:00, 45750.66 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 4488534/6990280 [00:45<00:14, 170735.06 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #15:\n",
      "   Records: 119,921\n",
      "   Duplicates removed: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 4509053/6990280 [00:46<00:51, 48076.48 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4785703/6990280 [00:48<00:13, 165707.64 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #16:\n",
      "   Records: 119,951\n",
      "   Duplicates removed: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 4805608/6990280 [00:49<00:47, 45962.72 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 5097184/6990280 [00:51<00:11, 167261.37 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #17:\n",
      "   Records: 119,948\n",
      "   Duplicates removed: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 5117130/6990280 [00:52<00:40, 45738.24 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 5397155/6990280 [00:54<00:09, 169664.76 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #18:\n",
      "   Records: 119,941\n",
      "   Duplicates removed: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 5417397/6990280 [00:55<00:33, 47636.76 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 5695100/6990280 [00:57<00:07, 170011.30 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #19:\n",
      "   Records: 119,961\n",
      "   Duplicates removed: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 5715639/6990280 [00:58<00:26, 47426.02 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 5979696/6990280 [01:00<00:06, 158798.24 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #20:\n",
      "   Records: 119,936\n",
      "   Duplicates removed: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6000000/6990280 [01:01<00:20, 48022.84 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 6297068/6990280 [01:03<00:04, 166703.60 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #21:\n",
      "   Records: 119,951\n",
      "   Duplicates removed: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 6316780/6990280 [01:04<00:14, 46027.72 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 6578854/6990280 [01:06<00:02, 158042.08 lines/s, RAM=478MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #22:\n",
      "   Records: 119,931\n",
      "   Duplicates removed: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 6621437/6990280 [01:07<00:05, 62709.39 lines/s, RAM=478MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 6882657/6990280 [01:09<00:00, 157872.13 lines/s, RAM=478MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Creating file #23:\n",
      "   Records: 119,923\n",
      "   Duplicates removed: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 6902446/6990280 [01:10<00:01, 46962.87 lines/s, RAM=478MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Saved: processed_data/review_combined_23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6990280/6990280 [01:11<00:00, 97880.31 lines/s, RAM=478MB, Sampled=40,000] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 6,990,280 records (0 errors)\n",
      "üíæ Peak RAM: 483 MB\n",
      "\n",
      "\n",
      "üì¶ Creating final file #24:\n",
      "   Records: 36,104\n",
      "   Duplicates removed: 8\n",
      "   ‚úÖ Saved: processed_data/review_combined_24.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Review processing complete!\n",
      "   Total sampled: 2,796,112 records\n",
      "   Null reviews removed: 0\n",
      "   Output files: 24 files\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìù PROCESSING REVIEW DATA (40% SAMPLE + MULTI-FILE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accumulated_data = []\n",
    "file_counter = 1\n",
    "total_sampled = 0\n",
    "null_removed = 0\n",
    "\n",
    "for sampled_df in load_and_sample_batch(\n",
    "    FILE_PATHS['review'], \n",
    "    BATCH_SIZE, \n",
    "    SAMPLE_RATE\n",
    "):\n",
    "    # Create sentiment labels\n",
    "    sampled_df['label'] = sampled_df['stars'].apply(\n",
    "        lambda x: 0 if x <= 2 else (2 if x == 3 else 1)\n",
    "    )\n",
    "    \n",
    "    # Remove null text\n",
    "    before = len(sampled_df)\n",
    "    sampled_df = sampled_df.dropna(subset=['text'])\n",
    "    null_removed += (before - len(sampled_df))\n",
    "    \n",
    "    # Select columns\n",
    "    sampled_df = sampled_df[['text', 'label']]\n",
    "    sampled_df.columns = ['review', 'label']\n",
    "    \n",
    "    # Accumulate data\n",
    "    accumulated_data.append(sampled_df)\n",
    "    total_sampled += len(sampled_df)\n",
    "    \n",
    "    # Check if we have enough data for a file\n",
    "    total_accumulated = sum(len(df) for df in accumulated_data)\n",
    "    \n",
    "    if total_accumulated >= RECORDS_PER_FILE:\n",
    "        # Combine accumulated batches\n",
    "        combined_df = pd.concat(accumulated_data, ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates within this file\n",
    "        before_dup = len(combined_df)\n",
    "        combined_df = combined_df.drop_duplicates(subset=['review'])\n",
    "        dup_removed = before_dup - len(combined_df)\n",
    "        \n",
    "        print(f\"\\nüì¶ Creating file #{file_counter}:\")\n",
    "        print(f\"   Records: {len(combined_df):,}\")\n",
    "        print(f\"   Duplicates removed: {dup_removed:,}\")\n",
    "        \n",
    "        # Save combined file\n",
    "        review_file = OUTPUT_DIR + f'review_combined_{file_counter}.csv'\n",
    "        combined_df.to_csv(review_file, index=False)\n",
    "        print(f\"   ‚úÖ Saved: {review_file}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del combined_df\n",
    "        accumulated_data = []\n",
    "        gc.collect()\n",
    "        \n",
    "        file_counter += 1\n",
    "    \n",
    "    del sampled_df\n",
    "    gc.collect()\n",
    "\n",
    "# Process remaining data\n",
    "if accumulated_data:\n",
    "    combined_df = pd.concat(accumulated_data, ignore_index=True)\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['review'])\n",
    "    dup_removed = before_dup - len(combined_df)\n",
    "    \n",
    "    print(f\"\\nüì¶ Creating final file #{file_counter}:\")\n",
    "    print(f\"   Records: {len(combined_df):,}\")\n",
    "    print(f\"   Duplicates removed: {dup_removed:,}\")\n",
    "    \n",
    "    review_file = OUTPUT_DIR + f'review_combined_{file_counter}.csv'\n",
    "    combined_df.to_csv(review_file, index=False)\n",
    "    print(f\"   ‚úÖ Saved: {review_file}\")\n",
    "    \n",
    "    del combined_df\n",
    "    gc.collect()\n",
    "\n",
    "num_review_files = file_counter\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Review processing complete!\")\n",
    "print(f\"   Total sampled: {total_sampled:,} records\")\n",
    "print(f\"   Null reviews removed: {null_removed:,}\")\n",
    "print(f\"   Output files: {num_review_files} files\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split cho t·ª´ng file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úÇÔ∏è TRAIN/TEST SPLIT (STRATIFIED)\n",
      "================================================================================\n",
      "\n",
      "üìÇ Processing file 1/24...\n",
      "   Loaded: 119,938 records\n",
      "   ‚úÖ Train: 95,950 ‚Üí processed_data/train_part1.csv\n",
      "   ‚úÖ Test: 23,988 ‚Üí processed_data/test_part1.csv\n",
      "\n",
      "üìÇ Processing file 2/24...\n",
      "   Loaded: 119,948 records\n",
      "   ‚úÖ Train: 95,958 ‚Üí processed_data/train_part2.csv\n",
      "   ‚úÖ Test: 23,990 ‚Üí processed_data/test_part2.csv\n",
      "\n",
      "üìÇ Processing file 3/24...\n",
      "   Loaded: 119,970 records\n",
      "   ‚úÖ Train: 95,976 ‚Üí processed_data/train_part3.csv\n",
      "   ‚úÖ Test: 23,994 ‚Üí processed_data/test_part3.csv\n",
      "\n",
      "üìÇ Processing file 4/24...\n",
      "   Loaded: 119,946 records\n",
      "   ‚úÖ Train: 95,956 ‚Üí processed_data/train_part4.csv\n",
      "   ‚úÖ Test: 23,990 ‚Üí processed_data/test_part4.csv\n",
      "\n",
      "üìÇ Processing file 5/24...\n",
      "   Loaded: 119,954 records\n",
      "   ‚úÖ Train: 95,963 ‚Üí processed_data/train_part5.csv\n",
      "   ‚úÖ Test: 23,991 ‚Üí processed_data/test_part5.csv\n",
      "\n",
      "üìÇ Processing file 6/24...\n",
      "   Loaded: 119,937 records\n",
      "   ‚úÖ Train: 95,949 ‚Üí processed_data/train_part6.csv\n",
      "   ‚úÖ Test: 23,988 ‚Üí processed_data/test_part6.csv\n",
      "\n",
      "üìÇ Processing file 7/24...\n",
      "   Loaded: 119,921 records\n",
      "   ‚úÖ Train: 95,936 ‚Üí processed_data/train_part7.csv\n",
      "   ‚úÖ Test: 23,985 ‚Üí processed_data/test_part7.csv\n",
      "\n",
      "üìÇ Processing file 8/24...\n",
      "   Loaded: 119,936 records\n",
      "   ‚úÖ Train: 95,948 ‚Üí processed_data/train_part8.csv\n",
      "   ‚úÖ Test: 23,988 ‚Üí processed_data/test_part8.csv\n",
      "\n",
      "üìÇ Processing file 9/24...\n",
      "   Loaded: 119,936 records\n",
      "   ‚úÖ Train: 95,948 ‚Üí processed_data/train_part9.csv\n",
      "   ‚úÖ Test: 23,988 ‚Üí processed_data/test_part9.csv\n",
      "\n",
      "üìÇ Processing file 10/24...\n",
      "   Loaded: 119,954 records\n",
      "   ‚úÖ Train: 95,963 ‚Üí processed_data/train_part10.csv\n",
      "   ‚úÖ Test: 23,991 ‚Üí processed_data/test_part10.csv\n",
      "\n",
      "üìÇ Processing file 11/24...\n",
      "   Loaded: 119,932 records\n",
      "   ‚úÖ Train: 95,945 ‚Üí processed_data/train_part11.csv\n",
      "   ‚úÖ Test: 23,987 ‚Üí processed_data/test_part11.csv\n",
      "\n",
      "üìÇ Processing file 12/24...\n",
      "   Loaded: 119,953 records\n",
      "   ‚úÖ Train: 95,962 ‚Üí processed_data/train_part12.csv\n",
      "   ‚úÖ Test: 23,991 ‚Üí processed_data/test_part12.csv\n",
      "\n",
      "üìÇ Processing file 13/24...\n",
      "   Loaded: 119,932 records\n",
      "   ‚úÖ Train: 95,945 ‚Üí processed_data/train_part13.csv\n",
      "   ‚úÖ Test: 23,987 ‚Üí processed_data/test_part13.csv\n",
      "\n",
      "üìÇ Processing file 14/24...\n",
      "   Loaded: 119,949 records\n",
      "   ‚úÖ Train: 95,959 ‚Üí processed_data/train_part14.csv\n",
      "   ‚úÖ Test: 23,990 ‚Üí processed_data/test_part14.csv\n",
      "\n",
      "üìÇ Processing file 15/24...\n",
      "   Loaded: 119,921 records\n",
      "   ‚úÖ Train: 95,936 ‚Üí processed_data/train_part15.csv\n",
      "   ‚úÖ Test: 23,985 ‚Üí processed_data/test_part15.csv\n",
      "\n",
      "üìÇ Processing file 16/24...\n",
      "   Loaded: 119,951 records\n",
      "   ‚úÖ Train: 95,960 ‚Üí processed_data/train_part16.csv\n",
      "   ‚úÖ Test: 23,991 ‚Üí processed_data/test_part16.csv\n",
      "\n",
      "üìÇ Processing file 17/24...\n",
      "   Loaded: 119,948 records\n",
      "   ‚úÖ Train: 95,958 ‚Üí processed_data/train_part17.csv\n",
      "   ‚úÖ Test: 23,990 ‚Üí processed_data/test_part17.csv\n",
      "\n",
      "üìÇ Processing file 18/24...\n",
      "   Loaded: 119,941 records\n",
      "   ‚úÖ Train: 95,952 ‚Üí processed_data/train_part18.csv\n",
      "   ‚úÖ Test: 23,989 ‚Üí processed_data/test_part18.csv\n",
      "\n",
      "üìÇ Processing file 19/24...\n",
      "   Loaded: 119,961 records\n",
      "   ‚úÖ Train: 95,968 ‚Üí processed_data/train_part19.csv\n",
      "   ‚úÖ Test: 23,993 ‚Üí processed_data/test_part19.csv\n",
      "\n",
      "üìÇ Processing file 20/24...\n",
      "   Loaded: 119,936 records\n",
      "   ‚úÖ Train: 95,948 ‚Üí processed_data/train_part20.csv\n",
      "   ‚úÖ Test: 23,988 ‚Üí processed_data/test_part20.csv\n",
      "\n",
      "üìÇ Processing file 21/24...\n",
      "   Loaded: 119,951 records\n",
      "   ‚úÖ Train: 95,960 ‚Üí processed_data/train_part21.csv\n",
      "   ‚úÖ Test: 23,991 ‚Üí processed_data/test_part21.csv\n",
      "\n",
      "üìÇ Processing file 22/24...\n",
      "   Loaded: 119,931 records\n",
      "   ‚úÖ Train: 95,944 ‚Üí processed_data/train_part22.csv\n",
      "   ‚úÖ Test: 23,987 ‚Üí processed_data/test_part22.csv\n",
      "\n",
      "üìÇ Processing file 23/24...\n",
      "   Loaded: 119,923 records\n",
      "   ‚úÖ Train: 95,938 ‚Üí processed_data/train_part23.csv\n",
      "   ‚úÖ Test: 23,985 ‚Üí processed_data/test_part23.csv\n",
      "\n",
      "üìÇ Processing file 24/24...\n",
      "   Loaded: 36,104 records\n",
      "   ‚úÖ Train: 28,883 ‚Üí processed_data/train_part24.csv\n",
      "   ‚úÖ Test: 7,221 ‚Üí processed_data/test_part24.csv\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Split complete!\n",
      "   Total train: 2,235,805 records\n",
      "   Total test: 558,968 records\n",
      "   Train files: 24\n",
      "   Test files: 24\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"‚úÇÔ∏è TRAIN/TEST SPLIT (STRATIFIED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "\n",
    "for i in range(1, num_review_files + 1):\n",
    "    review_file = OUTPUT_DIR + f'review_combined_{i}.csv'\n",
    "    \n",
    "    print(f\"\\nüìÇ Processing file {i}/{num_review_files}...\")\n",
    "    \n",
    "    # Load file\n",
    "    df = pd.read_csv(review_file)\n",
    "    print(f\"   Loaded: {len(df):,} records\")\n",
    "    \n",
    "    # Split with stratify\n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=TEST_RATIO,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    # Save train\n",
    "    train_file = OUTPUT_DIR + f'train_part{i}.csv'\n",
    "    train_df.to_csv(train_file, index=False)\n",
    "    print(f\"   ‚úÖ Train: {len(train_df):,} ‚Üí {train_file}\")\n",
    "    \n",
    "    # Save test\n",
    "    test_file = OUTPUT_DIR + f'test_part{i}.csv'\n",
    "    test_df.to_csv(test_file, index=False)\n",
    "    print(f\"   ‚úÖ Test: {len(test_df):,} ‚Üí {test_file}\")\n",
    "    \n",
    "    total_train += len(train_df)\n",
    "    total_test += len(test_df)\n",
    "    \n",
    "    # Clean memory\n",
    "    del df, train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úÖ Split complete!\")\n",
    "print(f\"   Total train: {total_train:,} records\")\n",
    "print(f\"   Total test: {total_test:,} records\")\n",
    "print(f\"   Train files: {num_review_files}\")\n",
    "print(f\"   Test files: {num_review_files}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä FINAL SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "üìÅ Output Files:\n",
      "   Business: 1 file(s)\n",
      "   User: 1 file(s)\n",
      "   Review combined: 24 file(s)\n",
      "   Train: 24 file(s)\n",
      "   Test: 24 file(s)\n",
      "\n",
      "üíæ Total output size: 6367.9 MB\n",
      "\n",
      "üìä Record Counts:\n",
      "   Business: 60,138\n",
      "   User: 795,158\n",
      "   Train (total): 2,235,805\n",
      "   Test (total): 558,968\n",
      "\n",
      "‚öôÔ∏è Processing Settings:\n",
      "   Sample rate: 40%\n",
      "   Batch size: 100,000\n",
      "   Records per file: ~100,000\n",
      "\n",
      "üíæ Peak RAM usage: 478 MB\n",
      "üìÖ Completed: 2025-10-12 15:40:01\n",
      "\n",
      "================================================================================\n",
      "üéâ PROCESSING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìö How to use:\n",
      "   1. Train files: train_part1.csv, train_part2.csv, ...\n",
      "   2. Test files: test_part1.csv, test_part2.csv, ...\n",
      "   3. Load multiple files in training loop\n",
      "   4. Labels: 0=Ti√™u c·ª±c, 1=T√≠ch c·ª±c, 2=Trung l·∫≠p\n",
      "\n",
      "‚úÖ Summary saved to: processed_data/processing_summary.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"üìä FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List all files\n",
    "all_files = os.listdir(OUTPUT_DIR)\n",
    "business_files = [f for f in all_files if f.startswith('business')]\n",
    "user_files = [f for f in all_files if f.startswith('user')]\n",
    "review_files = [f for f in all_files if f.startswith('review_combined')]\n",
    "train_files = [f for f in all_files if f.startswith('train_part')]\n",
    "test_files = [f for f in all_files if f.startswith('test_part')]\n",
    "\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"   Business: {len(business_files)} file(s)\")\n",
    "print(f\"   User: {len(user_files)} file(s)\")\n",
    "print(f\"   Review combined: {len(review_files)} file(s)\")\n",
    "print(f\"   Train: {len(train_files)} file(s)\")\n",
    "print(f\"   Test: {len(test_files)} file(s)\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size = 0\n",
    "for f in all_files:\n",
    "    filepath = OUTPUT_DIR + f\n",
    "    if os.path.isfile(filepath):\n",
    "        total_size += os.path.getsize(filepath)\n",
    "\n",
    "print(f\"\\nüíæ Total output size: {total_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(f\"\\nüìä Record Counts:\")\n",
    "print(f\"   Business: {pd.read_csv(OUTPUT_DIR + 'business.csv').shape[0]:,}\")\n",
    "print(f\"   User: {pd.read_csv(OUTPUT_DIR + 'user.csv').shape[0]:,}\")\n",
    "print(f\"   Train (total): {total_train:,}\")\n",
    "print(f\"   Test (total): {total_test:,}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Processing Settings:\")\n",
    "print(f\"   Sample rate: {SAMPLE_RATE:.0%}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE:,}\")\n",
    "print(f\"   Records per file: ~{RECORDS_PER_FILE:,}\")\n",
    "\n",
    "print(f\"\\nüíæ Peak RAM usage: {get_memory_mb():.0f} MB\")\n",
    "print(f\"üìÖ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PROCESSING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìö How to use:\")\n",
    "print(f\"   1. Train files: train_part1.csv, train_part2.csv, ...\")\n",
    "print(f\"   2. Test files: test_part1.csv, test_part2.csv, ...\")\n",
    "print(f\"   3. Load multiple files in training loop\")\n",
    "print(f\"   4. Labels: 0=Ti√™u c·ª±c, 1=T√≠ch c·ª±c, 2=Trung l·∫≠p\")\n",
    "\n",
    "# Save summary to file\n",
    "summary_file = OUTPUT_DIR + 'processing_summary.txt'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"YELP DATASET PROCESSING SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(f\"Business files: {len(business_files)}\\n\")\n",
    "    f.write(f\"User files: {len(user_files)}\\n\")\n",
    "    f.write(f\"Review combined files: {len(review_files)}\\n\")\n",
    "    f.write(f\"Train files: {len(train_files)}\\n\")\n",
    "    f.write(f\"Test files: {len(test_files)}\\n\\n\")\n",
    "    f.write(f\"Total train records: {total_train:,}\\n\")\n",
    "    f.write(f\"Total test records: {total_test:,}\\n\\n\")\n",
    "    f.write(f\"Sample rate: {SAMPLE_RATE:.0%}\\n\")\n",
    "    f.write(f\"Total size: {total_size / 1024 / 1024:.1f} MB\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã PREVIEW: train_part1.csv\n",
      "\n",
      "                                              review  label\n",
      "0  I stopped by Caf√© Beignet (bourbon street loca...      1\n",
      "1  First a disclaimer:  I only speak of one cab r...      1\n",
      "2  Wonderful brunch option in the Funk Zone. Food...      1\n",
      "3  I haven't been downtown for a while, but happe...      1\n",
      "4  Good as it gets for bbq in Tampa.  Service is ...      1\n",
      "5  I went to V Vegaz because they had Diva-Curls ...      1\n",
      "6  The food is 5 stars, but the service is 1. If ...      0\n",
      "7  Good food. Menu was too busy and they did not ...      2\n",
      "8  Nice late night food and pub. Wish the variety...      2\n",
      "9  Moonshine me up, baby!\\n\\nStopped in with my f...      2\n",
      "\n",
      "üìä Label distribution in train_part1:\n",
      "label\n",
      "0    18398\n",
      "1    66550\n",
      "2    11002\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preview first train file\n",
    "print(\"üìã PREVIEW: train_part1.csv\\n\")\n",
    "df_preview = pd.read_csv(OUTPUT_DIR + 'train_part1.csv')\n",
    "print(df_preview.head(10))\n",
    "\n",
    "print(f\"\\nüìä Label distribution in train_part1:\")\n",
    "print(df_preview['label'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
