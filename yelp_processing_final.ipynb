{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Dataset Processing - Sampling + Multi-file Strategy\n",
    "## Xá»­ lÃ½ dataset lá»›n khÃ´ng trÃ n RAM\n",
    "\n",
    "**Chiáº¿n lÆ°á»£c:**\n",
    "- Sample 40% tá»« má»—i dataset\n",
    "- Xá»­ lÃ½ theo batch 100k records\n",
    "- Output: Multiple small files\n",
    "- RAM usage: < 2GB\n",
    "\n",
    "**Expected output:**\n",
    "- ~2.4M reviews (from 6M)\n",
    "- ~24 train files + 24 test files\n",
    "- Each file ~80-100k records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. CÃ i Ä‘áº·t thÆ° viá»‡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported!\n",
      "ğŸ“… Start: 2025-10-12 15:37:39\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import psutil\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"âœ… Libraries imported!\")\n",
    "print(f\"ğŸ“… Start: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cáº¥u hÃ¬nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Data path: Yelp/yelp_dataset/\n",
      "ğŸ“‚ Output: processed_data/\n",
      "ğŸ“Š Batch size: 100,000\n",
      "ğŸ² Sample rate: 40%\n",
      "ğŸ“¦ Records per output file: ~100,000\n",
      "âœ‚ï¸ Train/Test: 80%/20%\n"
     ]
    }
   ],
   "source": [
    "# âš™ï¸ Cáº¤U HÃŒNH - THAY Äá»”I ÄÆ¯á»œNG DáºªN\n",
    "DATA_PATH = \"Yelp/yelp_dataset/\"\n",
    "\n",
    "FILE_PATHS = {\n",
    "    'business': DATA_PATH + 'yelp_academic_dataset_business.json',\n",
    "    'review': DATA_PATH + 'yelp_academic_dataset_review.json',\n",
    "    'user': DATA_PATH + 'yelp_academic_dataset_user.json'\n",
    "}\n",
    "\n",
    "OUTPUT_DIR = \"processed_data/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Processing settings\n",
    "BATCH_SIZE = 100000          # Äá»c 100k records/batch\n",
    "SAMPLE_RATE = 0.4            # Láº¥y 40% tá»« má»—i batch\n",
    "COMBINE_BATCHES = 5          # Gá»™p 5 batch thÃ nh 1 file output\n",
    "RECORDS_PER_FILE = 100000    # ~100k records/file output\n",
    "\n",
    "# Train/Test split\n",
    "TRAIN_RATIO = 0.8\n",
    "TEST_RATIO = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"ğŸ“‚ Data path: {DATA_PATH}\")\n",
    "print(f\"ğŸ“‚ Output: {OUTPUT_DIR}\")\n",
    "print(f\"ğŸ“Š Batch size: {BATCH_SIZE:,}\")\n",
    "print(f\"ğŸ² Sample rate: {SAMPLE_RATE:.0%}\")\n",
    "print(f\"ğŸ“¦ Records per output file: ~{RECORDS_PER_FILE:,}\")\n",
    "print(f\"âœ‚ï¸ Train/Test: {TRAIN_RATIO:.0%}/{TEST_RATIO:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions ready!\n"
     ]
    }
   ],
   "source": [
    "def get_memory_mb():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    return psutil.Process().memory_info().rss / 1024 / 1024\n",
    "\n",
    "def count_lines(filepath):\n",
    "    \"\"\"Count total lines\"\"\"\n",
    "    print(f\"â³ Counting lines in {filepath.split('/')[-1]}...\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        count = sum(1 for _ in f)\n",
    "    print(f\"   Total: {count:,} lines\")\n",
    "    return count\n",
    "\n",
    "def load_and_sample_batch(filepath, batch_size, sample_rate):\n",
    "    \"\"\"\n",
    "    Load JSON file in batches and sample immediately\n",
    "    Yields sampled DataFrames\n",
    "    \"\"\"\n",
    "    total_lines = count_lines(filepath)\n",
    "    filename = filepath.split('/')[-1]\n",
    "    \n",
    "    batch_data = []\n",
    "    processed = 0\n",
    "    errors = 0\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        pbar = tqdm(total=total_lines, desc=f\"Processing {filename}\", unit=\" lines\")\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                batch_data.append(obj)\n",
    "                processed += 1\n",
    "                \n",
    "                # When batch is full, sample and yield\n",
    "                if len(batch_data) >= batch_size:\n",
    "                    df_batch = pd.DataFrame(batch_data)\n",
    "                    \n",
    "                    # Random sample\n",
    "                    sample_size = int(len(df_batch) * sample_rate)\n",
    "                    df_sampled = df_batch.sample(n=sample_size, random_state=RANDOM_STATE)\n",
    "                    \n",
    "                    yield df_sampled\n",
    "                    \n",
    "                    # Clear memory\n",
    "                    batch_data = []\n",
    "                    del df_batch\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    pbar.set_postfix({\n",
    "                        'RAM': f'{get_memory_mb():.0f}MB',\n",
    "                        'Sampled': f'{sample_size:,}'\n",
    "                    })\n",
    "            \n",
    "            except json.JSONDecodeError:\n",
    "                errors += 1\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Process remaining data\n",
    "        if batch_data:\n",
    "            df_batch = pd.DataFrame(batch_data)\n",
    "            sample_size = int(len(df_batch) * sample_rate)\n",
    "            if sample_size > 0:\n",
    "                df_sampled = df_batch.sample(n=sample_size, random_state=RANDOM_STATE)\n",
    "                yield df_sampled\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "    print(f\"âœ… Processed {processed:,} records ({errors} errors)\")\n",
    "    print(f\"ğŸ’¾ Peak RAM: {get_memory_mb():.0f} MB\\n\")\n",
    "\n",
    "print(\"âœ… Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Business Data (40% Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¢ PROCESSING BUSINESS DATA (40% SAMPLE)\n",
      "================================================================================\n",
      "â³ Counting lines in yelp_academic_dataset_business.json...\n",
      "   Total: 150,348 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_business.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150348/150348 [00:03<00:00, 49442.92 lines/s, RAM=639MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 150,346 records (2 errors)\n",
      "ğŸ’¾ Peak RAM: 607 MB\n",
      "\n",
      "\n",
      "âœ… Business sampled: 60,138 records\n",
      "ğŸ“ Saved to: processed_data/business.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸ¢ PROCESSING BUSINESS DATA (40% SAMPLE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = OUTPUT_DIR + 'business.csv'\n",
    "first_write = True\n",
    "total_records = 0\n",
    "\n",
    "for sampled_df in load_and_sample_batch(\n",
    "    FILE_PATHS['business'], \n",
    "    BATCH_SIZE, \n",
    "    SAMPLE_RATE\n",
    "):\n",
    "    # Append to file\n",
    "    sampled_df.to_csv(output_file, mode='a', header=first_write, index=False)\n",
    "    first_write = False\n",
    "    total_records += len(sampled_df)\n",
    "    \n",
    "    del sampled_df\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… Business sampled: {total_records:,} records\")\n",
    "print(f\"ğŸ“ Saved to: {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process User Data (40% Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ‘¥ PROCESSING USER DATA (40% SAMPLE)\n",
      "================================================================================\n",
      "â³ Counting lines in yelp_academic_dataset_user.json...\n",
      "   Total: 1,987,897 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_user.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1987897/1987897 [00:28<00:00, 68945.94 lines/s, RAM=708MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 1,987,897 records (0 errors)\n",
      "ğŸ’¾ Peak RAM: 694 MB\n",
      "\n",
      "\n",
      "âœ… User sampled: 795,158 records\n",
      "ğŸ“ Saved to: processed_data/user.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸ‘¥ PROCESSING USER DATA (40% SAMPLE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "output_file = OUTPUT_DIR + 'user.csv'\n",
    "first_write = True\n",
    "total_records = 0\n",
    "\n",
    "for sampled_df in load_and_sample_batch(\n",
    "    FILE_PATHS['user'], \n",
    "    BATCH_SIZE, \n",
    "    SAMPLE_RATE\n",
    "):\n",
    "    # Map yelping_since to since\n",
    "    if 'yelping_since' in sampled_df.columns:\n",
    "        sampled_df['since'] = sampled_df['yelping_since']\n",
    "    \n",
    "    # Select required columns\n",
    "    user_cols = ['user_id', 'name', 'review_count', 'since', 'useful', 'fans', 'average_stars']\n",
    "    available_cols = [col for col in user_cols if col in sampled_df.columns]\n",
    "    sampled_df = sampled_df[available_cols]\n",
    "    \n",
    "    # Append to file\n",
    "    sampled_df.to_csv(output_file, mode='a', header=first_write, index=False)\n",
    "    first_write = False\n",
    "    total_records += len(sampled_df)\n",
    "    \n",
    "    del sampled_df\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… User sampled: {total_records:,} records\")\n",
    "print(f\"ğŸ“ Saved to: {output_file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Review Data (40% Sample + Multi-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“ PROCESSING REVIEW DATA (40% SAMPLE + MULTI-FILE)\n",
      "================================================================================\n",
      "â³ Counting lines in yelp_academic_dataset_review.json...\n",
      "   Total: 6,990,280 lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:   4%|â–         | 299571/6990280 [00:01<00:38, 174407.86 lines/s, RAM=420MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #1:\n",
      "   Records: 119,938\n",
      "   Duplicates removed: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:   4%|â–         | 299999/6990280 [00:03<00:38, 174407.86 lines/s, RAM=450MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:   9%|â–Š         | 597106/6990280 [00:05<00:38, 165939.93 lines/s, RAM=452MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #2:\n",
      "   Records: 119,948\n",
      "   Duplicates removed: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:   9%|â–‰         | 616959/6990280 [00:06<02:22, 44809.56 lines/s, RAM=464MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  13%|â–ˆâ–        | 899244/6990280 [00:08<00:34, 174346.49 lines/s, RAM=466MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #3:\n",
      "   Records: 119,970\n",
      "   Duplicates removed: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  13%|â–ˆâ–        | 920197/6990280 [00:09<02:04, 48806.45 lines/s, RAM=469MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  17%|â–ˆâ–‹        | 1181264/6990280 [00:11<00:38, 151559.07 lines/s, RAM=470MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #4:\n",
      "   Records: 119,946\n",
      "   Duplicates removed: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  17%|â–ˆâ–‹        | 1200678/6990280 [00:12<02:06, 45681.18 lines/s, RAM=473MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  21%|â–ˆâ–ˆ        | 1481782/6990280 [00:14<00:34, 159545.98 lines/s, RAM=473MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #5:\n",
      "   Records: 119,954\n",
      "   Duplicates removed: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  21%|â–ˆâ–ˆâ–       | 1501861/6990280 [00:15<01:59, 46044.71 lines/s, RAM=476MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_5.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  26%|â–ˆâ–ˆâ–Œ       | 1786740/6990280 [00:17<00:32, 161191.20 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #6:\n",
      "   Records: 119,937\n",
      "   Duplicates removed: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  26%|â–ˆâ–ˆâ–Œ       | 1806996/6990280 [00:18<01:47, 48285.82 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  30%|â–ˆâ–ˆâ–‰       | 2090046/6990280 [00:20<00:29, 168835.20 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #7:\n",
      "   Records: 119,921\n",
      "   Duplicates removed: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  30%|â–ˆâ–ˆâ–ˆ       | 2110297/6990280 [00:21<01:44, 46724.41 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  34%|â–ˆâ–ˆâ–ˆâ–      | 2390581/6990280 [00:23<00:26, 173783.16 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #8:\n",
      "   Records: 119,936\n",
      "   Duplicates removed: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  34%|â–ˆâ–ˆâ–ˆâ–      | 2411398/6990280 [00:24<01:32, 49704.10 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_8.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 2692208/6990280 [00:26<00:26, 165269.77 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #9:\n",
      "   Records: 119,936\n",
      "   Duplicates removed: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 2712239/6990280 [00:27<01:32, 46122.16 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_9.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 2991897/6990280 [00:29<00:23, 170852.51 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #10:\n",
      "   Records: 119,954\n",
      "   Duplicates removed: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 3012425/6990280 [00:30<01:22, 48252.88 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 3295833/6990280 [00:32<00:21, 168257.49 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #11:\n",
      "   Records: 119,932\n",
      "   Duplicates removed: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 3315975/6990280 [00:33<01:17, 47306.17 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3591495/6990280 [00:35<00:20, 168577.44 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #12:\n",
      "   Records: 119,953\n",
      "   Duplicates removed: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 3611644/6990280 [00:36<01:13, 46094.89 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3886774/6990280 [00:38<00:19, 162323.40 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #13:\n",
      "   Records: 119,932\n",
      "   Duplicates removed: 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 3906273/6990280 [00:40<01:07, 45748.72 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_13.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 4185758/6990280 [00:41<00:16, 166140.64 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #14:\n",
      "   Records: 119,949\n",
      "   Duplicates removed: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 4205819/6990280 [00:43<01:00, 45750.66 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_14.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 4488534/6990280 [00:45<00:14, 170735.06 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #15:\n",
      "   Records: 119,921\n",
      "   Duplicates removed: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 4509053/6990280 [00:46<00:51, 48076.48 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_15.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4785703/6990280 [00:48<00:13, 165707.64 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #16:\n",
      "   Records: 119,951\n",
      "   Duplicates removed: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 4805608/6990280 [00:49<00:47, 45962.72 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_16.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5097184/6990280 [00:51<00:11, 167261.37 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #17:\n",
      "   Records: 119,948\n",
      "   Duplicates removed: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 5117130/6990280 [00:52<00:40, 45738.24 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_17.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 5397155/6990280 [00:54<00:09, 169664.76 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #18:\n",
      "   Records: 119,941\n",
      "   Duplicates removed: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 5417397/6990280 [00:55<00:33, 47636.76 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_18.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5695100/6990280 [00:57<00:07, 170011.30 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #19:\n",
      "   Records: 119,961\n",
      "   Duplicates removed: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5715639/6990280 [00:58<00:26, 47426.02 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_19.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 5979696/6990280 [01:00<00:06, 158798.24 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #20:\n",
      "   Records: 119,936\n",
      "   Duplicates removed: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 6000000/6990280 [01:01<00:20, 48022.84 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_20.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 6297068/6990280 [01:03<00:04, 166703.60 lines/s, RAM=477MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #21:\n",
      "   Records: 119,951\n",
      "   Duplicates removed: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 6316780/6990280 [01:04<00:14, 46027.72 lines/s, RAM=477MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_21.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 6578854/6990280 [01:06<00:02, 158042.08 lines/s, RAM=478MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #22:\n",
      "   Records: 119,931\n",
      "   Duplicates removed: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 6621437/6990280 [01:07<00:05, 62709.39 lines/s, RAM=478MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_22.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 6882657/6990280 [01:09<00:00, 157872.13 lines/s, RAM=478MB, Sampled=40,000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¦ Creating file #23:\n",
      "   Records: 119,923\n",
      "   Duplicates removed: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 6902446/6990280 [01:10<00:01, 46962.87 lines/s, RAM=478MB, Sampled=40,000] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Saved: processed_data/review_combined_23.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing yelp_academic_dataset_review.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6990280/6990280 [01:11<00:00, 97880.31 lines/s, RAM=478MB, Sampled=40,000] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Processed 6,990,280 records (0 errors)\n",
      "ğŸ’¾ Peak RAM: 483 MB\n",
      "\n",
      "\n",
      "ğŸ“¦ Creating final file #24:\n",
      "   Records: 36,104\n",
      "   Duplicates removed: 8\n",
      "   âœ… Saved: processed_data/review_combined_24.csv\n",
      "\n",
      "================================================================================\n",
      "âœ… Review processing complete!\n",
      "   Total sampled: 2,796,112 records\n",
      "   Null reviews removed: 0\n",
      "   Output files: 24 files\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸ“ PROCESSING REVIEW DATA (40% SAMPLE + MULTI-FILE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "accumulated_data = []\n",
    "file_counter = 1\n",
    "total_sampled = 0\n",
    "null_removed = 0\n",
    "\n",
    "for sampled_df in load_and_sample_batch(\n",
    "    FILE_PATHS['review'], \n",
    "    BATCH_SIZE, \n",
    "    SAMPLE_RATE\n",
    "):\n",
    "    # Create sentiment labels\n",
    "    sampled_df['label'] = sampled_df['stars'].apply(\n",
    "        lambda x: 0 if x <= 2 else (2 if x == 3 else 1)\n",
    "    )\n",
    "    \n",
    "    # Remove null text\n",
    "    before = len(sampled_df)\n",
    "    sampled_df = sampled_df.dropna(subset=['text'])\n",
    "    null_removed += (before - len(sampled_df))\n",
    "    \n",
    "    # Select columns\n",
    "    sampled_df = sampled_df[['text', 'label']]\n",
    "    sampled_df.columns = ['review', 'label']\n",
    "    \n",
    "    # Accumulate data\n",
    "    accumulated_data.append(sampled_df)\n",
    "    total_sampled += len(sampled_df)\n",
    "    \n",
    "    # Check if we have enough data for a file\n",
    "    total_accumulated = sum(len(df) for df in accumulated_data)\n",
    "    \n",
    "    if total_accumulated >= RECORDS_PER_FILE:\n",
    "        # Combine accumulated batches\n",
    "        combined_df = pd.concat(accumulated_data, ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates within this file\n",
    "        before_dup = len(combined_df)\n",
    "        combined_df = combined_df.drop_duplicates(subset=['review'])\n",
    "        dup_removed = before_dup - len(combined_df)\n",
    "        \n",
    "        print(f\"\\nğŸ“¦ Creating file #{file_counter}:\")\n",
    "        print(f\"   Records: {len(combined_df):,}\")\n",
    "        print(f\"   Duplicates removed: {dup_removed:,}\")\n",
    "        \n",
    "        # Save combined file\n",
    "        review_file = OUTPUT_DIR + f'review_combined_{file_counter}.csv'\n",
    "        combined_df.to_csv(review_file, index=False)\n",
    "        print(f\"   âœ… Saved: {review_file}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del combined_df\n",
    "        accumulated_data = []\n",
    "        gc.collect()\n",
    "        \n",
    "        file_counter += 1\n",
    "    \n",
    "    del sampled_df\n",
    "    gc.collect()\n",
    "\n",
    "# Process remaining data\n",
    "if accumulated_data:\n",
    "    combined_df = pd.concat(accumulated_data, ignore_index=True)\n",
    "    before_dup = len(combined_df)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['review'])\n",
    "    dup_removed = before_dup - len(combined_df)\n",
    "    \n",
    "    print(f\"\\nğŸ“¦ Creating final file #{file_counter}:\")\n",
    "    print(f\"   Records: {len(combined_df):,}\")\n",
    "    print(f\"   Duplicates removed: {dup_removed:,}\")\n",
    "    \n",
    "    review_file = OUTPUT_DIR + f'review_combined_{file_counter}.csv'\n",
    "    combined_df.to_csv(review_file, index=False)\n",
    "    print(f\"   âœ… Saved: {review_file}\")\n",
    "    \n",
    "    del combined_df\n",
    "    gc.collect()\n",
    "\n",
    "num_review_files = file_counter\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… Review processing complete!\")\n",
    "print(f\"   Total sampled: {total_sampled:,} records\")\n",
    "print(f\"   Null reviews removed: {null_removed:,}\")\n",
    "print(f\"   Output files: {num_review_files} files\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split cho tá»«ng file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "âœ‚ï¸ TRAIN/TEST SPLIT (STRATIFIED)\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‚ Processing file 1/24...\n",
      "   Loaded: 119,938 records\n",
      "   âœ… Train: 95,950 â†’ processed_data/train_part1.csv\n",
      "   âœ… Test: 23,988 â†’ processed_data/test_part1.csv\n",
      "\n",
      "ğŸ“‚ Processing file 2/24...\n",
      "   Loaded: 119,948 records\n",
      "   âœ… Train: 95,958 â†’ processed_data/train_part2.csv\n",
      "   âœ… Test: 23,990 â†’ processed_data/test_part2.csv\n",
      "\n",
      "ğŸ“‚ Processing file 3/24...\n",
      "   Loaded: 119,970 records\n",
      "   âœ… Train: 95,976 â†’ processed_data/train_part3.csv\n",
      "   âœ… Test: 23,994 â†’ processed_data/test_part3.csv\n",
      "\n",
      "ğŸ“‚ Processing file 4/24...\n",
      "   Loaded: 119,946 records\n",
      "   âœ… Train: 95,956 â†’ processed_data/train_part4.csv\n",
      "   âœ… Test: 23,990 â†’ processed_data/test_part4.csv\n",
      "\n",
      "ğŸ“‚ Processing file 5/24...\n",
      "   Loaded: 119,954 records\n",
      "   âœ… Train: 95,963 â†’ processed_data/train_part5.csv\n",
      "   âœ… Test: 23,991 â†’ processed_data/test_part5.csv\n",
      "\n",
      "ğŸ“‚ Processing file 6/24...\n",
      "   Loaded: 119,937 records\n",
      "   âœ… Train: 95,949 â†’ processed_data/train_part6.csv\n",
      "   âœ… Test: 23,988 â†’ processed_data/test_part6.csv\n",
      "\n",
      "ğŸ“‚ Processing file 7/24...\n",
      "   Loaded: 119,921 records\n",
      "   âœ… Train: 95,936 â†’ processed_data/train_part7.csv\n",
      "   âœ… Test: 23,985 â†’ processed_data/test_part7.csv\n",
      "\n",
      "ğŸ“‚ Processing file 8/24...\n",
      "   Loaded: 119,936 records\n",
      "   âœ… Train: 95,948 â†’ processed_data/train_part8.csv\n",
      "   âœ… Test: 23,988 â†’ processed_data/test_part8.csv\n",
      "\n",
      "ğŸ“‚ Processing file 9/24...\n",
      "   Loaded: 119,936 records\n",
      "   âœ… Train: 95,948 â†’ processed_data/train_part9.csv\n",
      "   âœ… Test: 23,988 â†’ processed_data/test_part9.csv\n",
      "\n",
      "ğŸ“‚ Processing file 10/24...\n",
      "   Loaded: 119,954 records\n",
      "   âœ… Train: 95,963 â†’ processed_data/train_part10.csv\n",
      "   âœ… Test: 23,991 â†’ processed_data/test_part10.csv\n",
      "\n",
      "ğŸ“‚ Processing file 11/24...\n",
      "   Loaded: 119,932 records\n",
      "   âœ… Train: 95,945 â†’ processed_data/train_part11.csv\n",
      "   âœ… Test: 23,987 â†’ processed_data/test_part11.csv\n",
      "\n",
      "ğŸ“‚ Processing file 12/24...\n",
      "   Loaded: 119,953 records\n",
      "   âœ… Train: 95,962 â†’ processed_data/train_part12.csv\n",
      "   âœ… Test: 23,991 â†’ processed_data/test_part12.csv\n",
      "\n",
      "ğŸ“‚ Processing file 13/24...\n",
      "   Loaded: 119,932 records\n",
      "   âœ… Train: 95,945 â†’ processed_data/train_part13.csv\n",
      "   âœ… Test: 23,987 â†’ processed_data/test_part13.csv\n",
      "\n",
      "ğŸ“‚ Processing file 14/24...\n",
      "   Loaded: 119,949 records\n",
      "   âœ… Train: 95,959 â†’ processed_data/train_part14.csv\n",
      "   âœ… Test: 23,990 â†’ processed_data/test_part14.csv\n",
      "\n",
      "ğŸ“‚ Processing file 15/24...\n",
      "   Loaded: 119,921 records\n",
      "   âœ… Train: 95,936 â†’ processed_data/train_part15.csv\n",
      "   âœ… Test: 23,985 â†’ processed_data/test_part15.csv\n",
      "\n",
      "ğŸ“‚ Processing file 16/24...\n",
      "   Loaded: 119,951 records\n",
      "   âœ… Train: 95,960 â†’ processed_data/train_part16.csv\n",
      "   âœ… Test: 23,991 â†’ processed_data/test_part16.csv\n",
      "\n",
      "ğŸ“‚ Processing file 17/24...\n",
      "   Loaded: 119,948 records\n",
      "   âœ… Train: 95,958 â†’ processed_data/train_part17.csv\n",
      "   âœ… Test: 23,990 â†’ processed_data/test_part17.csv\n",
      "\n",
      "ğŸ“‚ Processing file 18/24...\n",
      "   Loaded: 119,941 records\n",
      "   âœ… Train: 95,952 â†’ processed_data/train_part18.csv\n",
      "   âœ… Test: 23,989 â†’ processed_data/test_part18.csv\n",
      "\n",
      "ğŸ“‚ Processing file 19/24...\n",
      "   Loaded: 119,961 records\n",
      "   âœ… Train: 95,968 â†’ processed_data/train_part19.csv\n",
      "   âœ… Test: 23,993 â†’ processed_data/test_part19.csv\n",
      "\n",
      "ğŸ“‚ Processing file 20/24...\n",
      "   Loaded: 119,936 records\n",
      "   âœ… Train: 95,948 â†’ processed_data/train_part20.csv\n",
      "   âœ… Test: 23,988 â†’ processed_data/test_part20.csv\n",
      "\n",
      "ğŸ“‚ Processing file 21/24...\n",
      "   Loaded: 119,951 records\n",
      "   âœ… Train: 95,960 â†’ processed_data/train_part21.csv\n",
      "   âœ… Test: 23,991 â†’ processed_data/test_part21.csv\n",
      "\n",
      "ğŸ“‚ Processing file 22/24...\n",
      "   Loaded: 119,931 records\n",
      "   âœ… Train: 95,944 â†’ processed_data/train_part22.csv\n",
      "   âœ… Test: 23,987 â†’ processed_data/test_part22.csv\n",
      "\n",
      "ğŸ“‚ Processing file 23/24...\n",
      "   Loaded: 119,923 records\n",
      "   âœ… Train: 95,938 â†’ processed_data/train_part23.csv\n",
      "   âœ… Test: 23,985 â†’ processed_data/test_part23.csv\n",
      "\n",
      "ğŸ“‚ Processing file 24/24...\n",
      "   Loaded: 36,104 records\n",
      "   âœ… Train: 28,883 â†’ processed_data/train_part24.csv\n",
      "   âœ… Test: 7,221 â†’ processed_data/test_part24.csv\n",
      "\n",
      "================================================================================\n",
      "âœ… Split complete!\n",
      "   Total train: 2,235,805 records\n",
      "   Total test: 558,968 records\n",
      "   Train files: 24\n",
      "   Test files: 24\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"âœ‚ï¸ TRAIN/TEST SPLIT (STRATIFIED)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_train = 0\n",
    "total_test = 0\n",
    "\n",
    "for i in range(1, num_review_files + 1):\n",
    "    review_file = OUTPUT_DIR + f'review_combined_{i}.csv'\n",
    "    \n",
    "    print(f\"\\nğŸ“‚ Processing file {i}/{num_review_files}...\")\n",
    "    \n",
    "    # Load file\n",
    "    df = pd.read_csv(review_file)\n",
    "    print(f\"   Loaded: {len(df):,} records\")\n",
    "    \n",
    "    # Split with stratify\n",
    "    train_df, test_df = train_test_split(\n",
    "        df,\n",
    "        test_size=TEST_RATIO,\n",
    "        random_state=RANDOM_STATE,\n",
    "        stratify=df['label']\n",
    "    )\n",
    "    \n",
    "    # Save train\n",
    "    train_file = OUTPUT_DIR + f'train_part{i}.csv'\n",
    "    train_df.to_csv(train_file, index=False)\n",
    "    print(f\"   âœ… Train: {len(train_df):,} â†’ {train_file}\")\n",
    "    \n",
    "    # Save test\n",
    "    test_file = OUTPUT_DIR + f'test_part{i}.csv'\n",
    "    test_df.to_csv(test_file, index=False)\n",
    "    print(f\"   âœ… Test: {len(test_df):,} â†’ {test_file}\")\n",
    "    \n",
    "    total_train += len(train_df)\n",
    "    total_test += len(test_df)\n",
    "    \n",
    "    # Clean memory\n",
    "    del df, train_df, test_df\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ… Split complete!\")\n",
    "print(f\"   Total train: {total_train:,} records\")\n",
    "print(f\"   Total test: {total_test:,} records\")\n",
    "print(f\"   Train files: {num_review_files}\")\n",
    "print(f\"   Test files: {num_review_files}\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š FINAL SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Output Files:\n",
      "   Business: 1 file(s)\n",
      "   User: 1 file(s)\n",
      "   Review combined: 24 file(s)\n",
      "   Train: 24 file(s)\n",
      "   Test: 24 file(s)\n",
      "\n",
      "ğŸ’¾ Total output size: 6367.9 MB\n",
      "\n",
      "ğŸ“Š Record Counts:\n",
      "   Business: 60,138\n",
      "   User: 795,158\n",
      "   Train (total): 2,235,805\n",
      "   Test (total): 558,968\n",
      "\n",
      "âš™ï¸ Processing Settings:\n",
      "   Sample rate: 40%\n",
      "   Batch size: 100,000\n",
      "   Records per file: ~100,000\n",
      "\n",
      "ğŸ’¾ Peak RAM usage: 478 MB\n",
      "ğŸ“… Completed: 2025-10-12 15:40:01\n",
      "\n",
      "================================================================================\n",
      "ğŸ‰ PROCESSING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š How to use:\n",
      "   1. Train files: train_part1.csv, train_part2.csv, ...\n",
      "   2. Test files: test_part1.csv, test_part2.csv, ...\n",
      "   3. Load multiple files in training loop\n",
      "   4. Labels: 0=TiÃªu cá»±c, 1=TÃ­ch cá»±c, 2=Trung láº­p\n",
      "\n",
      "âœ… Summary saved to: processed_data/processing_summary.txt\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ğŸ“Š FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# List all files\n",
    "all_files = os.listdir(OUTPUT_DIR)\n",
    "business_files = [f for f in all_files if f.startswith('business')]\n",
    "user_files = [f for f in all_files if f.startswith('user')]\n",
    "review_files = [f for f in all_files if f.startswith('review_combined')]\n",
    "train_files = [f for f in all_files if f.startswith('train_part')]\n",
    "test_files = [f for f in all_files if f.startswith('test_part')]\n",
    "\n",
    "print(f\"\\nğŸ“ Output Files:\")\n",
    "print(f\"   Business: {len(business_files)} file(s)\")\n",
    "print(f\"   User: {len(user_files)} file(s)\")\n",
    "print(f\"   Review combined: {len(review_files)} file(s)\")\n",
    "print(f\"   Train: {len(train_files)} file(s)\")\n",
    "print(f\"   Test: {len(test_files)} file(s)\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size = 0\n",
    "for f in all_files:\n",
    "    filepath = OUTPUT_DIR + f\n",
    "    if os.path.isfile(filepath):\n",
    "        total_size += os.path.getsize(filepath)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Total output size: {total_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Record Counts:\")\n",
    "print(f\"   Business: {pd.read_csv(OUTPUT_DIR + 'business.csv').shape[0]:,}\")\n",
    "print(f\"   User: {pd.read_csv(OUTPUT_DIR + 'user.csv').shape[0]:,}\")\n",
    "print(f\"   Train (total): {total_train:,}\")\n",
    "print(f\"   Test (total): {total_test:,}\")\n",
    "\n",
    "print(f\"\\nâš™ï¸ Processing Settings:\")\n",
    "print(f\"   Sample rate: {SAMPLE_RATE:.0%}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE:,}\")\n",
    "print(f\"   Records per file: ~{RECORDS_PER_FILE:,}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Peak RAM usage: {get_memory_mb():.0f} MB\")\n",
    "print(f\"ğŸ“… Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ PROCESSING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ“š How to use:\")\n",
    "print(f\"   1. Train files: train_part1.csv, train_part2.csv, ...\")\n",
    "print(f\"   2. Test files: test_part1.csv, test_part2.csv, ...\")\n",
    "print(f\"   3. Load multiple files in training loop\")\n",
    "print(f\"   4. Labels: 0=TiÃªu cá»±c, 1=TÃ­ch cá»±c, 2=Trung láº­p\")\n",
    "\n",
    "# Save summary to file\n",
    "summary_file = OUTPUT_DIR + 'processing_summary.txt'\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"YELP DATASET PROCESSING SUMMARY\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(f\"Business files: {len(business_files)}\\n\")\n",
    "    f.write(f\"User files: {len(user_files)}\\n\")\n",
    "    f.write(f\"Review combined files: {len(review_files)}\\n\")\n",
    "    f.write(f\"Train files: {len(train_files)}\\n\")\n",
    "    f.write(f\"Test files: {len(test_files)}\\n\\n\")\n",
    "    f.write(f\"Total train records: {total_train:,}\\n\")\n",
    "    f.write(f\"Total test records: {total_test:,}\\n\\n\")\n",
    "    f.write(f\"Sample rate: {SAMPLE_RATE:.0%}\\n\")\n",
    "    f.write(f\"Total size: {total_size / 1024 / 1024:.1f} MB\\n\")\n",
    "\n",
    "print(f\"\\nâœ… Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ PREVIEW: train_part1.csv\n",
      "\n",
      "                                              review  label\n",
      "0  I stopped by CafÃ© Beignet (bourbon street loca...      1\n",
      "1  First a disclaimer:  I only speak of one cab r...      1\n",
      "2  Wonderful brunch option in the Funk Zone. Food...      1\n",
      "3  I haven't been downtown for a while, but happe...      1\n",
      "4  Good as it gets for bbq in Tampa.  Service is ...      1\n",
      "5  I went to V Vegaz because they had Diva-Curls ...      1\n",
      "6  The food is 5 stars, but the service is 1. If ...      0\n",
      "7  Good food. Menu was too busy and they did not ...      2\n",
      "8  Nice late night food and pub. Wish the variety...      2\n",
      "9  Moonshine me up, baby!\\n\\nStopped in with my f...      2\n",
      "\n",
      "ğŸ“Š Label distribution in train_part1:\n",
      "label\n",
      "0    18398\n",
      "1    66550\n",
      "2    11002\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preview first train file\n",
    "print(\"ğŸ“‹ PREVIEW: train_part1.csv\\n\")\n",
    "df_preview = pd.read_csv(OUTPUT_DIR + 'train_part1.csv')\n",
    "print(df_preview.head(10))\n",
    "\n",
    "print(f\"\\nğŸ“Š Label distribution in train_part1:\")\n",
    "print(df_preview['label'].value_counts().sort_index())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
