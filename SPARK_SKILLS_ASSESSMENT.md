# ğŸ“Š ÄÃNH GIÃ Ká»¸ NÄ‚NG SPARK - Dá»° ÃN YELP ANALYSIS

**Má»¥c Ä‘Ã­ch:** ÄÃ¡nh giÃ¡ dá»± Ã¡n theo yÃªu cáº§u ká»¹ nÄƒng Spark má»©c trung cáº¥p vÃ  Ä‘á» xuáº¥t phÆ°Æ¡ng Ã¡n bá»• sung.

**NgÃ y Ä‘Ã¡nh giÃ¡:** 2025-12-15

---

## ğŸ“‹ Má»¤C Lá»¤C

1. [Tá»•ng quan](#1-tá»•ng-quan)
2. [Chi tiáº¿t Ä‘Ã¡nh giÃ¡](#2-chi-tiáº¿t-Ä‘Ã¡nh-giÃ¡)
3. [Ká»¹ nÄƒng cÃ²n thiáº¿u](#3-ká»¹-nÄƒng-cÃ²n-thiáº¿u)
4. [PhÆ°Æ¡ng Ã¡n bá»• sung](#4-phÆ°Æ¡ng-Ã¡n-bá»•-sung)
5. [Roadmap triá»ƒn khai](#5-roadmap-triá»ƒn-khai)

---

## 1. Tá»”NG QUAN

### âœ… Äiá»ƒm máº¡nh hiá»‡n táº¡i:
- CÃ³ 2 phiÃªn báº£n: Streaming (nhÃ¡nh HAI) vÃ  Batch (Spark_Batch)
- 7 hÃ m phÃ¢n tÃ­ch hoÃ n chá»‰nh
- Code structure tá»‘t, modular
- CÃ³ documentation Ä‘áº§y Ä‘á»§

### âš ï¸ Äiá»ƒm yáº¿u:
- Thiáº¿u nhiá»u ká»¹ thuáº­t Spark nÃ¢ng cao
- ChÆ°a cÃ³ Window Functions, Pivot/Unpivot
- ChÆ°a cÃ³ UDF tÃ¹y chá»‰nh
- ChÆ°a cÃ³ Broadcast Join optimization rÃµ rÃ ng
- ChÆ°a cÃ³ Machine Learning

---

## 2. CHI TIáº¾T ÄÃNH GIÃ

### ğŸ“Š 2.1. Táº¬P Há»¢P PHá»¨C Táº P (Complex Aggregations)

#### âœ… ÄÃƒ CÃ“:

**HÃ m tá»•ng há»£p cÆ¡ báº£n:**
```python
# File: batch_analytics.py, dÃ²ng 42-45
business_stats = salted_agg.groupBy("business_id").agg(
    sum("partial_count").alias("recent_review_count"),
    (sum("partial_sum_stars") / sum("partial_count_stars")).alias("avg_rating")
)
```

**Tá»•ng há»£p vá»›i Ä‘iá»u kiá»‡n (Conditional Aggregation):**
```python
# File: batch_analytics.py, dÃ²ng 172-187
review_stats = review_df.groupBy("business_id").agg(
    sum(when(col("stars") >= positive_threshold, 1).otherwise(0))
        .alias("positive_review_count"),
    count("review_id").alias("total_review_count"),
    avg(when(col("stars") >= positive_threshold, col("stars")))
        .alias("avg_positive_rating"),
    sum(when(col("stars") >= positive_threshold, col("useful")).otherwise(0))
        .alias("total_useful_votes")
)
```

**Multi-stage aggregation (Salted):**
```python
# File: batch_analytics.py, dÃ²ng 35-45
# Stage 1: Salted aggregation to avoid skew
salted_agg = recent_reviews.groupBy("business_id", "salt").agg(...)
# Stage 2: Final aggregation
business_stats = salted_agg.groupBy("business_id").agg(...)
```

#### âŒ CHÆ¯A CÃ“:

1. **Window Functions** - THIáº¾U HOÃ€N TOÃ€N
   - `row_number()`, `rank()`, `dense_rank()`
   - `lag()`, `lead()` cho time series
   - `cumsum()`, `cummean()` cho cumulative metrics
   - Partitioning vÃ  ordering trong window

2. **Pivot/Unpivot Operations** - THIáº¾U
   - Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u tá»« long â†’ wide format
   - PhÃ¢n tÃ­ch cross-tabulation
   - Dynamic pivoting

3. **Custom Aggregate Functions** - THIáº¾U
   - UDAF (User Defined Aggregate Functions)
   - Tá»•ng há»£p phá»©c táº¡p vá»›i logic tÃ¹y chá»‰nh

**Má»©c Ä‘á»™ Ä‘Ã¡p á»©ng:** 40% â­â­

---

### ğŸ”„ 2.2. BIáº¾N Äá»”I NÃ‚NG CAO (Advanced Transformations)

#### âœ… ÄÃƒ CÃ“:

**Multi-stage transformations:**
```python
# File: batch_analytics.py, dÃ²ng 24-45
review_with_salt = review_df.withColumn("salt", (rand() * 10).cast("int"))
review_with_salt = review_with_salt.withColumn('date_parsed', to_timestamp(...))
recent_reviews = review_with_salt.filter(col("date_parsed") >= cutoff_date)
salted_agg = recent_reviews.groupBy(...).agg(...)
business_stats = salted_agg.groupBy(...).agg(...)
```

**Complex operations:**
```python
# File: batch_analytics.py, dÃ²ng 90-92
result = business_filtered.withColumn(
    "category_count",
    size(split(trim(col("categories")), "\\s*,\\s*"))
)
```

#### âŒ CHÆ¯A CÃ“:

1. **UDF (User Defined Functions)** - THIáº¾U
   - Pandas UDF (vectorized)
   - Custom business logic functions
   - Text processing UDF

2. **Complex chaining** - CÃ“ NHÆ¯NG ÃT
   - Hiá»‡n táº¡i chá»‰ cÃ³ 2-3 stages
   - ChÆ°a cÃ³ pipeline phá»©c táº¡p > 5 stages

3. **Data quality transformations** - THIáº¾U
   - Outlier detection vÃ  handling
   - Missing value imputation
   - Data validation logic

**Má»©c Ä‘á»™ Ä‘Ã¡p á»©ng:** 50% â­â­â­

---

### ğŸ”— 2.3. JOIN OPERATIONS

#### âœ… ÄÃƒ CÃ“:

**Basic joins:**
```python
# File: batch_analytics.py, dÃ²ng 51-53
result = top_candidates.join(
    business_df.select("business_id", "name", "city", "state", "categories"),
    "business_id"
)
```

**Join optimization (limit before join):**
```python
# File: batch_analytics.py, dÃ²ng 48
top_candidates = business_stats.orderBy(desc("recent_review_count")).limit(top_n * 10)
# Sau Ä‘Ã³ má»›i join
```

#### âŒ CHÆ¯A CÃ“:

1. **Broadcast Join** - CHÆ¯A RÃ• RÃ€NG
   - Code comment cÃ³ Ä‘á» cáº­p "Broadcast join" nhÆ°ng khÃ´ng tháº¥y `broadcast()` function
   - Cáº§n explicit `from pyspark.sql.functions import broadcast`

2. **Sort-Merge Join optimization** - THIáº¾U
   - ChÆ°a cÃ³ repartition by join key
   - ChÆ°a cÃ³ bucketing strategy

3. **Multiple join optimization** - THIáº¾U
   - ChÆ°a cÃ³ phÃ¢n tÃ­ch vÃ  tá»‘i Æ°u multiple joins
   - ChÆ°a cÃ³ join reordering strategy

4. **Skewed join handling** - CÃ“ NHÆ¯NG ÃT
   - CÃ³ salting cho aggregation
   - ChÆ°a cÃ³ salting cho skewed joins

**Má»©c Ä‘á»™ Ä‘Ã¡p á»©ng:** 40% â­â­

---

### âš¡ 2.4. Tá»I Æ¯U HÃ“A HIá»†U NÄ‚NG

#### âœ… ÄÃƒ CÃ“:

**Caching:**
```python
# File: batch_load_data.py, dÃ²ng 43, 66
business_df.cache()
review_df.cache()
```

**Data skew handling (Salting):**
```python
# File: batch_analytics.py, dÃ²ng 25
review_with_salt = review_df.withColumn("salt", (rand() * 10).cast("int"))
```

**Early filtering:**
```python
# File: batch_analytics.py, dÃ²ng 122
business_stats = review_df.filter(col("stars").isNotNull()).groupBy(...)
```

**Column pruning:**
```python
# File: batch_analytics.py, dÃ²ng 52
business_df.select("business_id", "name", "city", "state", "categories")
```

#### âŒ CHÆ¯A CÃ“:

1. **Partitioning strategy** - THIáº¾U CHI TIáº¾T
   - Config cÃ³ `spark.sql.shuffle.partitions` nhÆ°ng chÆ°a dynamic
   - ChÆ°a cÃ³ repartition() theo business logic
   - ChÆ°a cÃ³ coalesce() khi cáº§n

2. **Persistence strategy** - CÆ  Báº¢N
   - Chá»‰ dÃ¹ng `cache()`, chÆ°a cÃ³ `persist(StorageLevel.xxx)`
   - ChÆ°a cÃ³ memory vs disk tradeoff

3. **Query optimization** - THIáº¾U
   - ChÆ°a cÃ³ `explain()` analysis
   - ChÆ°a cÃ³ cost-based optimization tuning
   - ChÆ°a cÃ³ adaptive query execution monitoring

4. **Pruning** - CÆ  Báº¢N
   - CÃ³ column pruning
   - ChÆ°a cÃ³ partition pruning (vÃ¬ khÃ´ng cÃ³ partitioned data)

**Má»©c Ä‘á»™ Ä‘Ã¡p á»©ng:** 50% â­â­â­

---

### ğŸŒŠ 2.5. Xá»¬ LÃ STREAMING

#### âœ… ÄÃƒ CÃ“ (NhÃ¡nh HAI):

**Structured Streaming:**
```python
# File: Spark/load_data.py (nhÃ¡nh HAI)
business_df = (
    self.spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", self.kafka_broker)
        .option("subscribe", "business")
        .option("startingOffsets", "earliest")
        .load()
)
```

**Watermarking:**
```python
# File: Spark/load_data.py (nhÃ¡nh HAI), dÃ²ng 37
.withWatermark('business_ts', '10 minutes')
```

**Multiple output modes:**
```python
# File: Spark/pipeline_orchestration.py (nhÃ¡nh HAI)
# - append mode cho HDFS
# - foreachBatch cho Elasticsearch
```

**Checkpointing:**
```python
# File: Spark/configuration.py (nhÃ¡nh HAI), dÃ²ng 70
spark.sparkContext.setCheckpointDir(os.path.abspath("checkpoints"))
```

#### âŒ CHÆ¯A CÃ“:

1. **State management** - CÆ  Báº¢N
   - CÃ³ watermarking nhÆ°ng chÆ°a rÃµ state store strategy
   - ChÆ°a cÃ³ stateful operations vá»›i `mapGroupsWithState`

2. **Exactly-once processing** - CHÆ¯A RÃ•
   - ChÆ°a cÃ³ idempotent writes
   - ChÆ°a cÃ³ transaction handling

3. **Late data handling** - CÆ  Báº¢N
   - CÃ³ watermark 10 minutes
   - ChÆ°a cÃ³ complex late data policy

4. **Multiple streaming queries coordination** - THIáº¾U
   - ChÆ°a cÃ³ query monitoring vÃ  coordination

**Má»©c Ä‘á»™ Ä‘Ã¡p á»©ng (NhÃ¡nh HAI):** 60% â­â­â­
**Má»©c Ä‘á»™ Ä‘Ã¡p á»©ng (Spark_Batch):** 0% (khÃ´ng cÃ³ streaming)

---

### ğŸ¤– 2.6. PHÃ‚N TÃCH NÃ‚NG CAO

#### âœ… ÄÃƒ CÃ“:

**Statistical computation (basic):**
```python
# avg, sum, count, stddev (cÃ³ sá»­ dá»¥ng)
avg("stars"), count("review_id"), sum("useful")
```

**Time series analysis (basic):**
```python
# File: batch_analytics.py, dÃ²ng 244-251
result = df.groupBy(
    year("date_parsed").alias("year"),
    month("date_parsed").alias("month")
).agg(count("review_id").alias("review_count"))
```

#### âŒ CHÆ¯A CÃ“:

1. **Machine Learning vá»›i MLlib** - THIáº¾U HOÃ€N TOÃ€N
   - NhÃ¡nh HAI cÃ³ model TF-IDF sentiment nhÆ°ng khÃ´ng tháº¥y code training
   - ChÆ°a cÃ³ feature engineering
   - ChÆ°a cÃ³ model evaluation
   - ChÆ°a cÃ³ pipeline ML

2. **Graph processing vá»›i GraphFrames** - THIáº¾U
   - ChÆ°a cÃ³ social network analysis
   - ChÆ°a cÃ³ user-business graph
   - ChÆ°a cÃ³ PageRank, Community Detection

3. **Advanced statistics** - THIáº¾U
   - ChÆ°a cÃ³ correlation analysis
   - ChÆ°a cÃ³ hypothesis testing
   - ChÆ°a cÃ³ anomaly detection

4. **Time series advanced** - THIáº¾U
   - ChÆ°a cÃ³ trend analysis
   - ChÆ°a cÃ³ seasonality detection
   - ChÆ°a cÃ³ forecasting

**Má»©c Ä‘á»™ Ä‘Ã¡p á»©ng:** 10% â­

---

## 3. Ká»¸ NÄ‚NG CÃ’N THIáº¾U

### ğŸ”´ Æ¯u tiÃªn CAO (Critical):

| # | Ká»¹ nÄƒng thiáº¿u | LÃ½ do quan trá»ng | Äá»™ khÃ³ |
|---|--------------|------------------|--------|
| 1 | **Window Functions** | Cáº§n thiáº¿t cho ranking, moving averages | Trung bÃ¬nh |
| 2 | **Broadcast Join (explicit)** | Tá»‘i Æ°u joins vá»›i small tables | Dá»… |
| 3 | **UDF/Pandas UDF** | Custom business logic | Trung bÃ¬nh |
| 4 | **Machine Learning Pipeline** | YÃªu cáº§u cá»‘t lÃµi cho advanced analytics | KhÃ³ |

### ğŸŸ¡ Æ¯u tiÃªn TRUNG BÃŒNH (Important):

| # | Ká»¹ nÄƒng thiáº¿u | LÃ½ do quan trá»ng | Äá»™ khÃ³ |
|---|--------------|------------------|--------|
| 5 | **Pivot/Unpivot** | Data transformation cho reporting | Dá»… |
| 6 | **Custom UDAF** | Tá»•ng há»£p phá»©c táº¡p | KhÃ³ |
| 7 | **Advanced partitioning** | Performance optimization | Trung bÃ¬nh |
| 8 | **Graph processing** | Social network insights | KhÃ³ |

### ğŸŸ¢ Æ¯u tiÃªn THáº¤P (Nice to have):

| # | Ká»¹ nÄƒng thiáº¿u | LÃ½ do | Äá»™ khÃ³ |
|---|--------------|-------|--------|
| 9 | **Advanced time series** | Forecasting | KhÃ³ |
| 10 | **Streaming state management** | Advanced streaming | KhÃ³ |

---

## 4. PHÆ¯Æ NG ÃN Bá»” SUNG

### ğŸ¯ PhÆ°Æ¡ng Ã¡n 1: Bá»” SUNG NHANH (2-3 ngÃ y)

**Má»¥c tiÃªu:** Bá»• sung cÃ¡c ká»¹ nÄƒng dá»… vÃ  cÃ³ tÃ¡c Ä‘á»™ng lá»›n

#### 4.1.1. ThÃªm Window Functions (1 ngÃ y)

**Analysis má»›i: Analysis 8 - Trending Businesses**

```python
@staticmethod
def trending_businesses(business_df, review_df, window_days=30):
    """
    8. Trending Businesses Analysis
    Find businesses with increasing review trends using window functions
    """
    from pyspark.sql.window import Window

    # Parse review dates
    df = review_df.withColumn(
        "review_date",
        to_date(col("date"), "yyyy-MM-dd HH:mm:ss")
    )

    # Group by business and week
    weekly_reviews = df.groupBy(
        "business_id",
        window("review_date", "7 days").alias("week")
    ).agg(
        count("review_id").alias("weekly_count")
    ).select(
        "business_id",
        col("week.start").alias("week_start"),
        "weekly_count"
    )

    # Define window: partition by business, order by week
    windowSpec = Window.partitionBy("business_id").orderBy("week_start")

    # Calculate trend metrics
    trending = weekly_reviews.withColumn(
        "prev_week_count", lag("weekly_count", 1).over(windowSpec)
    ).withColumn(
        "growth_rate",
        when(col("prev_week_count") > 0,
             (col("weekly_count") - col("prev_week_count")) / col("prev_week_count")
        ).otherwise(0)
    ).withColumn(
        "avg_last_4_weeks",
        avg("weekly_count").over(
            windowSpec.rowsBetween(-3, 0)
        )
    ).withColumn(
        "rank_this_week",
        dense_rank().over(
            Window.orderBy(desc("weekly_count"))
        )
    )

    # Filter recent and high-growth businesses
    result = trending.filter(
        col("week_start") >= date_sub(current_date(), window_days)
    ).filter(
        col("growth_rate") > 0.2  # 20% growth
    ).join(
        business_df.select("business_id", "name", "city", "categories"),
        "business_id"
    ).select(
        "business_id",
        "name",
        "city",
        "week_start",
        "weekly_count",
        "growth_rate",
        "avg_last_4_weeks",
        "rank_this_week"
    ).orderBy(desc("growth_rate"))

    return result
```

**Ká»¹ nÄƒng thá»ƒ hiá»‡n:**
- âœ… `lag()`, `lead()` - window functions
- âœ… `avg()` over window - moving average
- âœ… `dense_rank()` - ranking
- âœ… `partitionBy` + `orderBy` - window specification
- âœ… `rowsBetween()` - frame specification

---

#### 4.1.2. ThÃªm Broadcast Join Explicit (0.5 ngÃ y)

**Sá»­a láº¡i cÃ¡c hÃ m hiá»‡n táº¡i:**

```python
from pyspark.sql.functions import broadcast

# BEFORE (implicit broadcast)
result = top_candidates.join(
    business_df.select("business_id", "name", "city"),
    "business_id"
)

# AFTER (explicit broadcast)
result = top_candidates.join(
    broadcast(
        business_df.select("business_id", "name", "city")
    ),
    "business_id"
)
```

**ThÃªm analysis má»›i vá»›i skewed join:**

```python
@staticmethod
def handle_skewed_join_example(review_df, business_df):
    """
    Example: Handle skewed join with salting
    Some businesses have millions of reviews (skewed)
    """
    # Add salt to skewed side
    review_salted = review_df.withColumn(
        "salt", (rand() * 10).cast("int")
    ).withColumn(
        "business_id_salted",
        concat(col("business_id"), lit("_"), col("salt"))
    )

    # Replicate small side
    from pyspark.sql.functions import explode, array
    business_replicated = business_df.withColumn(
        "salt", explode(array([lit(i) for i in range(10)]))
    ).withColumn(
        "business_id_salted",
        concat(col("business_id"), lit("_"), col("salt"))
    )

    # Join on salted key
    result = review_salted.join(
        business_replicated,
        "business_id_salted"
    )

    return result
```

---

#### 4.1.3. ThÃªm Pivot/Unpivot (0.5 ngÃ y)

**Analysis má»›i: Analysis 9 - Category Performance Matrix**

```python
@staticmethod
def category_performance_matrix(business_df, review_df):
    """
    9. Category Performance Matrix
    Pivot analysis: categories vs cities
    """
    # Explode categories
    df = business_df.withColumn(
        "category",
        explode(split(col("categories"), ",\\s*"))
    )

    # Join with reviews
    joined = df.join(review_df, "business_id")

    # Aggregate by category and city
    agg_df = joined.groupBy("category", "city").agg(
        avg("stars").alias("avg_stars"),
        count("review_id").alias("review_count")
    )

    # Pivot: categories as rows, cities as columns
    pivoted = agg_df.groupBy("category").pivot("city").agg(
        first("avg_stars").alias("avg_stars"),
        first("review_count").alias("count")
    )

    return pivoted
```

**Unpivot example:**

```python
@staticmethod
def unpivot_example(pivoted_df):
    """
    Unpivot: wide format â†’ long format
    """
    from pyspark.sql.functions import expr, stack

    # Get city columns (all except category)
    city_cols = [c for c in pivoted_df.columns if c != 'category']

    # Stack columns
    unpivoted = pivoted_df.select(
        "category",
        expr(f"stack({len(city_cols)}, " +
             ", ".join([f"'{c}', `{c}`" for c in city_cols]) +
             ") as (city, avg_stars)")
    )

    return unpivoted
```

---

#### 4.1.4. ThÃªm UDF (1 ngÃ y)

**Regular UDF:**

```python
from pyspark.sql.types import StringType, FloatType
from pyspark.sql.functions import udf

@udf(returnType=StringType())
def categorize_rating(stars):
    """Custom UDF: Categorize rating"""
    if stars >= 4.5:
        return "Excellent"
    elif stars >= 3.5:
        return "Good"
    elif stars >= 2.5:
        return "Average"
    else:
        return "Poor"

# Usage
df = review_df.withColumn(
    "rating_category",
    categorize_rating(col("stars"))
)
```

**Pandas UDF (Vectorized):**

```python
from pyspark.sql.functions import pandas_udf
import pandas as pd

@pandas_udf(FloatType())
def sentiment_score(text: pd.Series) -> pd.Series:
    """
    Pandas UDF: Calculate sentiment score
    (Much faster than regular UDF)
    """
    # Simple sentiment: count positive/negative words
    positive_words = ['great', 'excellent', 'amazing', 'love', 'best']
    negative_words = ['bad', 'terrible', 'worst', 'hate', 'awful']

    def score(t):
        if pd.isna(t):
            return 0.0
        t_lower = t.lower()
        pos_count = sum(t_lower.count(w) for w in positive_words)
        neg_count = sum(t_lower.count(w) for w in negative_words)
        total = pos_count + neg_count
        if total == 0:
            return 0.5
        return pos_count / total

    return text.apply(score)

# Usage
df = review_df.withColumn(
    "sentiment_score",
    sentiment_score(col("text"))
)
```

**Tá»•ng thá»i gian PhÆ°Æ¡ng Ã¡n 1: 2-3 ngÃ y**

---

### ğŸš€ PhÆ°Æ¡ng Ã¡n 2: Bá»” SUNG Äáº¦Y Äá»¦ (1-2 tuáº§n)

**Bao gá»“m PhÆ°Æ¡ng Ã¡n 1 + Machine Learning + Graph Processing**

#### 4.2.1. Machine Learning Pipeline (3-4 ngÃ y)

**Analysis má»›i: Analysis 10 - Review Sentiment Prediction**

```python
from pyspark.ml import Pipeline
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator

@staticmethod
def train_sentiment_model(review_df):
    """
    10. Sentiment Analysis with MLlib
    Train model to predict positive/negative reviews
    """
    # Prepare data
    df = review_df.withColumn(
        "label",
        when(col("stars") >= 4, 1.0).otherwise(0.0)
    ).select("text", "label")

    # Split train/test
    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)

    # Build pipeline
    tokenizer = Tokenizer(inputCol="text", outputCol="words")
    remover = StopWordsRemover(inputCol="words", outputCol="filtered")
    hashingTF = HashingTF(inputCol="filtered", outputCol="raw_features", numFeatures=10000)
    idf = IDF(inputCol="raw_features", outputCol="features")
    lr = LogisticRegression(maxIter=10, regParam=0.01)

    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])

    # Train
    model = pipeline.fit(train_df)

    # Evaluate
    predictions = model.transform(test_df)
    evaluator = BinaryClassificationEvaluator()
    auc = evaluator.evaluate(predictions)

    print(f"Model AUC: {auc}")

    # Save model
    model.write().overwrite().save("models/sentiment_model")

    return model, auc

@staticmethod
def predict_sentiment(model, review_df):
    """Apply model to new data"""
    predictions = model.transform(review_df)
    return predictions.select(
        "business_id",
        "text",
        "probability",
        "prediction"
    )
```

**Ká»¹ nÄƒng thá»ƒ hiá»‡n:**
- âœ… Feature engineering (Tokenizer, TF-IDF)
- âœ… ML Pipeline
- âœ… Model training vÃ  evaluation
- âœ… Model persistence

---

#### 4.2.2. Graph Processing (3-4 ngÃ y)

**Analysis má»›i: Analysis 11 - User-Business Network**

```python
from graphframes import GraphFrame

@staticmethod
def user_business_network(review_df, business_df, user_df):
    """
    11. Social Network Analysis
    Build user-business bipartite graph
    """
    # Create vertices
    business_vertices = business_df.select(
        col("business_id").alias("id"),
        lit("business").alias("type"),
        col("name")
    )

    user_vertices = user_df.select(
        col("user_id").alias("id"),
        lit("user").alias("type"),
        col("name")
    )

    vertices = business_vertices.union(user_vertices)

    # Create edges (reviews as edges)
    edges = review_df.select(
        col("user_id").alias("src"),
        col("business_id").alias("dst"),
        col("stars").alias("weight")
    )

    # Build graph
    graph = GraphFrame(vertices, edges)

    # PageRank (influential businesses)
    pagerank = graph.pageRank(resetProbability=0.15, maxIter=10)
    influential_businesses = pagerank.vertices.filter(
        col("type") == "business"
    ).orderBy(desc("pagerank")).limit(20)

    # Connected components (user communities)
    communities = graph.connectedComponents()

    # Degree analysis
    in_degrees = graph.inDegrees  # Number of reviews per business
    out_degrees = graph.outDegrees  # Number of reviews per user

    return {
        'graph': graph,
        'influential_businesses': influential_businesses,
        'communities': communities,
        'in_degrees': in_degrees,
        'out_degrees': out_degrees
    }
```

---

#### 4.2.3. Advanced Statistics (2 ngÃ y)

```python
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler

@staticmethod
def correlation_analysis(business_df, review_df):
    """
    12. Statistical Analysis
    Correlation between features
    """
    # Join and prepare features
    joined = business_df.join(
        review_df.groupBy("business_id").agg(
            avg("stars").alias("avg_review_stars"),
            count("review_id").alias("review_count")
        ),
        "business_id"
    )

    # Assemble features
    assembler = VectorAssembler(
        inputCols=["stars", "review_count", "avg_review_stars"],
        outputCol="features"
    )

    feature_df = assembler.transform(joined)

    # Correlation matrix
    correlation_matrix = Correlation.corr(feature_df, "features", "pearson")

    return correlation_matrix

@staticmethod
def anomaly_detection(review_df):
    """
    Detect anomalous reviews (outliers)
    """
    # Calculate statistics
    stats = review_df.groupBy("business_id").agg(
        avg("stars").alias("mean_stars"),
        stddev("stars").alias("std_stars")
    )

    # Join back and find outliers (> 3 std deviations)
    with_stats = review_df.join(stats, "business_id")

    anomalies = with_stats.withColumn(
        "z_score",
        abs((col("stars") - col("mean_stars")) / col("std_stars"))
    ).filter(col("z_score") > 3)

    return anomalies
```

---

#### 4.2.4. Time Series Advanced (2 ngÃ y)

```python
@staticmethod
def time_series_analysis(review_df):
    """
    13. Advanced Time Series
    Trend, seasonality, forecasting
    """
    # Aggregate by date
    daily_reviews = review_df.groupBy(
        to_date(col("date"), "yyyy-MM-dd HH:mm:ss").alias("date")
    ).agg(
        count("review_id").alias("count"),
        avg("stars").alias("avg_stars")
    ).orderBy("date")

    # Moving average (7-day)
    windowSpec = Window.orderBy("date").rowsBetween(-6, 0)

    with_ma = daily_reviews.withColumn(
        "ma_7day",
        avg("count").over(windowSpec)
    )

    # Trend detection (linear regression slope)
    from pyspark.ml.regression import LinearRegression
    from pyspark.ml.feature import VectorAssembler

    # Convert date to numeric
    with_numeric = with_ma.withColumn(
        "days_since_start",
        datediff(col("date"), lit("2020-01-01")).cast("double")
    )

    assembler = VectorAssembler(
        inputCols=["days_since_start"],
        outputCol="features"
    )

    feature_df = assembler.transform(with_numeric).select(
        "features",
        col("count").alias("label")
    )

    lr = LinearRegression(maxIter=10)
    model = lr.fit(feature_df)

    trend_slope = model.coefficients[0]
    print(f"Trend: {'Increasing' if trend_slope > 0 else 'Decreasing'}")

    return with_ma, trend_slope
```

**Tá»•ng thá»i gian PhÆ°Æ¡ng Ã¡n 2: 1-2 tuáº§n**

---

## 5. ROADMAP TRIá»‚N KHAI

### ğŸ“… Timeline Ä‘á» xuáº¥t

#### âš¡ Sprint 1: Quick Wins (Tuáº§n 1)
- **Day 1-2:** Window Functions (Analysis 8: Trending Businesses)
- **Day 3:** Broadcast Join + Pivot/Unpivot (Analysis 9: Performance Matrix)
- **Day 4-5:** UDF + Pandas UDF + Testing

**Deliverable:** 2 analyses má»›i + code refactor

#### ğŸš€ Sprint 2: ML & Advanced (Tuáº§n 2-3)
- **Week 2:**
  - Day 1-3: Machine Learning Pipeline (Analysis 10: Sentiment)
  - Day 4-5: Advanced Statistics (Analysis 12: Correlation)
- **Week 3:**
  - Day 1-3: Graph Processing (Analysis 11: Network)
  - Day 4-5: Time Series (Analysis 13: Forecasting)

**Deliverable:** 4 analyses má»›i + ML models

---

### ğŸ“ Cáº¥u trÃºc code má»›i

```
Spark_Batch/
â”œâ”€â”€ batch_analytics.py              (hiá»‡n táº¡i: 7 analyses)
â”œâ”€â”€ batch_analytics_advanced.py     (Má»šI: 6 analyses má»›i)
â”‚   â”œâ”€â”€ Analysis 8: Trending (Window)
â”‚   â”œâ”€â”€ Analysis 9: Pivot Matrix
â”‚   â”œâ”€â”€ Analysis 10: ML Sentiment
â”‚   â”œâ”€â”€ Analysis 11: Graph Network
â”‚   â”œâ”€â”€ Analysis 12: Statistics
â”‚   â””â”€â”€ Analysis 13: Time Series
â”‚
â”œâ”€â”€ batch_udf.py                    (Má»šI: UDF functions)
â”œâ”€â”€ batch_ml.py                     (Má»šI: ML utilities)
â””â”€â”€ batch_graph.py                  (Má»šI: Graph utilities)
```

---

### âœ… Checklist hoÃ n thÃ nh

**Sau khi triá»ƒn khai PhÆ°Æ¡ng Ã¡n 1 (Quick):**
- [ ] Window Functions: `row_number`, `rank`, `lag`, `lead`
- [ ] Pivot/Unpivot operations
- [ ] Broadcast join explicit
- [ ] Regular UDF
- [ ] Pandas UDF (vectorized)
- [ ] Skewed join handling

**Sau khi triá»ƒn khai PhÆ°Æ¡ng Ã¡n 2 (Full):**
- [ ] ML Pipeline vá»›i MLlib
- [ ] Feature engineering
- [ ] Model training + evaluation
- [ ] Graph processing vá»›i GraphFrames
- [ ] PageRank, Connected Components
- [ ] Correlation analysis
- [ ] Anomaly detection
- [ ] Time series vá»›i trend/seasonality

---

## 6. ÄÃNH GIÃ Tá»”NG Káº¾T

### Äiá»ƒm sá»‘ hiá»‡n táº¡i theo tá»«ng tiÃªu chÃ­:

| TiÃªu chÃ­ | Äiá»ƒm hiá»‡n táº¡i | Äiá»ƒm sau PA1 | Äiá»ƒm sau PA2 |
|----------|---------------|--------------|--------------|
| 1. Táº­p há»£p phá»©c táº¡p | 40% â­â­ | 80% â­â­â­â­ | 90% â­â­â­â­â­ |
| 2. Biáº¿n Ä‘á»•i nÃ¢ng cao | 50% â­â­â­ | 75% â­â­â­â­ | 85% â­â­â­â­ |
| 3. Join operations | 40% â­â­ | 75% â­â­â­â­ | 85% â­â­â­â­ |
| 4. Tá»‘i Æ°u hÃ³a | 50% â­â­â­ | 65% â­â­â­ | 80% â­â­â­â­ |
| 5. Streaming | 60%* â­â­â­ | 60% â­â­â­ | 75% â­â­â­â­ |
| 6. PhÃ¢n tÃ­ch nÃ¢ng cao | 10% â­ | 30% â­â­ | 85% â­â­â­â­ |
| **Tá»”NG** | **42%** | **64%** | **83%** |

*Note: Streaming score chá»‰ tÃ­nh cho nhÃ¡nh HAI

### Khuyáº¿n nghá»‹:

1. **Ngáº¯n háº¡n (1 tuáº§n):** Triá»ƒn khai PhÆ°Æ¡ng Ã¡n 1 Ä‘á»ƒ Ä‘áº¡t 64%
   - Äá»§ Ä‘á»ƒ pass yÃªu cáº§u "trung cáº¥p"
   - Thá»i gian há»£p lÃ½
   - ROI cao

2. **Trung háº¡n (2-3 tuáº§n):** Triá»ƒn khai PhÆ°Æ¡ng Ã¡n 2 Ä‘á»ƒ Ä‘áº¡t 83%
   - Xuáº¥t sáº¯c cho yÃªu cáº§u "trung cáº¥p"
   - CÃ³ thá»ƒ lÃªn "nÃ¢ng cao"
   - Impressive cho reviewer

3. **Priority order:**
   - âœ… Window Functions (cáº§n thiáº¿t nháº¥t)
   - âœ… Broadcast Join (dá»… nháº¥t, tÃ¡c Ä‘á»™ng lá»›n)
   - âœ… UDF/Pandas UDF (practical)
   - âœ… ML Pipeline (impressive nháº¥t)

---

**Báº¡n muá»‘n tÃ´i báº¯t Ä‘áº§u triá»ƒn khai phÆ°Æ¡ng Ã¡n nÃ o? TÃ´i khuyáº¿n nghá»‹ báº¯t Ä‘áº§u vá»›i PhÆ°Æ¡ng Ã¡n 1! ğŸš€**
