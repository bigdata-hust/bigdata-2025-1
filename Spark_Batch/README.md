# ğŸš€ YELP BIG DATA ANALYSIS - BATCH MODE

Há»‡ thá»‘ng phÃ¢n tÃ­ch dá»¯ liá»‡u Yelp sá»­ dá»¥ng PySpark vá»›i cÃ¡c tÃ­nh nÄƒng Spark nÃ¢ng cao.

---

## âš¡ QUICK START

### 1. CÃ i Ä‘áº·t dependencies
```bash
pip3 install pyspark pandas pyarrow
```

### 2. Cháº¡y pipeline
```bash
# Cháº¡y phiÃªn báº£n Ä‘áº§y Ä‘á»§ (9 analyses)
python3 batch_main_v2.py --data-path ./data/

# Hoáº·c cháº¡y phiÃªn báº£n cÆ¡ báº£n (7 analyses)
python3 batch_main.py --data-path ./data/
```

### 3. Test cÃ¡c tÃ­nh nÄƒng
```bash
# Test táº¥t cáº£ tÃ­nh nÄƒng nÃ¢ng cao
python3 test_local_features.py --test all

# Test tá»«ng feature riÃªng
python3 test_local_features.py --test udf
python3 test_local_features.py --test window
python3 test_local_features.py --test pivot
python3 test_local_features.py --test broadcast
```

---

## ğŸ“ Cáº¤U TRÃšC NGáº®N Gá»ŒN

```
Spark_Batch/
â”‚
â”œâ”€â”€ ğŸ¯ Entry Points
â”‚   â”œâ”€â”€ batch_main.py         â†’ Cháº¡y 7 analyses cÆ¡ báº£n
â”‚   â””â”€â”€ batch_main_v2.py      â†’ Cháº¡y 9 analyses nÃ¢ng cao â­
â”‚
â”œâ”€â”€ ğŸ”§ Core Modules
â”‚   â”œâ”€â”€ batch_configuration.py â†’ Spark config & schemas
â”‚   â”œâ”€â”€ batch_load_data.py     â†’ Load dá»¯ liá»‡u tá»« JSON
â”‚   â””â”€â”€ batch_pipeline.py      â†’ Pipeline orchestrator
â”‚
â”œâ”€â”€ ğŸ“Š Analytics
â”‚   â”œâ”€â”€ batch_analytics.py            â†’ 7 analyses cÆ¡ báº£n
â”‚   â””â”€â”€ batch_analytics_advanced.py   â†’ 2 analyses nÃ¢ng cao â­
â”‚
â”œâ”€â”€ ğŸ¯ UDF Library
â”‚   â””â”€â”€ batch_udf.py           â†’ 7 UDFs (3 Regular + 4 Pandas) â­
â”‚
â””â”€â”€ ğŸ§ª Testing
    â””â”€â”€ test_local_features.py â†’ Test suite â­
```

â­ = Files má»›i trong Phase 1 (Advanced Features)

---

## ğŸ“ CÃC PHÃ‚N TÃCH

### V1 - 7 Analyses CÆ¡ báº£n

| ID | TÃªn Analysis | TÃ­nh nÄƒng |
|---|---|---|
| 1 | Top Selling Products | Broadcast Join |
| 2 | User Purchase Patterns | Aggregation |
| 3 | Top Users by Reviews | Broadcast Join |
| 4 | Category Trends Over Time | Broadcast Join |
| 5 | High Rating Low Review Count | Filter & Aggregation |
| 6 | Geographic Distribution | Broadcast Join |
| 7 | Seasonal Trends | Time-based Analysis |

### V2 - 2 Analyses NÃ¢ng cao (Má»›i!)

| ID | TÃªn Analysis | TÃ­nh nÄƒng |
|---|---|---|
| 8 | Trending Businesses | **Window Functions** (lag, lead, rank, avg) |
| 9 | Category Performance Matrix | **Pivot/Unpivot** Operations |

---

## ğŸ”¥ TÃNH NÄ‚NG Ná»”I Báº¬T

### 1. UDF Library (`batch_udf.py`)
- **3 Regular UDFs**: categorize_rating, is_weekend, extract_city_state
- **4 Pandas UDFs**: sentiment_score, extract_keywords, text_length_normalized, extract_hashtags
- **Performance**: Pandas UDF nhanh hÆ¡n 10-100x

### 2. Window Functions (Analysis 8)
- `lag()`, `lead()` - So sÃ¡nh time series
- `avg() over window` - Moving averages
- `dense_rank()` - Ranking
- `rowsBetween()` - Sliding windows

### 3. Pivot/Unpivot (Analysis 9)
- `pivot()` - Long â†’ Wide format
- `stack()` - Wide â†’ Long format
- Cross-tabulation analysis

### 4. Broadcast Join Optimization
- Optimized cho join vá»›i small tables
- Ãp dá»¥ng cho analyses 1, 3, 4, 6

---

## ğŸ“Š Káº¾T QUáº¢

### Spark Skills Coverage
- **TrÆ°á»›c**: 42%
- **Sau**: 64%
- **Cáº£i thiá»‡n**: +22%

### Code Statistics
- **Tá»•ng dÃ²ng code**: 2,712 lines
- **Sá»‘ analyses**: 9
- **Sá»‘ UDFs**: 7
- **Test coverage**: 4 feature tests

---

## ğŸ“– TÃ€I LIá»†U CHI TIáº¾T

| TÃ i liá»‡u | Ná»™i dung |
|---|---|
| **PROJECT_STRUCTURE.md** | Cáº¥u trÃºc chi tiáº¿t toÃ n bá»™ dá»± Ã¡n (Äá»ŒCNÃ€Y!) |
| **LOCAL_TEST_GUIDE.md** | HÆ°á»›ng dáº«n test tá»«ng bÆ°á»›c |
| **IMPLEMENTATION_PLAN_PA1.md** | Káº¿ hoáº¡ch triá»ƒn khai Phase 1 |
| **QUICKSTART.md** | HÆ°á»›ng dáº«n nhanh |

---

## ğŸ¯ Sá»¬ Dá»¤NG Tá»ªNG MODULE

### Chá»‰ cáº§n Spark Session?
```python
from batch_configuration import SparkConfig
spark = SparkConfig.create_spark_session()
```

### Chá»‰ cáº§n load dá»¯ liá»‡u?
```python
from batch_load_data import DataLoader
loader = DataLoader(spark, "./data/")
business_df = loader.load_business_data()
review_df = loader.load_review_data()
```

### Chá»‰ cháº¡y 1 analysis?
```python
from batch_analytics import YelpAnalytics
analytics = YelpAnalytics()
result = analytics.top_selling_products_recent(review_df, business_df)
result.show()
```

### Sá»­ dá»¥ng UDF?
```python
from batch_udf import sentiment_score, categorize_rating
from pyspark.sql.functions import col

df = df.withColumn("sentiment", sentiment_score(col("text")))
df = df.withColumn("rating_label", categorize_rating(col("stars")))
```

### Cháº¡y analysis nÃ¢ng cao?
```python
from batch_analytics_advanced import AdvancedYelpAnalytics
advanced = AdvancedYelpAnalytics()
result = advanced.trending_businesses(review_df, business_df, window_days=90)
result.show()
```

---

## ğŸ› TROUBLESHOOTING

### ImportError: No module named 'pyspark'
```bash
pip3 install pyspark==3.4.1
```

### ImportError: No module named 'pandas'
```bash
pip3 install pandas==2.0.3 pyarrow==12.0.0
```

### OutOfMemoryError
Giáº£m memory trong `batch_configuration.py`:
```python
.config("spark.driver.memory", "4g")  # thay vÃ¬ 8g
.config("spark.executor.memory", "2g")  # thay vÃ¬ 4g
```

### Broadcast join khÃ´ng hoáº¡t Ä‘á»™ng
Check physical plan:
```python
result.explain()
# Pháº£i tháº¥y "BroadcastHashJoin" hoáº·c "BroadcastExchange"
```

---

## ğŸ’¡ TIPS & BEST PRACTICES

### Performance
- âœ… Sá»­ dá»¥ng **Pandas UDF** thay vÃ¬ Regular UDF khi cÃ³ thá»ƒ
- âœ… Sá»­ dá»¥ng **broadcast()** cho joins vá»›i báº£ng nhá» (<10MB)
- âœ… **Cache** DataFrame náº¿u sá»­ dá»¥ng nhiá»u láº§n
- âœ… Sá»­ dá»¥ng **explicit schemas** khi load data

### Code Organization
- âœ… Má»—i analysis lÃ  má»™t static method riÃªng biá»‡t
- âœ… TÃ¡ch configuration, data loading, vÃ  analytics logic
- âœ… Error handling trong má»—i analysis function
- âœ… Modularity - má»—i module cÃ³ thá»ƒ cháº¡y Ä‘á»™c láº­p

### Testing
- âœ… Test tá»«ng feature trÆ°á»›c khi integration
- âœ… Sá»­ dá»¥ng sample data nhá» khi develop
- âœ… Check physical plan vá»›i `.explain()` Ä‘á»ƒ verify optimizations

---

## ğŸš€ ROADMAP

### âœ… Phase 1 (Completed)
- [x] UDF Library (7 UDFs)
- [x] Window Functions (Analysis 8)
- [x] Pivot/Unpivot (Analysis 9)
- [x] Broadcast Join Optimization
- [x] Test Suite
- [x] Documentation

### ğŸ”œ Phase 2 (Future)
- [ ] Machine Learning Pipeline
- [ ] Graph Processing (GraphFrames)
- [ ] Advanced Aggregations (ROLLUP, CUBE)
- [ ] Performance tuning vá»›i AQE

---

## ğŸ“ Há»– TRá»¢

### Cáº§n hiá»ƒu cáº¥u trÃºc chi tiáº¿t?
ğŸ‘‰ Äá»c **PROJECT_STRUCTURE.md** (80KB, ráº¥t chi tiáº¿t!)

### Cáº§n test local?
ğŸ‘‰ Äá»c **LOCAL_TEST_GUIDE.md**

### Cáº§n cháº¡y nhanh?
ğŸ‘‰ Xem â¬†ï¸ pháº§n QUICK START á»Ÿ trÃªn

---

## ğŸ“ VERSION HISTORY

| Version | Date | Changes |
|---|---|---|
| 2.0 | 2025-12-16 | âœ¨ Add advanced features (UDF, Window, Pivot, Broadcast) |
| 1.0 | 2025-12-15 | ğŸ‰ Initial batch implementation (7 analyses) |

---

**Current Version**: 2.0 (Advanced Features)
**Last Updated**: 2025-12-16
**Branch**: `claude/review-project-structure-pfDJE`

---

## ğŸ‰ Báº®T Äáº¦U NGAY!

```bash
# 1. Clone repo (náº¿u chÆ°a cÃ³)
git clone <repo-url>
cd bigdata-2025-1/Spark_Batch

# 2. CÃ i Ä‘áº·t dependencies
pip3 install pyspark pandas pyarrow

# 3. Test features
python3 test_local_features.py --test all

# 4. Cháº¡y pipeline
python3 batch_main_v2.py --data-path ./data/

# 5. Xem káº¿t quáº£
ls -lh output_v2/
```

---

**Happy Analyzing! ğŸŠ**
