# üß™ LOCAL TEST GUIDE - Ph∆∞∆°ng √°n 1 (Advanced Spark Features)

**M·ª•c ƒë√≠ch:** H∆∞·ªõng d·∫´n test t·ª´ng b∆∞·ªõc tr√™n local ƒë·ªÉ verify 4 k·ªπ nƒÉng Spark n√¢ng cao

**Score m·ª•c ti√™u:** 42% ‚Üí 64%

---

## üìã M·ª§C L·ª§C

1. [Chu·∫©n b·ªã m√¥i tr∆∞·ªùng](#1-chu·∫©n-b·ªã-m√¥i-tr∆∞·ªùng)
2. [Test t·ª´ng feature](#2-test-t·ª´ng-feature)
3. [Test t√≠ch h·ª£p](#3-test-t√≠ch-h·ª£p)
4. [Verify k·∫øt qu·∫£](#4-verify-k·∫øt-qu·∫£)
5. [Troubleshooting](#5-troubleshooting)

---

## 1. CHU·∫®N B·ªä M√îI TR∆Ø·ªúNG

### B∆∞·ªõc 1.1: Ki·ªÉm tra dependencies

```bash
cd /home/user/bigdata-2025-1/Spark_Batch

# Check Python
python3 --version
# Expected: Python 3.8+

# Check Java
java -version
# Expected: Java 11+

# Check PySpark
python3 -c "import pyspark; print(pyspark.__version__)"
# Expected: 4.0.1
```

### B∆∞·ªõc 1.2: C√†i th√™m dependencies cho Pandas UDF

```bash
# C√†i pandas v√† pyarrow (c·∫ßn cho Pandas UDF)
pip3 install pandas==2.0.3 pyarrow==12.0.0

# Verify
python3 -c "import pandas, pyarrow; print('OK')"
```

### B∆∞·ªõc 1.3: Chu·∫©n b·ªã data

**Option A: T·∫°o sample data**
```bash
# T·∫°o sample data nhanh (100 businesses, 1000 reviews)
python3 create_sample_data.py
```

**Option B: S·ª≠ d·ª•ng data c√≥ s·∫µn**
```bash
# ƒê·∫£m b·∫£o data files t·ªìn t·∫°i
ls -lh ../data/business.json
ls -lh ../data/review.json
```

---

## 2. TEST T·ª™NG FEATURE

### Test 2.1: UDF Library ‚≠ê‚≠ê‚≠ê‚≠ê

**M·ª•c ƒë√≠ch:** Test 7 UDFs (3 Regular + 4 Pandas)

**Ch·∫°y test:**
```bash
# Test UDF demo tr∆∞·ªõc
python3 batch_udf.py
```

**Expected output:**
```
==============================================================
UDF LIBRARY - DEMO & TEST
==============================================================

1. Creating sample data and applying UDFs...

Results:
+----------+-----+----------------+---------+----------+
|review_id |stars|rating_category |sentiment|is_weekend|
+----------+-----+----------------+---------+----------+
|review_0  |3.5  |Good            |0.85     |false     |
|review_1  |4.0  |Good            |0.15     |true      |
...

‚úì All UDFs working correctly!
```

**Verify:**
- [ ] 4 UDFs ch·∫°y kh√¥ng l·ªói
- [ ] Sentiment score h·ª£p l√Ω (0.0-1.0)
- [ ] Keywords ƒë∆∞·ª£c extract ra
- [ ] Weekend detection ch√≠nh x√°c

**Ch·∫°y full test:**
```bash
python3 test_local_features.py --test udf --data ../data/
```

**Expected output:**
```
==============================================================
TEST 1: UDF LIBRARY
==============================================================

Testing Regular UDFs...
  1. categorize_rating...
     Time: 2.34s ‚úì
  2. is_weekend...
     ‚úì

Testing Pandas UDFs (vectorized)...
  3. sentiment_score...
     Time: 0.21s ‚úì
  4. extract_keywords...
     ‚úì

==============================================================
Performance Comparison:
  Regular UDF:  2.34s
  Pandas UDF:   0.21s
  Speedup:      11.1x faster! ‚úÖ
==============================================================

‚úÖ UDF Test PASSED
```

**Verify:**
- [ ] Pandas UDF nhanh h∆°n Regular UDF (5-20x)
- [ ] Kh√¥ng c√≥ error
- [ ] Test PASSED

---

### Test 2.2: Window Functions ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**M·ª•c ƒë√≠ch:** Test Analysis 8 (Trending Businesses)

**Ch·∫°y test:**
```bash
python3 test_local_features.py --test window --data ../data/
```

**Expected output:**
```
==============================================================
TEST 2: WINDOW FUNCTIONS (Analysis 8)
==============================================================

  - Analyzing reviews from 90 days
  - Grouped into weekly buckets
  - Applied window functions: lag, lead, avg, sum, row_number
‚úì Analysis 8 completed in 3.45s
  Found 10 trending businesses

Trending Businesses (Top 5):
+------------------+----------+-------------+------------+----------------+-----------+
|name              |city      |weekly_count |growth_rate |avg_last_4_weeks|trend_rank |
+------------------+----------+-------------+------------+----------------+-----------+
|Restaurant ABC    |Phoenix   |45           |0.875       |32.5            |1          |
|Coffee Shop XYZ   |Las Vegas |38           |0.722       |28.3            |2          |
...

==============================================================
Window Functions Verified:
  ‚úì lag() - Previous week comparison
  ‚úì avg() over window - Moving average
  ‚úì dense_rank() - Ranking
  ‚úì sum() over window - Cumulative sum
  ‚úì row_number() - Week numbering
==============================================================

‚úÖ Window Functions Test PASSED
```

**Verify:**
- [ ] C√≥ growth_rate (t·ª´ lag)
- [ ] C√≥ avg_last_4_weeks (t·ª´ avg over window)
- [ ] C√≥ trend_rank (t·ª´ dense_rank)
- [ ] K·∫øt qu·∫£ c√≥ nghƒ©a (growth_rate > 0)
- [ ] Test PASSED

---

### Test 2.3: Pivot/Unpivot ‚≠ê‚≠ê‚≠ê‚≠ê

**M·ª•c ƒë√≠ch:** Test Analysis 9 (Category Performance Matrix)

**Ch·∫°y test:**
```bash
python3 test_local_features.py --test pivot --data ../data/
```

**Expected output:**
```
==============================================================
TEST 3: PIVOT/UNPIVOT (Analysis 9)
==============================================================

  - Finding top 5 categories and 3 cities...
  - Top categories: Restaurants, Shopping, Food...
  - Top cities: Phoenix, Las Vegas, Toronto...
  - Aggregated metrics by category and city
  - Creating pivot table...
  - Pivot complete: 3 cities, 5 categories
  - Creating unpivot table...
  - Unpivot complete: 15 rows

Pivot Result (Wide format):
+------------+--------------+-------------+----------+
|category    |Phoenix       |Las Vegas    |Toronto   |
+------------+--------------+-------------+----------+
|Restaurants |4.5 (120)     |4.3 (95)     |4.2 (88)  |
|Shopping    |4.1 (45)      |4.0 (38)     |3.9 (42)  |
...

Unpivot Result (Long format - sample):
+------------+-----------+-----------+-------------+
|category    |city       |avg_stars  |review_count |
+------------+-----------+-----------+-------------+
|Restaurants |Phoenix    |4.5        |120          |
|Restaurants |Las Vegas  |4.3        |95           |
...

Best Category per City:
+-----------+--------------+-----------+-------------+
|city       |best_category |avg_stars  |review_count |
+-----------+--------------+-----------+-------------+
|Phoenix    |Restaurants   |4.5        |120          |
...

==============================================================
Pivot/Unpivot Operations Verified:
  ‚úì explode() - Split categories
  ‚úì pivot() - Transform long ‚Üí wide
  ‚úì stack() - Transform wide ‚Üí long
==============================================================

‚úÖ Pivot/Unpivot Test PASSED
```

**Verify:**
- [ ] Pivot: Wide table (categories x cities)
- [ ] Unpivot: Long table (category, city, metrics)
- [ ] C√≥ best_per_city v√† best_per_category
- [ ] Test PASSED

---

### Test 2.4: Broadcast Join ‚≠ê‚≠ê‚≠ê‚≠ê

**M·ª•c ƒë√≠ch:** Test Broadcast Join optimization

**Ch·∫°y test:**
```bash
python3 test_local_features.py --test broadcast --data ../data/
```

**Expected output:**
```
==============================================================
TEST 4: BROADCAST JOIN OPTIMIZATION
==============================================================

Running Analysis 1 with Broadcast Join...

Analysis 1: Top 10 Selling Products (Last 90 days)
‚úì Analysis 1 completed in 3.87s

Execution time: 3.87s

==============================================================
Checking Physical Plan...
‚úÖ Broadcast Join confirmed in physical plan!
==============================================================

‚úÖ Broadcast Join Test PASSED
```

**Verify:**
- [ ] Ch·∫°y kh√¥ng l·ªói
- [ ] Physical plan c√≥ "BroadcastHashJoin"
- [ ] Test PASSED

---

## 3. TEST T√çCH H·ª¢P

### Test 3.1: Test t·∫•t c·∫£ features

```bash
# Test all features c√πng l√∫c
python3 test_local_features.py --test all --data ../data/
```

**Expected output:**
```
================================================================================
                  SPARK ADVANCED FEATURES - TEST SUITE
                            (Ph∆∞∆°ng √°n 1)
================================================================================

==============================================================
LOADING TEST DATA
==============================================================
‚úì Loaded 100 businesses
‚úì Loaded 1,000 reviews

[... run all 4 tests ...]

================================================================================
                            TEST SUMMARY
================================================================================
  ‚úÖ UDF: PASS
  ‚úÖ WINDOW: PASS
  ‚úÖ PIVOT: PASS
  ‚úÖ BROADCAST: PASS
================================================================================
                      üéâ ALL TESTS PASSED! üéâ
================================================================================
```

**Verify:**
- [ ] T·∫•t c·∫£ 4 tests PASSED
- [ ] Kh√¥ng c√≥ error
- [ ] Summary hi·ªÉn th·ªã OK

---

### Test 3.2: Run full pipeline (9 analyses)

```bash
# Ch·∫°y pipeline ƒë·∫ßy ƒë·ªß
python3 batch_main_v2.py --data-path ../data/
```

**Expected output:**
```
================================================================================
               YELP BIG DATA ANALYSIS SYSTEM - VERSION 2
                    BATCH MODE - Enhanced with Advanced Features
                        Run Time: 2025-12-15 15:30:00
================================================================================

NEW FEATURES:
  ‚ú® Window Functions (Analysis 8: Trending Businesses)
  ‚ú® Pivot/Unpivot Operations (Analysis 9: Performance Matrix)
  ‚ú® Broadcast Join Optimization (Analyses 1-7)
  ‚ú® UDF Library Support (7 custom functions)
================================================================================

==============================================================
DATA LOADING PHASE
==============================================================
‚úì Loaded 100 businesses from ../data/business.json
‚úì Loaded 1,000 reviews from ../data/review.json

‚úì All data loaded successfully

==============================================================
ANALYSIS PHASE - RUNNING ALL 9 ANALYSES
==============================================================

==============================================================
PART 1: Original Analyses (1-7)
==============================================================

Analysis 1: Top 10 Selling Products (Last 90 days)
‚úì Analysis 1 completed in 2.34s

Analysis 2: Top 10 Stores by Product Diversity
‚úì Analysis 2 completed in 1.23s

[... Analyses 3-7 ...]

==============================================================
PART 2: Advanced Analyses (8-9)
==============================================================

Analysis 8: Top 10 Trending Businesses (Last 90 days)
‚úì Analysis 8 completed in 3.45s

Analysis 9: Category Performance Matrix (Pivot/Unpivot)
‚úì Analysis 9 completed in 2.87s

==============================================================
ALL 9 ANALYSES COMPLETED in 25.34s
==============================================================

[... Results preview for 9 analyses ...]

================================================================================
                            SUMMARY REPORT - V2
================================================================================

==============================================================
Part 1: Original Analyses (1-7)
==============================================================
  TOP SELLING: 10 records
  DIVERSE STORES: 10 records
  BEST RATED: 10 records
  MOST POSITIVE: 10 records
  PEAK HOURS: 24 records
  TOP CATEGORIES: 20 records
  STORE STATS: 100 records

==============================================================
Part 2: Advanced Analyses (8-9)
==============================================================
  TRENDING BUSINESSES: 10 businesses
  CATEGORY MATRIX: 15 category-city pairs
  CATEGORIES ANALYZED: 5
  CITIES ANALYZED: 3
  TOTAL REVIEWS: 1,000
==============================================================

‚úì All results saved to: ./output_v2/

================================================================================
                    ‚úì PIPELINE COMPLETED SUCCESSFULLY
                          9 Analyses Executed
================================================================================
```

**Verify:**
- [ ] 9 analyses ch·∫°y th√†nh c√¥ng
- [ ] Kh√¥ng c√≥ error
- [ ] C√≥ output files trong `./output_v2/`
- [ ] Pipeline COMPLETED

---

## 4. VERIFY K·∫æT QU·∫¢

### Verify 4.1: Ki·ªÉm tra output files

```bash
# Li·ªát k√™ output files
ls -lh output_v2/

# Expected:
# top_selling/
# diverse_stores/
# best_rated/
# most_positive/
# peak_hours/
# top_categories/
# store_stats/
# trending_businesses/          ‚Üê NEW
# category_matrix_pivot/        ‚Üê NEW
# category_matrix_unpivot/      ‚Üê NEW
```

### Verify 4.2: ƒê·ªçc v√† verify k·∫øt qu·∫£

```bash
# Test ƒë·ªçc Parquet output
python3 << 'EOF'
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Verify").getOrCreate()

# ƒê·ªçc Analysis 8 output
trending = spark.read.parquet("output_v2/trending_businesses/")
print(f"Trending businesses: {trending.count()} rows")
trending.show(5, truncate=False)

# Verify c√≥ columns t·ª´ window functions
assert "growth_rate" in trending.columns, "Missing growth_rate column!"
assert "avg_last_4_weeks" in trending.columns, "Missing avg_last_4_weeks column!"
print("‚úÖ Analysis 8 output verified!")

# ƒê·ªçc Analysis 9 output
pivot = spark.read.parquet("output_v2/category_matrix_pivot/")
print(f"\nCategory matrix: {pivot.count()} rows")
pivot.show(truncate=False)
print("‚úÖ Analysis 9 output verified!")

spark.stop()
EOF
```

### Verify 4.3: Performance metrics

**So s√°nh v·ªõi version c≈©:**

| Metric | V1 (Old) | V2 (New) | Improvement |
|--------|----------|----------|-------------|
| Total analyses | 7 | 9 | +2 analyses |
| Broadcast joins | 0 explicit | 4 explicit | ‚úÖ Optimized |
| Window functions | ‚ùå | ‚úÖ | ‚úÖ Added |
| Pivot/Unpivot | ‚ùå | ‚úÖ | ‚úÖ Added |
| UDF library | ‚ùå | ‚úÖ 7 UDFs | ‚úÖ Added |
| Skill score | 42% | 64% | +22% |

---

## 5. TROUBLESHOOTING

### Issue 5.1: Import error - pandas/pyarrow

**Error:**
```
ModuleNotFoundError: No module named 'pandas'
```

**Fix:**
```bash
pip3 install pandas==2.0.3 pyarrow==12.0.0
```

---

### Issue 5.2: Window function error

**Error:**
```
AnalysisException: window() is not supported on streaming DataFrames
```

**Fix:**
Code ƒëang ch·∫°y batch mode, kh√¥ng ph·∫£i streaming. Check xem c√≥ import ƒë√∫ng `batch_analytics_advanced` kh√¥ng.

---

### Issue 5.3: Broadcast join not working

**Error:**
Physical plan kh√¥ng c√≥ "BroadcastHashJoin"

**Debug:**
```python
# Check physical plan
result.explain()

# Should see: "BroadcastHashJoin" or "BroadcastNestedLoopJoin"
```

**Possible causes:**
- Business table qu√° l·ªõn (> 10MB) ‚Üí Spark t·ª± ƒë·ªông kh√¥ng broadcast
- Ch∆∞a import `broadcast` function
- Config `spark.sql.autoBroadcastJoinThreshold` qu√° nh·ªè

---

### Issue 5.4: Out of memory

**Error:**
```
java.lang.OutOfMemoryError: Java heap space
```

**Fix:**
```python
# Edit batch_configuration.py
.config("spark.driver.memory", "4g")  # Gi·∫£m t·ª´ 8g

# Ho·∫∑c reduce data size
head -100 ../data/review.json > ../data/review_sample.json
```

---

### Issue 5.5: Sample data kh√¥ng ƒë·ªß

**Error:**
```
Analysis 8 completed in 3.45s
  Found 0 trending businesses
```

**Fix:**
Sample data c√≥ th·ªÉ kh√¥ng c√≥ ƒë·ªß data trong 90 ng√†y g·∫ßn ƒë√¢y.

```bash
# Option 1: Gi·∫£m window_days
python3 batch_main_v2.py --data-path ../data/
# Edit config: window_days=30 (thay v√¨ 90)

# Option 2: T·∫°o sample data l·ªõn h∆°n
python3 << 'EOF'
from create_sample_data import *
businesses = generate_business_data(num_businesses=500)
reviews = generate_review_data(num_reviews=5000, num_businesses=500)
save_json_lines(businesses, "../data/business.json")
save_json_lines(reviews, "../data/review.json")
EOF
```

---

## 6. CHECKLIST HO√ÄN TH√ÄNH

### Phase 1: Setup ‚úÖ
- [ ] Python 3.8+ installed
- [ ] Java 11+ installed
- [ ] PySpark 4.0.1 installed
- [ ] Pandas + PyArrow installed
- [ ] Data files ready

### Phase 2: Individual Tests ‚úÖ
- [ ] UDF test PASSED
- [ ] Window Functions test PASSED
- [ ] Pivot/Unpivot test PASSED
- [ ] Broadcast Join test PASSED

### Phase 3: Integration ‚úÖ
- [ ] test_local_features.py --test all PASSED
- [ ] batch_main_v2.py runs successfully
- [ ] 9 analyses completed
- [ ] Output files created

### Phase 4: Verification ‚úÖ
- [ ] Output files readable
- [ ] Results make sense (no NaN, no empty)
- [ ] Performance acceptable (< 60s for sample data)
- [ ] Spark UI confirms optimizations

---

## 7. NEXT STEPS

**Sau khi t·∫•t c·∫£ tests PASS:**

1. ‚úÖ **Commit code**
2. ‚úÖ **Update documentation**
3. ‚úÖ **Prepare demo**
4. üéâ **Score: 42% ‚Üí 64% achieved!**

**N·∫øu mu·ªën l√™n 83% (Ph∆∞∆°ng √°n 2):**
- Add Machine Learning Pipeline
- Add Graph Processing
- Add Advanced Statistics

---

**Congratulations! üéâ**

B·∫°n ƒë√£ ho√†n th√†nh Ph∆∞∆°ng √°n 1 v·ªõi 4 k·ªπ nƒÉng Spark n√¢ng cao:
- ‚úÖ Window Functions
- ‚úÖ Broadcast Join
- ‚úÖ Pivot/Unpivot
- ‚úÖ UDF/Pandas UDF

**Skill score: 42% ‚Üí 64%** ‚≠ê‚≠ê‚≠ê‚≠ê
