# üöÄ PH∆Ø∆†NG √ÅN 1 - IMPLEMENTATION PLAN & LOCAL TEST GUIDE

**M·ª•c ti√™u:** Tri·ªÉn khai 4 k·ªπ nƒÉng Spark n√¢ng cao v√† test tr√™n local

**Timeline:** 2-3 ng√†y

**Score:** 42% ‚Üí 64%

---

## üìã M·ª§C L·ª§C

1. [T·ªïng quan tri·ªÉn khai](#1-t·ªïng-quan-tri·ªÉn-khai)
2. [Y√™u c·∫ßu m√¥i tr∆∞·ªùng](#2-y√™u-c·∫ßu-m√¥i-tr∆∞·ªùng)
3. [Chi ti·∫øt t·ª´ng ph·∫ßn](#3-chi-ti·∫øt-t·ª´ng-ph·∫ßn)
4. [C√°ch test local](#4-c√°ch-test-local)
5. [Troubleshooting](#5-troubleshooting)

---

## 1. T·ªîNG QUAN TRI·ªÇN KHAI

### üéØ 4 K·ªπ nƒÉng s·∫Ω th√™m:

| # | K·ªπ nƒÉng | File m·ªõi/s·ª≠a | Test ƒë·ªôc l·∫≠p? | Th·ªùi gian |
|---|---------|--------------|---------------|-----------|
| 1 | **Window Functions** | `batch_analytics_advanced.py` | ‚úÖ C√≥ | 1 ng√†y |
| 2 | **Broadcast Join** | Refactor 7 analyses c≈© | ‚úÖ C√≥ | 0.5 ng√†y |
| 3 | **Pivot/Unpivot** | `batch_analytics_advanced.py` | ‚úÖ C√≥ | 0.5 ng√†y |
| 4 | **UDF/Pandas UDF** | `batch_udf.py` | ‚úÖ C√≥ | 1 ng√†y |

### üìÅ C·∫•u tr√∫c files m·ªõi:

```
Spark_Batch/
‚îú‚îÄ‚îÄ batch_analytics.py                  (hi·ªán t·∫°i - s·∫Ω refactor)
‚îú‚îÄ‚îÄ batch_analytics_advanced.py         (M·ªöI - Analysis 8, 9)
‚îú‚îÄ‚îÄ batch_udf.py                        (M·ªöI - UDF collection)
‚îú‚îÄ‚îÄ batch_main_v2.py                    (M·ªöI - Run all 9 analyses)
‚îú‚îÄ‚îÄ test_local_features.py              (M·ªöI - Test script)
‚îî‚îÄ‚îÄ LOCAL_TEST_GUIDE.md                 (M·ªöI - H∆∞·ªõng d·∫´n test)
```

### üîÑ Workflow tri·ªÉn khai:

```
Step 1: T·∫°o UDF library
   ‚Üì
Step 2: Implement Window Functions (Analysis 8)
   ‚Üì (Test ƒë·ªôc l·∫≠p)
Step 3: Implement Pivot/Unpivot (Analysis 9)
   ‚Üì (Test ƒë·ªôc l·∫≠p)
Step 4: Refactor 7 analyses v·ªõi Broadcast Join
   ‚Üì (Test compare k·∫øt qu·∫£)
Step 5: T√≠ch h·ª£p t·∫•t c·∫£ v√†o main_v2.py
   ‚Üì
Step 6: Full test v·ªõi real data
```

---

## 2. Y√äU C·∫¶U M√îI TR∆Ø·ªúNG

### ‚úÖ ƒê√£ c√≥ (t·ª´ Spark_Batch):
- Python 3.8+
- Java 11+
- PySpark 4.0.1
- Data files (business.json, review.json)

### ‚ûï Th√™m cho Ph∆∞∆°ng √°n 1:
```bash
# C√†i th√™m cho Pandas UDF
pip install pandas==2.0.3 pyarrow==12.0.0
```

### üíæ Dung l∆∞·ª£ng c·∫ßn thi·∫øt:
- Code m·ªõi: ~5KB
- Sample data (n·∫øu test): ~10MB
- Output results: ~50MB

---

## 3. CHI TI·∫æT T·ª™NG PH·∫¶N

### üìä 3.1. WINDOW FUNCTIONS (Priority 1)

#### üéØ M·ª•c ti√™u:
T·∫°o Analysis 8: **Trending Businesses** - Ph√°t hi·ªán businesses ƒëang tƒÉng tr∆∞·ªüng

#### üîß K·ªπ nƒÉng th·ªÉ hi·ªán:
- `lag()`, `lead()` - So s√°nh v·ªõi tu·∫ßn tr∆∞·ªõc/sau
- `avg()` over window - Moving average 4 tu·∫ßn
- `dense_rank()` - Ranking trong tu·∫ßn
- `rowsBetween()` - Window frame specification
- `partitionBy()` - Partition theo business_id

#### üí° √ù t∆∞·ªüng:
```
Input: review_df
  ‚Üì
Group by business_id + week (sliding window 7 days)
  ‚Üì
Calculate: weekly_count, prev_week_count
  ‚Üì
Calculate: growth_rate = (current - prev) / prev
  ‚Üì
Calculate: avg_last_4_weeks (moving average)
  ‚Üì
Rank businesses by growth_rate
  ‚Üì
Output: Top trending businesses
```

#### üìù Pseudo code:
```python
# 1. Group by week
weekly_reviews = review_df.groupBy(
    "business_id",
    window("review_date", "7 days")
).agg(count("review_id"))

# 2. Window functions
windowSpec = Window.partitionBy("business_id").orderBy("week_start")

trending = weekly_reviews.withColumn(
    "prev_week_count", lag("weekly_count", 1).over(windowSpec)
).withColumn(
    "growth_rate", (col("weekly_count") - col("prev_week_count")) / col("prev_week_count")
).withColumn(
    "avg_last_4_weeks", avg("weekly_count").over(windowSpec.rowsBetween(-3, 0))
).withColumn(
    "rank", dense_rank().over(Window.orderBy(desc("growth_rate")))
)
```

#### üß™ Test ƒë·ªôc l·∫≠p:
```bash
# Test ch·ªâ Analysis 8
python3 test_local_features.py --test window

# Expected output:
# - Top 10 businesses with highest growth rate
# - C√°c columns: business_id, name, weekly_count, growth_rate, avg_last_4_weeks, rank
```

---

### üîó 3.2. BROADCAST JOIN (Priority 2)

#### üéØ M·ª•c ti√™u:
Refactor 7 analyses hi·ªán t·∫°i ƒë·ªÉ d√πng explicit broadcast join

#### üîß K·ªπ nƒÉng th·ªÉ hi·ªán:
- `broadcast()` function explicit
- Optimization cho small table joins
- Before/after performance comparison

#### üí° Thay ƒë·ªïi:

**BEFORE (implicit):**
```python
result = top_candidates.join(
    business_df.select("business_id", "name", "city"),
    "business_id"
)
```

**AFTER (explicit broadcast):**
```python
from pyspark.sql.functions import broadcast

result = top_candidates.join(
    broadcast(business_df.select("business_id", "name", "city")),
    "business_id"
)
```

#### üìÇ Files c·∫ßn s·ª≠a:
- `batch_analytics.py`:
  - Analysis 1: line ~51-53
  - Analysis 3: line ~137-142
  - Analysis 4: line ~203-208
  - Analysis 6: line ~276-279
  - Analysis 7: line ~316

#### üß™ Test ƒë·ªôc l·∫≠p:
```bash
# Test broadcast join performance
python3 test_local_features.py --test broadcast

# Expected:
# - K·∫øt qu·∫£ gi·ªëng h·ªát tr∆∞·ªõc (correctness)
# - Th·ªùi gian ch·∫°y nhanh h∆°n 10-30% (performance)
# - Spark UI confirm broadcast join (check physical plan)
```

---

### üîÑ 3.3. PIVOT/UNPIVOT (Priority 3)

#### üéØ M·ª•c ti√™u:
T·∫°o Analysis 9: **Category Performance Matrix** - Pivot categories vs cities

#### üîß K·ªπ nƒÉng th·ªÉ hi·ªán:
- `pivot()` - Wide format transformation
- `unpivot()` / `stack()` - Long format transformation
- Cross-tabulation analysis

#### üí° √ù t∆∞·ªüng:
```
Input: business_df + review_df
  ‚Üì
Explode categories
  ‚Üì
Join with reviews
  ‚Üì
Aggregate: avg_stars, review_count per (category, city)
  ‚Üì
Pivot: categories as rows, cities as columns
  ‚Üì
Output: Performance matrix
```

#### üìù Pseudo code:
```python
# 1. Explode and join
df = business_df.withColumn("category", explode(split(col("categories"), ",")))
joined = df.join(review_df, "business_id")

# 2. Aggregate
agg_df = joined.groupBy("category", "city").agg(
    avg("stars").alias("avg_stars"),
    count("review_id").alias("review_count")
)

# 3. Pivot
pivoted = agg_df.groupBy("category").pivot("city").agg(
    first("avg_stars"),
    first("review_count")
)

# Bonus: Unpivot back
unpivoted = pivoted.select("category", expr("stack(N, ...)"))
```

#### üß™ Test ƒë·ªôc l·∫≠p:
```bash
# Test pivot/unpivot
python3 test_local_features.py --test pivot

# Expected:
# - Pivot: Wide table (categories x cities)
# - Unpivot: Long table (category, city, avg_stars, review_count)
```

---

### üé® 3.4. UDF / PANDAS UDF (Priority 4)

#### üéØ M·ª•c ti√™u:
T·∫°o UDF library v·ªõi 4 functions:

1. **categorize_rating** (Regular UDF) - Ph√¢n lo·∫°i rating
2. **sentiment_score** (Pandas UDF) - T√≠nh sentiment score
3. **extract_keywords** (Pandas UDF) - Extract keywords t·ª´ text
4. **is_weekend** (Regular UDF) - Check weekend

#### üîß K·ªπ nƒÉng th·ªÉ hi·ªán:
- Regular UDF v·ªõi `@udf` decorator
- Pandas UDF (vectorized) v·ªõi `@pandas_udf`
- Performance comparison: Regular vs Pandas UDF

#### üí° Functions:

**1. categorize_rating (Regular UDF):**
```python
@udf(returnType=StringType())
def categorize_rating(stars):
    """Categorize rating: Excellent, Good, Average, Poor"""
    if stars >= 4.5: return "Excellent"
    elif stars >= 3.5: return "Good"
    elif stars >= 2.5: return "Average"
    else: return "Poor"
```

**2. sentiment_score (Pandas UDF - Fast!):**
```python
@pandas_udf(FloatType())
def sentiment_score(text: pd.Series) -> pd.Series:
    """Calculate sentiment score from review text"""
    positive_words = ['great', 'excellent', 'amazing', 'love', 'best']
    negative_words = ['bad', 'terrible', 'worst', 'hate', 'awful']

    def score(t):
        pos = sum(t.lower().count(w) for w in positive_words)
        neg = sum(t.lower().count(w) for w in negative_words)
        return pos / (pos + neg + 1)

    return text.apply(score)
```

**3. extract_keywords (Pandas UDF):**
```python
@pandas_udf(StringType())
def extract_keywords(text: pd.Series) -> pd.Series:
    """Extract top keywords from text"""
    import re

    def extract(t):
        words = re.findall(r'\b[a-z]{4,}\b', t.lower())
        # Return top 5 most common
        from collections import Counter
        top = Counter(words).most_common(5)
        return ', '.join([w for w, c in top])

    return text.apply(extract)
```

**4. is_weekend (Regular UDF):**
```python
@udf(returnType=BooleanType())
def is_weekend(date_str):
    """Check if date is weekend"""
    from datetime import datetime
    dt = datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
    return dt.weekday() >= 5  # 5=Saturday, 6=Sunday
```

#### üß™ Test ƒë·ªôc l·∫≠p:
```bash
# Test UDFs
python3 test_local_features.py --test udf

# Expected:
# - All 4 UDFs work correctly
# - Pandas UDF 10-100x faster than Regular UDF
# - Output: review_df with new columns (rating_category, sentiment_score, etc.)
```

---

## 4. C√ÅCH TEST LOCAL

### üéØ Test Strategy:

**Level 1: Unit Test** (Test t·ª´ng feature ri√™ng)
```bash
cd /home/user/bigdata-2025-1/Spark_Batch

# Test Window Functions only
python3 test_local_features.py --test window --data ../data/

# Test Broadcast Join only
python3 test_local_features.py --test broadcast --data ../data/

# Test Pivot only
python3 test_local_features.py --test pivot --data ../data/

# Test UDFs only
python3 test_local_features.py --test udf --data ../data/
```

**Level 2: Integration Test** (Test t√≠ch h·ª£p)
```bash
# Test all new features
python3 test_local_features.py --test all --data ../data/

# Compare old vs new (7 analyses)
python3 test_local_features.py --test compare --data ../data/
```

**Level 3: Full Pipeline Test**
```bash
# Run full pipeline v·ªõi 9 analyses (7 old + 2 new)
python3 batch_main_v2.py --data-path ../data/
```

---

### üß™ Test Script Structure:

File `test_local_features.py` s·∫Ω c√≥:

```python
#!/usr/bin/env python3
"""
Test script for Spark advanced features (Ph∆∞∆°ng √°n 1)
"""
import argparse
from pyspark.sql import SparkSession

def test_window_functions(spark, data_path):
    """Test Analysis 8: Window Functions"""
    print("Testing Window Functions...")
    # Load data
    # Run Analysis 8
    # Verify results
    # Print summary
    pass

def test_broadcast_join(spark, data_path):
    """Test Broadcast Join optimization"""
    print("Testing Broadcast Join...")
    # Run old Analysis 1 (without broadcast)
    # Run new Analysis 1 (with broadcast)
    # Compare results (should be identical)
    # Compare performance (new should be faster)
    pass

def test_pivot_unpivot(spark, data_path):
    """Test Analysis 9: Pivot/Unpivot"""
    print("Testing Pivot/Unpivot...")
    # Run Analysis 9
    # Verify pivot result
    # Verify unpivot result
    pass

def test_udfs(spark, data_path):
    """Test UDF library"""
    print("Testing UDFs...")
    # Load UDFs
    # Apply each UDF
    # Verify outputs
    # Compare performance (Pandas vs Regular)
    pass

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--test', choices=['window', 'broadcast', 'pivot', 'udf', 'all', 'compare'])
    parser.add_argument('--data', default='../data/')
    args = parser.parse_args()

    # Create Spark session
    spark = SparkSession.builder.appName("Feature Test").getOrCreate()

    # Run tests
    if args.test == 'window':
        test_window_functions(spark, args.data)
    elif args.test == 'broadcast':
        test_broadcast_join(spark, args.data)
    # ... etc
```

---

### üìä Expected Output:

**Test Window Functions:**
```
Testing Window Functions...
‚úì Loaded 100 businesses, 1000 reviews
‚úì Analysis 8 completed in 3.45s
‚úì Found 10 trending businesses

Top 3 Trending:
+------------+------------------+-------------+------------+----------------+------+
|business_id |name              |weekly_count |growth_rate |avg_last_4_weeks|rank  |
+------------+------------------+-------------+------------+----------------+------+
|business_001|Restaurant ABC    |45           |0.875       |32.5            |1     |
|business_023|Coffee Shop XYZ   |38           |0.722       |28.3            |2     |
|business_012|Bar & Grill       |32           |0.600       |24.0            |3     |
+------------+------------------+-------------+------------+----------------+------+

‚úÖ Window Functions test PASSED
```

**Test Broadcast Join:**
```
Testing Broadcast Join...
‚úì Running old Analysis 1 (no broadcast)... 5.23s
‚úì Running new Analysis 1 (with broadcast)... 3.87s

Performance improvement: 26% faster
Results match: ‚úÖ 100% identical

Spark Physical Plan (new):
- BroadcastHashJoin confirmed ‚úÖ

‚úÖ Broadcast Join test PASSED
```

**Test Pivot/Unpivot:**
```
Testing Pivot/Unpivot...
‚úì Analysis 9 completed

Pivot result (sample):
+------------+---------+----------+-----------+
|category    |Phoenix  |Las Vegas |Toronto    |
+------------+---------+----------+-----------+
|Restaurants |4.5 (120)|4.3 (95)  |4.2 (88)   |
|Shopping    |4.1 (45) |4.0 (38)  |3.9 (42)   |
+------------+---------+----------+-----------+

Unpivot result:
+------------+-----------+-----------+-------------+
|category    |city       |avg_stars  |review_count |
+------------+-----------+-----------+-------------+
|Restaurants |Phoenix    |4.5        |120          |
|Restaurants |Las Vegas  |4.3        |95           |
...

‚úÖ Pivot/Unpivot test PASSED
```

**Test UDFs:**
```
Testing UDFs...

1. categorize_rating (Regular UDF):
   Time: 2.34s
   Sample: 4.5 ‚Üí "Excellent", 3.2 ‚Üí "Good"
   ‚úÖ PASS

2. sentiment_score (Pandas UDF):
   Time: 0.21s (11x faster!)
   Sample: "Great food!" ‚Üí 0.85, "Terrible service" ‚Üí 0.15
   ‚úÖ PASS

3. extract_keywords (Pandas UDF):
   Time: 0.45s
   Sample: "Amazing food and great service" ‚Üí "food, great, service, amazing"
   ‚úÖ PASS

4. is_weekend (Regular UDF):
   Time: 1.23s
   Sample: "2023-01-14" ‚Üí True (Saturday)
   ‚úÖ PASS

Performance comparison:
- Pandas UDF ~10-50x faster than Regular UDF ‚úÖ

‚úÖ UDF test PASSED
```

---

### üìà Verification Checklist:

**Sau m·ªói test, verify:**
- [ ] Code ch·∫°y kh√¥ng l·ªói
- [ ] Output ƒë√∫ng format expected
- [ ] Performance acceptable (< 10s v·ªõi sample data)
- [ ] K·∫øt qu·∫£ c√≥ √Ω nghƒ©a business (kh√¥ng c√≥ gi√° tr·ªã v√¥ l√Ω)
- [ ] Spark UI confirm optimization (broadcast, etc.)

---

## 5. TROUBLESHOOTING

### ‚ùå L·ªói th∆∞·ªùng g·∫∑p:

**1. Import error: pandas/pyarrow**
```bash
# Fix:
pip install pandas==2.0.3 pyarrow==12.0.0
```

**2. Window function error: "column not found"**
```python
# Fix: Ensure correct column names
windowSpec = Window.partitionBy("business_id").orderBy("week_start")
# Check: df.columns ƒë·ªÉ xem t√™n c·ªôt ch√≠nh x√°c
```

**3. Broadcast join not applied**
```python
# Fix: Check Spark UI ‚Üí SQL tab ‚Üí Physical Plan
# Should see: "BroadcastHashJoin" not "SortMergeJoin"

# Debug:
df.explain()  # Check execution plan
```

**4. UDF slow performance**
```python
# Fix: Use Pandas UDF instead of Regular UDF
# Regular UDF: Row-by-row (slow)
# Pandas UDF: Vectorized (fast)
```

**5. Out of memory**
```bash
# Fix: Reduce data size for testing
head -1000 data/review.json > data/review_sample.json

# Or increase Spark memory:
# Edit batch_configuration.py:
.config("spark.driver.memory", "8g")
```

---

## 6. EXPECTED TIMELINE

### Day 1: UDF + Window Functions
```
Morning (3-4h):
- [x] Create batch_udf.py
- [x] Test UDFs independently
- [x] Fix any issues

Afternoon (3-4h):
- [x] Create batch_analytics_advanced.py
- [x] Implement Analysis 8 (Window Functions)
- [x] Test Analysis 8
- [x] Verify trending results
```

### Day 2: Pivot + Broadcast Join
```
Morning (2-3h):
- [x] Implement Analysis 9 (Pivot/Unpivot)
- [x] Test Analysis 9
- [x] Verify matrix results

Afternoon (3-4h):
- [x] Refactor 7 analyses v·ªõi Broadcast Join
- [x] Test performance comparison
- [x] Verify correctness
```

### Day 3: Integration + Documentation
```
Morning (2-3h):
- [x] Create batch_main_v2.py
- [x] Integration test all 9 analyses
- [x] Fix any integration issues

Afternoon (2-3h):
- [x] Create test_local_features.py
- [x] Run full test suite
- [x] Update documentation
- [x] Commit & push
```

---

## 7. NEXT STEPS

### ‚úÖ Ready to start?

**T√¥i s·∫Ω t·∫°o c√°c files sau theo th·ª© t·ª±:**

1. `batch_udf.py` - UDF library
2. `batch_analytics_advanced.py` - Analysis 8, 9
3. `test_local_features.py` - Test script
4. Refactor `batch_analytics.py` - Add broadcast joins
5. `batch_main_v2.py` - Integration
6. `LOCAL_TEST_GUIDE.md` - Step-by-step test guide

**Sau ƒë√≥ b·∫°n s·∫Ω:**
1. Ch·∫°y test t·ª´ng ph·∫ßn
2. Verify k·∫øt qu·∫£
3. Ch·∫°y full pipeline
4. Review v√† approve

---

**B·∫°n ƒë√£ s·∫µn s√†ng? T√¥i s·∫Ω b·∫Øt ƒë·∫ßu t·∫°o code! üöÄ**

**Ho·∫∑c b·∫°n c√≥ c√¢u h·ªèi n√†o v·ªÅ plan n√†y kh√¥ng?**
