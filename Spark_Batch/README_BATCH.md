# ğŸš€ Yelp Big Data Analysis - BATCH MODE

**PhiÃªn báº£n Ä‘Æ¡n giáº£n hÃ³a Ä‘á»ƒ cháº¡y local trÃªn mÃ¡y tÃ­nh cÃ¡ nhÃ¢n**

---

## ğŸ“‹ Má»¤C Lá»¤C

- [Tá»•ng quan](#-tá»•ng-quan)
- [YÃªu cáº§u há»‡ thá»‘ng](#-yÃªu-cáº§u-há»‡-thá»‘ng)
- [CÃ i Ä‘áº·t](#-cÃ i-Ä‘áº·t)
- [Chuáº©n bá»‹ dá»¯ liá»‡u](#-chuáº©n-bá»‹-dá»¯-liá»‡u)
- [Cháº¡y phÃ¢n tÃ­ch](#-cháº¡y-phÃ¢n-tÃ­ch)
- [Káº¿t quáº£](#-káº¿t-quáº£)
- [TÃ¹y chá»‰nh](#-tÃ¹y-chá»‰nh)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ“– Tá»”NG QUAN

PhiÃªn báº£n batch mode nÃ y cho phÃ©p báº¡n:
- âœ… Cháº¡y 7 hÃ m phÃ¢n tÃ­ch Yelp trÃªn mÃ¡y local
- âœ… KhÃ´ng cáº§n Docker, Kafka, HDFS
- âœ… Äá»c dá»¯ liá»‡u tá»« file JSON local
- âœ… Xem káº¿t quáº£ trá»±c tiáº¿p trÃªn console
- âœ… LÆ°u káº¿t quáº£ ra file (Parquet/CSV/JSON)

### Kiáº¿n trÃºc Ä‘Æ¡n giáº£n:

```
Local Data Files (JSON)
    â†“
PySpark Batch Processing
    â†“
7 Analytics Functions
    â†“
Console Output + Saved Files
```

### 7 hÃ m phÃ¢n tÃ­ch:

1. **Top Selling Products** - Sáº£n pháº©m bÃ¡n cháº¡y gáº§n Ä‘Ã¢y
2. **Diverse Stores** - Cá»­a hÃ ng Ä‘a dáº¡ng nháº¥t
3. **Best Rated** - ÄÃ¡nh giÃ¡ tá»‘t nháº¥t
4. **Positive Reviews** - Review tÃ­ch cá»±c nháº¥t
5. **Peak Hours** - Thá»i gian cao Ä‘iá»ƒm
6. **Top Categories** - Categories phá»• biáº¿n
7. **Store Statistics** - Thá»‘ng kÃª tá»•ng há»£p

---

## ğŸ’» YÃŠU Cáº¦U Há»† THá»NG

### Pháº§n cá»©ng:
- **RAM**: 4GB minimum, 8GB+ khuyáº¿n nghá»‹
- **Disk**: ~2GB trá»‘ng (cho PySpark + output)
- **CPU**: 2+ cores

### Pháº§n má»m:
- **OS**: Ubuntu/Linux (Ä‘Ã£ test), MacOS, Windows
- **Python**: 3.8 hoáº·c cao hÆ¡n
- **Java**: Java 11 hoáº·c cao hÆ¡n (required cho PySpark)

### Python packages:
- `pyspark==4.0.1`
- `requests`

---

## ğŸ”§ CÃ€I Äáº¶T

### BÆ°á»›c 1: Kiá»ƒm tra Python

```bash
python3 --version
# Cáº§n: Python 3.8+
```

Náº¿u chÆ°a cÃ³ Python 3.8+:
```bash
# Ubuntu/Debian
sudo apt update
sudo apt install python3 python3-pip

# CentOS/RHEL
sudo yum install python3 python3-pip
```

### BÆ°á»›c 2: CÃ i Ä‘áº·t Java

```bash
java -version
# Cáº§n: Java 11+
```

Náº¿u chÆ°a cÃ³ Java:
```bash
# Ubuntu/Debian
sudo apt install openjdk-11-jdk

# Download tá»«: https://adoptium.net/
```

### BÆ°á»›c 3: CÃ i Ä‘áº·t PySpark

```bash
# CÃ i Ä‘áº·t dependencies
pip install pyspark==4.0.1 requests

# Hoáº·c dÃ¹ng pip3
pip3 install pyspark==4.0.1 requests
```

### BÆ°á»›c 4: Verify cÃ i Ä‘áº·t

```bash
python3 -c "import pyspark; print(pyspark.__version__)"
# Expected output: 4.0.1
```

---

## ğŸ“ CHUáº¨N Bá»Š Dá»® LIá»†U

### Cáº¥u trÃºc thÆ° má»¥c cáº§n cÃ³:

```
bigdata-2025-1/
â”œâ”€â”€ Spark_Batch/          â† Code á»Ÿ Ä‘Ã¢y (Ä‘Ã£ cÃ³)
â”‚   â”œâ”€â”€ batch_main.py
â”‚   â”œâ”€â”€ batch_configuration.py
â”‚   â”œâ”€â”€ batch_load_data.py
â”‚   â”œâ”€â”€ batch_analytics.py
â”‚   â”œâ”€â”€ batch_pipeline.py
â”‚   â”œâ”€â”€ run_local.sh
â”‚   â””â”€â”€ README_BATCH.md   â† File nÃ y
â”‚
â””â”€â”€ data/                 â† Data Ä‘áº·t á»Ÿ Ä‘Ã¢y (cáº§n táº¡o)
    â”œâ”€â”€ business.json
    â””â”€â”€ review.json
```

### Táº¡o thÆ° má»¥c data:

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c project
cd /home/user/bigdata-2025-1

# Táº¡o thÆ° má»¥c data
mkdir -p data
```

### Copy data files:

```bash
# Copy data files cá»§a báº¡n vÃ o thÆ° má»¥c data/
cp /path/to/your/business.json data/
cp /path/to/your/review.json data/

# Verify
ls -lh data/
```

### Format data cáº§n cÃ³:

**business.json** (má»—i dÃ²ng lÃ  1 JSON object):
```json
{"business_id":"abc123","name":"Restaurant Name","city":"Phoenix","state":"AZ","categories":"Food, Restaurant","stars":4.5,"review_count":120,"is_open":1,"latitude":33.4484,"longitude":-112.074}
```

**review.json** (má»—i dÃ²ng lÃ  1 JSON object):
```json
{"review_id":"xyz789","business_id":"abc123","user_id":"user456","stars":5.0,"date":"2022-01-15 10:30:00","text":"Great!","useful":10,"funny":2,"cool":5}
```

---

## ğŸš€ CHáº Y PHÃ‚N TÃCH

### CÃ¡ch 1: Sá»­ dá»¥ng script nhanh (Khuyáº¿n nghá»‹)

```bash
# Di chuyá»ƒn vÃ o thÆ° má»¥c Spark_Batch
cd Spark_Batch

# Cháº¡y vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh
./run_local.sh

# Cháº¡y vá»›i custom data path
./run_local.sh --data-path ../data/

# Cháº¡y vÃ  lÆ°u káº¿t quáº£ dáº¡ng CSV
./run_local.sh --save-format csv

# Chá»‰ xem káº¿t quáº£, khÃ´ng lÆ°u file
./run_local.sh --skip-save

# Xem thÃªm options
./run_local.sh --help
```

### CÃ¡ch 2: Cháº¡y trá»±c tiáº¿p vá»›i Python

```bash
cd Spark_Batch

# Cháº¡y vá»›i cáº¥u hÃ¬nh máº·c Ä‘á»‹nh
python3 batch_main.py

# Cháº¡y vá»›i custom paths
python3 batch_main.py --data-path ../data/ --output-path ../output/

# Xem thÃªm options
python3 batch_main.py --help
```

### Thá»i gian cháº¡y dá»± kiáº¿n:

- **Small dataset** (1K businesses, 10K reviews): ~2-5 phÃºt
- **Medium dataset** (10K businesses, 100K reviews): ~5-10 phÃºt
- **Large dataset** (100K+ businesses, 1M+ reviews): ~15-30 phÃºt

---

## ğŸ“Š Káº¾T QUáº¢

### Káº¿t quáº£ trÃªn console:

Sau khi cháº¡y xong, báº¡n sáº½ tháº¥y káº¿t quáº£ cá»§a cáº£ 7 phÃ¢n tÃ­ch hiá»ƒn thá»‹ trÃªn console:

```
================================================================================
                            RESULTS PREVIEW
================================================================================

=============================== TOP SELLING ====================================
+--------------------+------------------------+----------+-------+
|business_id         |name                    |city      |recent_|
|                    |                        |          |review_|
|                    |                        |          |count  |
+--------------------+------------------------+----------+-------+
|abc123              |Restaurant ABC          |Phoenix   |450    |
|xyz789              |Store XYZ               |Las Vegas |320    |
...
```

### Káº¿t quáº£ lÆ°u file:

Náº¿u khÃ´ng dÃ¹ng `--skip-save`, káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u vÃ o `./output/`:

```
output/
â”œâ”€â”€ top_selling/          â† Analysis 1
â”œâ”€â”€ diverse_stores/       â† Analysis 2
â”œâ”€â”€ best_rated/           â† Analysis 3
â”œâ”€â”€ most_positive/        â† Analysis 4
â”œâ”€â”€ peak_hours/           â† Analysis 5
â”œâ”€â”€ top_categories/       â† Analysis 6
â””â”€â”€ store_stats/          â† Analysis 7
```

### Äá»c káº¿t quáº£ Ä‘Ã£ lÆ°u:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Äá»c Parquet
df = spark.read.parquet("output/top_selling/")
df.show()

# Äá»c CSV
df = spark.read.csv("output/top_selling/", header=True)
df.show()
```

---

## âš™ï¸ TÃ™Y CHá»ˆNH

### Thay Ä‘á»•i cáº¥u hÃ¬nh phÃ¢n tÃ­ch:

Edit file `batch_main.py`, dÃ²ng ~86:

```python
pipeline.run_all_analyses(config={
    'analysis_1': {'days': 90, 'top_n': 10},      # Thay Ä‘á»•i sá»‘ ngÃ y, top N
    'analysis_2': {'top_n': 10},
    'analysis_3': {'min_reviews': 10, 'top_n': 10},
    'analysis_4': {'positive_threshold': 4, 'top_n': 10},
    'analysis_6': {'top_n': 20}
})
```

### Cháº¡y riÃªng 1 phÃ¢n tÃ­ch:

```python
from batch_main import run_single_analysis

# Chá»‰ cháº¡y Analysis 1
result = run_single_analysis(
    analysis_number=1,
    data_path='../data/',
    days=30,
    top_n=5
)
```

### Äiá»u chá»‰nh Spark memory:

Edit file `batch_configuration.py`, dÃ²ng ~28:

```python
.config("spark.driver.memory", "8g")      # TÄƒng/giáº£m náº¿u cáº§n
.config("spark.executor.memory", "4g")
```

---

## ğŸ†˜ TROUBLESHOOTING

### Lá»—i 1: FileNotFoundError: data/business.json

**NguyÃªn nhÃ¢n**: Data files khÃ´ng tá»“n táº¡i hoáº·c sai Ä‘Æ°á»ng dáº«n

**Giáº£i phÃ¡p**:
```bash
# Kiá»ƒm tra data files
ls -la data/

# Äáº£m báº£o cÃ³ business.json vÃ  review.json
# Náº¿u data á»Ÿ chá»— khÃ¡c, chá»‰ Ä‘á»‹nh Ä‘Æ°á»ng dáº«n:
./run_local.sh --data-path /path/to/your/data/
```

### Lá»—i 2: Java not found

**NguyÃªn nhÃ¢n**: Java chÆ°a cÃ i hoáº·c khÃ´ng cÃ³ trong PATH

**Giáº£i phÃ¡p**:
```bash
# CÃ i Java 11
sudo apt install openjdk-11-jdk

# Hoáº·c download tá»«: https://adoptium.net/

# Verify
java -version
```

### Lá»—i 3: Out of memory / Java heap space

**NguyÃªn nhÃ¢n**: Data quÃ¡ lá»›n so vá»›i RAM

**Giáº£i phÃ¡p**:
1. Giáº£m Spark memory trong `batch_configuration.py`:
```python
.config("spark.driver.memory", "4g")  # Giáº£m tá»« 8g
```

2. Lá»c data trÆ°á»›c khi phÃ¢n tÃ­ch:
```python
# Trong batch_load_data.py
review_df = review_df.filter(col("date") >= "2022-01-01")
```

### Lá»—i 4: Module 'pyspark' not found

**NguyÃªn nhÃ¢n**: PySpark chÆ°a cÃ i Ä‘áº·t

**Giáº£i phÃ¡p**:
```bash
pip3 install pyspark==4.0.1 requests

# Verify
python3 -c "import pyspark"
```

### Lá»—i 5: Permission denied: ./run_local.sh

**NguyÃªn nhÃ¢n**: Script khÃ´ng cÃ³ quyá»n execute

**Giáº£i phÃ¡p**:
```bash
chmod +x run_local.sh
./run_local.sh
```

### Lá»—i 6: JSON parsing error

**NguyÃªn nhÃ¢n**: Data file khÃ´ng Ä‘Ãºng format JSON

**Giáº£i phÃ¡p**:
- Kiá»ƒm tra format: má»—i dÃ²ng pháº£i lÃ  1 JSON object há»£p lá»‡
- KhÃ´ng Ä‘Æ°á»£c cÃ³ JSON array `[...]`
- Encoding pháº£i lÃ  UTF-8

```bash
# Kiá»ƒm tra 5 dÃ²ng Ä‘áº§u
head -5 data/business.json

# Má»—i dÃ²ng pháº£i báº¯t Ä‘áº§u báº±ng { vÃ  káº¿t thÃºc báº±ng }
```

---

## ğŸ¯ WORKFLOW Äá»€ XUáº¤T

### Láº§n Ä‘áº§u cháº¡y:

```bash
# 1. CÃ i Ä‘áº·t dependencies
pip3 install pyspark==4.0.1 requests

# 2. Chuáº©n bá»‹ data
mkdir -p data
cp /path/to/your/business.json data/
cp /path/to/your/review.json data/

# 3. Test vá»›i dataset nhá» trÆ°á»›c
head -1000 data/business.json > data/business_sample.json
head -10000 data/review.json > data/review_sample.json

# 4. Cháº¡y vá»›i sample data
cd Spark_Batch
./run_local.sh --data-path ../data/ --skip-save

# 5. Náº¿u OK, cháº¡y vá»›i full data
./run_local.sh --data-path ../data/
```

### Development workflow:

```bash
# Test 1 analysis
python3 -c "from batch_main import run_single_analysis; run_single_analysis(1, '../data/', days=30)"

# Test full pipeline
./run_local.sh --skip-save

# Cháº¡y vÃ  lÆ°u káº¿t quáº£
./run_local.sh --save-format parquet
```

---

## ğŸ“ˆ TIPS & BEST PRACTICES

### 1. TÄƒng performance:
```python
# Giáº£m shuffle partitions cho small data
.config("spark.sql.shuffle.partitions", "10")  # Default: 20

# Cache dataframes thÆ°á»ng dÃ¹ng
business_df.cache()
```

### 2. Debug:
```python
# Báº­t DEBUG logs
spark.sparkContext.setLogLevel("INFO")

# Xem execution plan
df.explain()
```

### 3. Monitor:
- Spark UI: http://localhost:4040 (trong khi cháº¡y)
- Check memory: `free -h`
- Check CPU: `top`

---

## ğŸ“ Há»– TRá»¢

### Náº¿u gáº·p váº¥n Ä‘á»:

1. **Check logs** - Äá»c error message ká»¹
2. **Verify data** - Kiá»ƒm tra format JSON
3. **Test small** - Cháº¡y vá»›i sample data trÆ°á»›c
4. **Check resources** - RAM, disk space Ä‘á»§ chÆ°a

### ThÃ´ng tin há»¯u Ã­ch:

- PySpark docs: https://spark.apache.org/docs/latest/api/python/
- Yelp dataset: https://www.yelp.com/dataset
- Project issues: [GitHub Issues]

---

## âœ… CHECKLIST TRÆ¯á»šC KHI CHáº Y

- [ ] Python 3.8+ Ä‘Ã£ cÃ i
- [ ] Java 11+ Ä‘Ã£ cÃ i
- [ ] PySpark 4.0.1 Ä‘Ã£ cÃ i (`pip3 install pyspark==4.0.1`)
- [ ] Data files tá»“n táº¡i trong `data/business.json` vÃ  `data/review.json`
- [ ] Data files Ä‘Ãºng format (JSON lines)
- [ ] Äá»§ RAM (~4GB+ free)
- [ ] Äá»§ disk space (~2GB+)

---

## ğŸ‰ Káº¾T LUáº¬N

Báº¡n Ä‘Ã£ cÃ³ má»™t phiÃªn báº£n Ä‘Æ¡n giáº£n Ä‘á»ƒ cháº¡y 7 phÃ¢n tÃ­ch Yelp trÃªn local!

**CÃ¡c bÆ°á»›c chÃ­nh:**
1. âœ… CÃ i Python + Java + PySpark
2. âœ… Chuáº©n bá»‹ data files
3. âœ… Cháº¡y `./run_local.sh`
4. âœ… Xem káº¿t quáº£!

**ChÃºc báº¡n phÃ¢n tÃ­ch thÃ nh cÃ´ng! ğŸš€ğŸ“Š**

---

*Version: 1.0.0*
*Last Updated: 2025-12-15*
