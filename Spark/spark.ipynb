{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5165c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\"\"\"\n",
    "Yelp Big Data Analysis System\n",
    "Optimized PySpark Pipeline for Large-Scale Data Processing\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType, BooleanType\n",
    ")\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION & INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "class SparkConfig:\n",
    "    \"\"\"Spark configuration optimized for big data processing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_spark_session():\n",
    "        \"\"\"\n",
    "        Initialize Spark Session with optimized configurations\n",
    "        \"\"\"\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"Yelp Big Data Analysis System\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") \\\n",
    "            .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "            .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "            .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "            .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\") \\\n",
    "            .config(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"10000\") \\\n",
    "            .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
    "            .config(\"spark.sql.files.openCostInBytes\", \"4194304\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        spark.sparkContext.setCheckpointDir(\"checkpoints/\")\n",
    "        \n",
    "        return spark\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA SCHEMAS\n",
    "# ============================================================================\n",
    "\n",
    "class YelpSchemas:\n",
    "    \"\"\"Explicit schemas for better performance\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def business_schema():\n",
    "        return StructType([\n",
    "            StructField(\"business_id\", StringType(), False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"review_count\", IntegerType(), True),\n",
    "            StructField(\"is_open\", IntegerType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def review_schema():\n",
    "        return StructType([\n",
    "            StructField(\"review_id\", StringType(), False),\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"stars\", IntegerType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"useful\", IntegerType(), True),\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def user_schema():\n",
    "        return StructType([\n",
    "            StructField(\"user_id\", StringType(), False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"review_count\", IntegerType(), True),\n",
    "            StructField(\"yelping_since\", StringType(), True),\n",
    "            StructField(\"useful\", IntegerType(), True),\n",
    "            StructField(\"fans\", IntegerType(), True),\n",
    "            StructField(\"average_stars\", DoubleType(), True),\n",
    "        ])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING & PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Handles data loading and preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, spark, data_path=\"../data/\"):\n",
    "        self.spark = spark\n",
    "        self.data_path = data_path\n",
    "        self.schemas = YelpSchemas()\n",
    "    \n",
    "    def load_business_data(self):\n",
    "        \"\"\"Load and prepare business data\"\"\"\n",
    "        print(\"Loading business data...\")\n",
    "        \n",
    "        business_df = self.spark.read \\\n",
    "            .schema(self.schemas.business_schema()) \\\n",
    "            .json(self.data_path + 'business.json')\n",
    "        \n",
    "        # Repartition và cache\n",
    "        business_df = business_df \\\n",
    "            .repartition(100, \"business_id\") \\\n",
    "            .cache()\n",
    "        \n",
    "        # Trigger cache\n",
    "        count = business_df.count()\n",
    "        print(f\"Loaded {count:,} businesses\")\n",
    "        \n",
    "        return business_df\n",
    "    \n",
    "    def load_review_data(self):\n",
    "        \"\"\"Load and prepare review data\"\"\"\n",
    "        print(\"Loading review data...\")\n",
    "        \n",
    "        review_df = self.spark.read \\\n",
    "            .schema(self.schemas.review_schema()) \\\n",
    "            .json(self.data_path + 'review.json')\n",
    "        \n",
    "        # Preprocess dates\n",
    "        review_df = review_df \\\n",
    "            .withColumn(\"review_date\", to_date(col(\"date\"))) \\\n",
    "            .withColumn(\"review_timestamp\", unix_timestamp(col(\"date\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "            .withColumn(\"review_year\", year(col(\"date\"))) \\\n",
    "            .withColumn(\"review_month\", month(col(\"date\")))\n",
    "        \n",
    "        # Repartition by business_id for joins\n",
    "        review_df = review_df.repartition(200, \"business_id\")\n",
    "        \n",
    "        count = review_df.count()\n",
    "        print(f\"Loaded {count:,} reviews\")\n",
    "        \n",
    "        return review_df\n",
    "    \n",
    "    def load_user_data(self):\n",
    "        \"\"\"Load user data\"\"\"\n",
    "        print(\"Loading user data...\")\n",
    "        \n",
    "        user_df = self.spark.read \\\n",
    "            .schema(self.schemas.user_schema()) \\\n",
    "            .json(self.data_path + 'user.json')\n",
    "        \n",
    "        count = user_df.count()\n",
    "        print(f\"Loaded {count:,} users\")\n",
    "        \n",
    "        return user_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f02a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSIS FUNCTIONS - OPTIMIZED FOR BIG DATA\n",
    "# ============================================================================\n",
    "\n",
    "class YelpAnalytics:\n",
    "    \"\"\"Core analytics functions optimized for big data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_selling_products_recent(review_df, business_df, days=90, top_n=10):\n",
    "        \"\"\"\n",
    "        1. Top sản phẩm (doanh nghiệp) bán chạy nhất trong khoảng thời gian gần\n",
    "        \n",
    "        Optimizations:\n",
    "        - Salting to handle data skew\n",
    "        - Two-stage aggregation\n",
    "        - Broadcast join for business info\n",
    "        - Early filtering and limiting\n",
    "        \n",
    "        Args:\n",
    "            review_df: DataFrame chứa dữ liệu review (đã preprocess dates)\n",
    "            business_df: DataFrame chứa dữ liệu business\n",
    "            days: số ngày gần đây cần phân tích\n",
    "            top_n: số lượng top sản phẩm\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame với top sản phẩm bán chạy nhất\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 1: Top {top_n} Selling Products (Last {days} days)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Add salt to handle skew\n",
    "        review_with_salt = review_df.withColumn(\"salt\", (rand() * 10).cast(\"int\"))\n",
    "        \n",
    "        # Filter by date range\n",
    "        cutoff_date = current_date() - lit(days)\n",
    "        recent_reviews = review_with_salt.filter(col(\"review_date\") >= cutoff_date)\n",
    "        \n",
    "        # Stage 1: Salted aggregation\n",
    "        salted_agg = recent_reviews.groupBy(\"business_id\", \"salt\").agg(\n",
    "            count(\"review_id\").alias(\"partial_count\"),\n",
    "            sum(\"stars\").alias(\"partial_sum_stars\"),\n",
    "            count(\"stars\").alias(\"partial_count_stars\")\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Final aggregation\n",
    "        business_stats = salted_agg.groupBy(\"business_id\").agg(\n",
    "            sum(\"partial_count\").alias(\"recent_review_count\"),\n",
    "            (sum(\"partial_sum_stars\") / sum(\"partial_count_stars\")).alias(\"avg_rating\")\n",
    "        )\n",
    "        \n",
    "        # Get top candidates before join\n",
    "        top_candidates = business_stats \\\n",
    "            .orderBy(desc(\"recent_review_count\")) \\\n",
    "            .limit(top_n * 10)\n",
    "        \n",
    "        # Broadcast join with business info\n",
    "        result = top_candidates.join(\n",
    "            broadcast(business_df.select(\n",
    "                \"business_id\", \"name\", \"city\", \"state\", \"categories\"\n",
    "            )),\n",
    "            \"business_id\"\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"categories\",\n",
    "            \"recent_review_count\",\n",
    "            \"avg_rating\"\n",
    "        ).orderBy(desc(\"recent_review_count\")).limit(top_n)\n",
    "        \n",
    "        # Materialize result\n",
    "        result_count = result.count()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} results\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_stores_by_product_count(business_df, top_n=10):\n",
    "        \"\"\"\n",
    "        2. Cửa hàng bán nhiều sản phẩm nhất (dựa trên categories)\n",
    "        \n",
    "        Optimizations:\n",
    "        - Early null filtering\n",
    "        - Minimal column selection\n",
    "        - Efficient string processing\n",
    "        \n",
    "        Args:\n",
    "            business_df: DataFrame chứa dữ liệu business\n",
    "            top_n: số lượng top cửa hàng\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame với top cửa hàng đa dạng nhất\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 2: Top {top_n} Stores by Product Diversity\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Filter and select only needed columns\n",
    "        business_filtered = business_df \\\n",
    "            .filter(col(\"categories\").isNotNull()) \\\n",
    "            .filter(length(col(\"categories\")) > 0) \\\n",
    "            .select(\n",
    "                \"business_id\", \"name\", \"city\", \"state\", \n",
    "                \"categories\", \"review_count\", \"stars\"\n",
    "            )\n",
    "        \n",
    "        # Count categories\n",
    "        result = business_filtered.withColumn(\n",
    "            \"category_count\",\n",
    "            size(split(trim(col(\"categories\")), \"\\\\s*,\\\\s*\"))\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"categories\",\n",
    "            \"category_count\",\n",
    "            \"review_count\",\n",
    "            \"stars\"\n",
    "        ).orderBy(\n",
    "            desc(\"category_count\"), \n",
    "            desc(\"review_count\")\n",
    "        ).limit(top_n)\n",
    "        \n",
    "        result_count = result.count()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} results\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_rated_products(business_df, review_df, min_reviews=50, top_n=10):\n",
    "        \"\"\"\n",
    "        3. Sản phẩm (doanh nghiệp) đánh giá tích cực nhất\n",
    "        \n",
    "        Optimizations:\n",
    "        - Partitioning by business_id\n",
    "        - Strategic caching\n",
    "        - Early filtering by min_reviews\n",
    "        - Broadcast join\n",
    "        \n",
    "        Args:\n",
    "            business_df: DataFrame chứa dữ liệu business\n",
    "            review_df: DataFrame chứa dữ liệu review\n",
    "            min_reviews: số lượng review tối thiểu\n",
    "            top_n: số lượng top sản phẩm\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame với top sản phẩm có rating cao nhất\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 3: Top {top_n} Rated Products (Min {min_reviews} reviews)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Repartition and cache\n",
    "        review_partitioned = review_df \\\n",
    "            .select(\"business_id\", \"review_id\", \"stars\", \"useful\") \\\n",
    "            .repartition(200, \"business_id\") \\\n",
    "            .cache()\n",
    "        \n",
    "        # Aggregate review stats\n",
    "        business_stats = review_partitioned \\\n",
    "            .filter(col(\"stars\").isNotNull()) \\\n",
    "            .groupBy(\"business_id\") \\\n",
    "            .agg(\n",
    "                count(\"review_id\").alias(\"total_reviews\"),\n",
    "                avg(\"stars\").alias(\"avg_review_stars\"),\n",
    "                sum(\"useful\").alias(\"total_useful\")\n",
    "            )\n",
    "        \n",
    "        # Filter by minimum reviews\n",
    "        qualified = business_stats.filter(col(\"total_reviews\") >= min_reviews)\n",
    "        \n",
    "        # Get top candidates\n",
    "        top_candidates = qualified \\\n",
    "            .orderBy(desc(\"avg_review_stars\"), desc(\"total_reviews\")) \\\n",
    "            .limit(top_n * 5)\n",
    "        \n",
    "        # Broadcast join\n",
    "        result = top_candidates.join(\n",
    "            broadcast(business_df.select(\n",
    "                \"business_id\", \"name\", \"city\", \"state\", \"categories\", \"stars\"\n",
    "            )),\n",
    "            \"business_id\"\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"categories\",\n",
    "            \"total_reviews\",\n",
    "            \"avg_review_stars\",\n",
    "            \"total_useful\",\n",
    "            col(\"stars\").alias(\"business_avg_stars\")\n",
    "        ).orderBy(\n",
    "            desc(\"avg_review_stars\"), \n",
    "            desc(\"total_reviews\")\n",
    "        ).limit(top_n)\n",
    "        \n",
    "        result_count = result.count()\n",
    "        \n",
    "        # Cleanup\n",
    "        review_partitioned.unpersist()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} results\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_stores_by_positive_reviews(business_df, review_df, \n",
    "                                       positive_threshold=4, top_n=10):\n",
    "        \"\"\"\n",
    "        4. Cửa hàng nhận nhiều đánh giá tích cực nhất\n",
    "        \n",
    "        Optimizations:\n",
    "        - Single-pass aggregation with conditional logic\n",
    "        - Repartitioning and caching\n",
    "        - Early filtering\n",
    "        - Broadcast join\n",
    "        \n",
    "        Args:\n",
    "            business_df: DataFrame chứa dữ liệu business\n",
    "            review_df: DataFrame chứa dữ liệu review\n",
    "            positive_threshold: ngưỡng sao tích cực (default: 4)\n",
    "            top_n: số lượng top cửa hàng\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame với top cửa hàng có nhiều review tích cực nhất\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 4: Top {top_n} Stores by Positive Reviews (>= {positive_threshold} stars)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Repartition and cache\n",
    "        review_partitioned = review_df \\\n",
    "            .select(\"business_id\", \"review_id\", \"stars\", \"useful\") \\\n",
    "            .repartition(200, \"business_id\") \\\n",
    "            .cache()\n",
    "        \n",
    "        # Single-pass aggregation with conditional logic\n",
    "        review_stats = review_partitioned.groupBy(\"business_id\").agg(\n",
    "            # Count positive reviews\n",
    "            sum(when(col(\"stars\") >= positive_threshold, 1).otherwise(0))\n",
    "                .alias(\"positive_review_count\"),\n",
    "            \n",
    "            # Total review count\n",
    "            count(\"review_id\").alias(\"total_review_count\"),\n",
    "            \n",
    "            # Average stars of positive reviews\n",
    "            avg(when(col(\"stars\") >= positive_threshold, col(\"stars\")))\n",
    "                .alias(\"avg_positive_rating\"),\n",
    "            \n",
    "            # Total useful votes from positive reviews\n",
    "            sum(when(col(\"stars\") >= positive_threshold, col(\"useful\")).otherwise(0))\n",
    "                .alias(\"total_useful_votes\")\n",
    "        )\n",
    "        \n",
    "        # Calculate positive ratio and filter\n",
    "        review_stats_filtered = review_stats \\\n",
    "            .withColumn(\n",
    "                \"positive_ratio\", \n",
    "                col(\"positive_review_count\") / col(\"total_review_count\")\n",
    "            ) \\\n",
    "            .filter(col(\"positive_review_count\") > 0)\n",
    "        \n",
    "        # Get top candidates\n",
    "        top_candidates = review_stats_filtered \\\n",
    "            .orderBy(desc(\"positive_review_count\"), desc(\"positive_ratio\")) \\\n",
    "            .limit(top_n * 3)\n",
    "        \n",
    "        # Broadcast join\n",
    "        result = top_candidates.join(\n",
    "            broadcast(business_df.select(\n",
    "                \"business_id\", \"name\", \"city\", \"state\", \"categories\"\n",
    "            )),\n",
    "            \"business_id\"\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"categories\",\n",
    "            \"positive_review_count\",\n",
    "            \"total_review_count\",\n",
    "            \"positive_ratio\",\n",
    "            \"avg_positive_rating\",\n",
    "            \"total_useful_votes\"\n",
    "        ).orderBy(\n",
    "            desc(\"positive_review_count\"), \n",
    "            desc(\"positive_ratio\")\n",
    "        ).limit(top_n)\n",
    "        \n",
    "        result_count = result.count()\n",
    "        \n",
    "        # Cleanup\n",
    "        review_partitioned.unpersist()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} results\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "    # ================================================================\n",
    "    # 5.Phân tích thời gian cao điểm (review nhiều nhất)\n",
    "    # ================================================================\n",
    "    @staticmethod\n",
    "    def get_peak_hours(review_df):\n",
    "        \"\"\"\n",
    "        Phân tích số lượng review theo năm / tháng / giờ.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Analysis 2: Peak Review Hours (Activity Over Time)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Cột date có dạng \"yyyy-MM-dd HH:mm:ss\"\n",
    "        df = review_df.withColumn(\"date_parsed\", to_date(col(\"date\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "        result = (\n",
    "            df.groupBy(\n",
    "                year(\"date_parsed\").alias(\"year\"),\n",
    "                month(\"date_parsed\").alias(\"month\")\n",
    "            )\n",
    "            .agg(count(\"review_id\").alias(\"review_count\"))\n",
    "            .orderBy(desc(\"review_count\"))\n",
    "        )\n",
    "\n",
    "        result_count = result.count()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} time groups\")\n",
    "        return result\n",
    "\n",
    "    # ================================================================\n",
    "    # 6. Top danh mục (category) có nhiều review nhất\n",
    "    # ================================================================\n",
    "    @staticmethod\n",
    "    def get_top_categories(business_df, review_df, top_n=20):\n",
    "        \"\"\"\n",
    "        Phân tích top danh mục (category) bán chạy nhất - dựa trên số lượng review.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 3: Top {top_n} Categories by Review Count\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Tách categories thành từng dòng riêng\n",
    "        df_business = business_df.withColumn(\"category\", explode(split(col(\"categories\"), \",\\\\s*\")))\n",
    "\n",
    "        # Join review với business\n",
    "        joined = review_df.join(broadcast(df_business.select(\"business_id\", \"category\")), \"business_id\")\n",
    "\n",
    "        # Đếm số lượng review cho từng category\n",
    "        result = (\n",
    "            joined.groupBy(\"category\")\n",
    "            .agg(count(\"review_id\").alias(\"total_reviews\"))\n",
    "            .orderBy(desc(\"total_reviews\"))\n",
    "            .limit(top_n)\n",
    "        )\n",
    "\n",
    "        result_count = result.count()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} categories\")\n",
    "        return result\n",
    "\n",
    "    # ================================================================\n",
    "    # 7 Thống kê thông tin tất cả cửa hàng\n",
    "    # ================================================================\n",
    "    @staticmethod\n",
    "    def get_store_stats(business_df, review_df):\n",
    "        \"\"\"\n",
    "        Trả về thống kê tổng hợp của tất cả cửa hàng:\n",
    "        - Tên, danh mục, điểm sao trung bình, tổng số review thực tế,...\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Analysis 4: Store Statistics Summary\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Tính toán lại số lượng review và sao trung bình thực tế\n",
    "        review_stats = (\n",
    "            review_df.groupBy(\"business_id\")\n",
    "            .agg(\n",
    "                count(\"review_id\").alias(\"actual_review_count\"),\n",
    "                avg(\"stars\").alias(\"actual_avg_stars\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Gộp với thông tin cửa hàng\n",
    "        result = (\n",
    "            business_df.join(broadcast(review_stats), \"business_id\", \"left\")\n",
    "            .select(\n",
    "                \"business_id\",\n",
    "                \"name\",\n",
    "                \"city\",\n",
    "                \"state\",\n",
    "                \"categories\",\n",
    "                \"stars\",\n",
    "                \"review_count\",\n",
    "                \"actual_review_count\",\n",
    "                \"actual_avg_stars\"\n",
    "            )\n",
    "            .orderBy(\"business_id\")\n",
    "        )\n",
    "\n",
    "        result_count = result.count()\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} businesses\")\n",
    "        return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee9871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE ORCHESTRATION\n",
    "# ============================================================================\n",
    "\n",
    "class YelpAnalysisPipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline orchestrator\n",
    "    Production-ready with error handling, monitoring, and checkpointing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=\"../data/\", output_path=\"output/\"):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.spark = SparkConfig.create_spark_session()\n",
    "        self.data_loader = DataLoader(self.spark, data_path)\n",
    "        self.analytics = YelpAnalytics()\n",
    "        self.results = {}\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA LOADING PHASE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.business_df = self.data_loader.load_business_data()\n",
    "        self.review_df = self.data_loader.load_review_data()\n",
    "        # self.user_df = self.data_loader.load_user_data()  # Load if needed\n",
    "        \n",
    "        # Checkpoint business data (reused multiple times)\n",
    "        self.business_df.checkpoint()\n",
    "        \n",
    "        print(\"\\n✓ All data loaded successfully\")\n",
    "    \n",
    "    def run_analysis_1(self, days=90, top_n=10):\n",
    "        \"\"\"Run Analysis 1: Top Selling Products\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.top_selling_products_recent(\n",
    "                self.review_df, self.business_df, days=days, top_n=top_n\n",
    "            )\n",
    "            self.results['top_selling'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 1: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_analysis_2(self, top_n=10):\n",
    "        \"\"\"Run Analysis 2: Top Diverse Stores\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.top_stores_by_product_count(\n",
    "                self.business_df, top_n=top_n\n",
    "            )\n",
    "            self.results['diverse_stores'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 2: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_analysis_3(self, min_reviews=50, top_n=10):\n",
    "        \"\"\"Run Analysis 3: Top Rated Products\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.top_rated_products(\n",
    "                self.business_df, self.review_df, \n",
    "                min_reviews=min_reviews, top_n=top_n\n",
    "            )\n",
    "            self.results['best_rated'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 3: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_analysis_4(self, positive_threshold=4, top_n=10):\n",
    "        \"\"\"Run Analysis 4: Top Stores by Positive Reviews\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.top_stores_by_positive_reviews(\n",
    "                self.business_df, self.review_df,\n",
    "                positive_threshold=positive_threshold, top_n=top_n\n",
    "            )\n",
    "            self.results['most_positive'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 4: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_analysis_5(self):\n",
    "        \"\"\"Run Analysis 5: Review Activity Over Time (Peak Hours)\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.get_peak_hours(self.review_df)\n",
    "            self.results['peak_hours'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 5: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "   \n",
    "    def run_analysis_6(self, top_n=20):\n",
    "        \"\"\"Run Analysis 6: Top Business Categories by Review Count\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.get_top_categories(self.business_df, self.review_df, top_n=top_n)\n",
    "            self.results['top_categories'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 6: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def run_analysis_7(self):\n",
    "        \"\"\"Run Analysis 7: Overall Store Statistics Summary\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.get_store_stats(self.business_df, self.review_df)\n",
    "            self.results['store_stats'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 7: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    \n",
    "    def run_all_analyses(self, config=None):\n",
    "        \"\"\"\n",
    "        Run all analyses with custom configuration\n",
    "        \"\"\"\n",
    "        if config is None:\n",
    "            config = {\n",
    "                'analysis_1': {'days': 90, 'top_n': 10},\n",
    "                'analysis_2': {'top_n': 10},\n",
    "                'analysis_3': {'min_reviews': 50, 'top_n': 10},\n",
    "                'analysis_4': {'positive_threshold': 4, 'top_n': 10},\n",
    "                'analysis_6': {'top_n': 20}\n",
    "            }\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS PHASE - RUNNING ALL ANALYSES\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        total_start = time.time()\n",
    "\n",
    "        \n",
    "        self.run_analysis_1(**config['analysis_1'])\n",
    "        self.run_analysis_2(**config['analysis_2'])\n",
    "        self.run_analysis_3(**config['analysis_3'])\n",
    "        self.run_analysis_4(**config['analysis_4'])\n",
    "        self.run_analysis_5()\n",
    "        self.run_analysis_6(**config['analysis_6'])\n",
    "        self.run_analysis_7()\n",
    "\n",
    "        total_elapsed = time.time() - total_start\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"ALL ANALYSES COMPLETED in {total_elapsed:.2f}s\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    \n",
    "    def display_results(self):\n",
    "        \"\"\"Display all results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS PREVIEW\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for name, df in self.results.items():\n",
    "            print(f\"\\n{name.upper().replace('_', ' ')}:\")\n",
    "            print(\"-\" * 60)\n",
    "            df.show(truncate=False)\n",
    "    \n",
    "    def save_results(self, format='parquet', coalesce=True):\n",
    "        \"\"\"\n",
    "        Save results to disk\n",
    "        \n",
    "        Args:\n",
    "            format: output format ('parquet', 'csv', 'json')\n",
    "            coalesce: whether to coalesce to single file\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SAVING RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for name, df in self.results.items():\n",
    "            output_path = f\"{self.output_path}{name}\"\n",
    "            \n",
    "            try:\n",
    "                writer = df.coalesce(1) if coalesce else df\n",
    "                \n",
    "                if format == 'parquet':\n",
    "                    writer.write \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"compression\", \"snappy\") \\\n",
    "                        .parquet(output_path)\n",
    "                elif format == 'csv':\n",
    "                    writer.write \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .csv(output_path)\n",
    "                elif format == 'json':\n",
    "                    writer.write \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .json(output_path)\n",
    "                \n",
    "                print(f\"✓ Saved {name} to {output_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error saving {name}: {str(e)}\")\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nTotal Businesses: {self.business_df.count():,}\")\n",
    "        print(f\"Total Reviews: {self.review_df.count():,}\")\n",
    "        \n",
    "        for name, df in self.results.items():\n",
    "            print(f\"\\n{name.upper().replace('_', ' ')}:\")\n",
    "            print(f\"  Results: {df.count()}\")\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CLEANUP\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Unpersist cached data\n",
    "        self.business_df.unpersist()\n",
    "        \n",
    "        print(\"✓ Resources cleaned up\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop Spark session\"\"\"\n",
    "        self.spark.stop()\n",
    "        print(\"✓ Spark session stopped\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef410fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "                    YELP BIG DATA ANALYSIS SYSTEM\n",
      "                         Optimized for Large-Scale Processing\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'YelpAnalysisPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    126\u001b[39m         traceback.print_exc()\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;66;03m# Run full pipeline \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Initialize pipeline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m pipeline = \u001b[43mYelpAnalysisPipeline\u001b[49m(\n\u001b[32m     16\u001b[39m     data_path=\u001b[33m\"\u001b[39m\u001b[33m../data/\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     output_path=\u001b[33m\"\u001b[39m\u001b[33moutput/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Step 1: Load data\u001b[39;00m\n\u001b[32m     22\u001b[39m     pipeline.load_data()\n",
      "\u001b[31mNameError\u001b[39m: name 'YelpAnalysisPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 20 + \"YELP BIG DATA ANALYSIS SYSTEM\")\n",
    "    print(\" \" * 25 + \"Optimized for Large-Scale Processing\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = YelpAnalysisPipeline(\n",
    "        data_path=\"../data/\",\n",
    "        output_path=\"output/\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        pipeline.load_data()\n",
    "        \n",
    "        # Step 2: Run all analyses (1–7)\n",
    "        pipeline.run_all_analyses(config={\n",
    "            'analysis_1': {'days': 90, 'top_n': 10},\n",
    "            'analysis_2': {'top_n': 10},\n",
    "            'analysis_3': {'min_reviews': 50, 'top_n': 10},\n",
    "            'analysis_4': {'positive_threshold': 4, 'top_n': 10},\n",
    "            'analysis_6': {'top_n': 20}  # cho top category\n",
    "        })\n",
    "        \n",
    "        # Step 3: Display results\n",
    "        pipeline.display_results()\n",
    "        \n",
    "        # Step 4: Save results\n",
    "        pipeline.save_results(format='parquet', coalesce=True)\n",
    "        \n",
    "        # Step 5: Generate summary\n",
    "        pipeline.generate_summary_report()\n",
    "        \n",
    "        # Step 6: Cleanup\n",
    "        pipeline.cleanup()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" \" * 25 + \"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Pipeline failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # Always stop Spark\n",
    "        pipeline.stop()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_analysis(analysis_number, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a single analysis independently\n",
    "    \n",
    "    Args:\n",
    "        analysis_number: 1–7\n",
    "        **kwargs: parameters for the specific analysis\n",
    "    \"\"\"\n",
    "    pipeline = YelpAnalysisPipeline()\n",
    "    \n",
    "    try:\n",
    "        pipeline.load_data()\n",
    "        \n",
    "        if analysis_number == 1:\n",
    "            result = pipeline.run_analysis_1(**kwargs)\n",
    "        elif analysis_number == 2:\n",
    "            result = pipeline.run_analysis_2(**kwargs)\n",
    "        elif analysis_number == 3:\n",
    "            result = pipeline.run_analysis_3(**kwargs)\n",
    "        elif analysis_number == 4:\n",
    "            result = pipeline.run_analysis_4(**kwargs)\n",
    "        elif analysis_number == 5:\n",
    "            result = pipeline.run_analysis_5()\n",
    "        elif analysis_number == 6:\n",
    "            result = pipeline.run_analysis_6(**kwargs)\n",
    "        elif analysis_number == 7:\n",
    "            result = pipeline.run_analysis_7()\n",
    "        else:\n",
    "            raise ValueError(\"Analysis number must be between 1 and 7\")\n",
    "        \n",
    "        print(f\"\\n✓ Analysis {analysis_number} completed successfully!\")\n",
    "        result.show(truncate=False)\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error running analysis {analysis_number}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        pipeline.stop()\n",
    "\n",
    "\n",
    "def run_custom_analysis(business_df, review_df, analysis_func, **kwargs):\n",
    "    \"\"\"\n",
    "    Run custom analysis function (standalone)\n",
    "    \n",
    "    Args:\n",
    "        business_df: business DataFrame\n",
    "        review_df: review DataFrame\n",
    "        analysis_func: custom analysis function (e.g. YelpAnalytics.get_top_categories)\n",
    "        **kwargs: parameters for the analysis function\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"RUNNING CUSTOM ANALYSIS: {analysis_func.__name__}\")\n",
    "    print(\"=\"*60)\n",
    "    try:\n",
    "        result = analysis_func(business_df, review_df, **kwargs)\n",
    "        result.show(truncate=False)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error in custom analysis: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\": # Run full pipeline \n",
    "    main() \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
