{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c49953e",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell 1: Import và tạo Spark Session\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      4\u001b[39m spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTest BigData Kernel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal[*]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpark Version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark.version\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSpark Master: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark.sparkContext.master\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\bigdata\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\bigdata\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\bigdata\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\bigdata\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\bigdata\\Lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "# Cell 1: Import và tạo Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test BigData Kernel\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Cell 2: Test với dữ liệu mẫu\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineer\"),\n",
    "    (\"Bob\", 30, \"Manager\"),\n",
    "    (\"Charlie\", 35, \"Director\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\", \"Role\"])\n",
    "df.show()\n",
    "\n",
    "# Cell 3: Test aggregation\n",
    "df.groupBy(\"Role\").count().show()\n",
    "\n",
    "# Cell 4: Stop Spark\n",
    "spark.stop()\n",
    "print(\"✓ Spark test completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5165c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yelp Big Data Analysis System\n",
    "Optimized PySpark Pipeline for Large-Scale Data Processing\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, avg, sum, desc, to_date, current_date, lit, \n",
    "    expr, rand, when, year, month, unix_timestamp, size, \n",
    "    split, trim, length, broadcast\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, TimestampType, BooleanType\n",
    ")\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION & INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "class SparkConfig:\n",
    "    \"\"\"Spark configuration optimized for big data processing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_spark_session():\n",
    "        \"\"\"\n",
    "        Initialize Spark Session with optimized configurations\n",
    "        \"\"\"\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"Yelp Big Data Analysis System\") \\\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "            .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\") \\\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "            .config(\"spark.default.parallelism\", \"200\") \\\n",
    "            .config(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\") \\\n",
    "            .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "            .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "            .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "            .config(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\") \\\n",
    "            .config(\"spark.sql.inMemoryColumnarStorage.batchSize\", \"10000\") \\\n",
    "            .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
    "            .config(\"spark.sql.files.openCostInBytes\", \"4194304\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        spark.sparkContext.setLogLevel(\"WARN\")\n",
    "        spark.sparkContext.setCheckpointDir(\"checkpoints/\")\n",
    "        \n",
    "        return spark\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA SCHEMAS\n",
    "# ============================================================================\n",
    "\n",
    "class YelpSchemas:\n",
    "    \"\"\"Explicit schemas for better performance\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def business_schema():\n",
    "        return StructType([\n",
    "            StructField(\"business_id\", StringType(), False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"address\", StringType(), True),\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"latitude\", DoubleType(), True),\n",
    "            StructField(\"longitude\", DoubleType(), True),\n",
    "            StructField(\"stars\", DoubleType(), True),\n",
    "            StructField(\"review_count\", IntegerType(), True),\n",
    "            StructField(\"is_open\", IntegerType(), True),\n",
    "            StructField(\"categories\", StringType(), True),\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def review_schema():\n",
    "        return StructType([\n",
    "            StructField(\"review_id\", StringType(), False),\n",
    "            StructField(\"user_id\", StringType(), True),\n",
    "            StructField(\"business_id\", StringType(), True),\n",
    "            StructField(\"stars\", IntegerType(), True),\n",
    "            StructField(\"date\", StringType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"useful\", IntegerType(), True),\n",
    "        ])\n",
    "    \n",
    "    @staticmethod\n",
    "    def user_schema():\n",
    "        return StructType([\n",
    "            StructField(\"user_id\", StringType(), False),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"review_count\", IntegerType(), True),\n",
    "            StructField(\"yelping_since\", StringType(), True),\n",
    "            StructField(\"useful\", IntegerType(), True),\n",
    "            StructField(\"fans\", IntegerType(), True),\n",
    "            StructField(\"average_stars\", DoubleType(), True),\n",
    "        ])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DATA LOADING & PREPROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Handles data loading and preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, spark, data_path=\"data/\"):\n",
    "        self.spark = spark\n",
    "        self.data_path = data_path\n",
    "        self.schemas = YelpSchemas()\n",
    "    \n",
    "    def load_business_data(self):\n",
    "        \"\"\"Load and prepare business data\"\"\"\n",
    "        print(\"Loading business data...\")\n",
    "        \n",
    "        business_df = self.spark.read \\\n",
    "            .schema(self.schemas.business_schema()) \\\n",
    "            .json(f\"{self.data_path}business.json\")\n",
    "        \n",
    "        # Repartition và cache\n",
    "        business_df = business_df \\\n",
    "            .repartition(100, \"business_id\") \\\n",
    "            .cache()\n",
    "        \n",
    "        # Trigger cache\n",
    "        count = business_df.count()\n",
    "        print(f\"Loaded {count:,} businesses\")\n",
    "        \n",
    "        return business_df\n",
    "    \n",
    "    def load_review_data(self):\n",
    "        \"\"\"Load and prepare review data\"\"\"\n",
    "        print(\"Loading review data...\")\n",
    "        \n",
    "        review_df = self.spark.read \\\n",
    "            .schema(self.schemas.review_schema()) \\\n",
    "            .json(f\"{self.data_path}review.json\")\n",
    "        \n",
    "        # Preprocess dates\n",
    "        review_df = review_df \\\n",
    "            .withColumn(\"review_date\", to_date(col(\"date\"))) \\\n",
    "            .withColumn(\"review_timestamp\", unix_timestamp(col(\"date\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "            .withColumn(\"review_year\", year(col(\"date\"))) \\\n",
    "            .withColumn(\"review_month\", month(col(\"date\")))\n",
    "        \n",
    "        # Repartition by business_id for joins\n",
    "        review_df = review_df.repartition(200, \"business_id\")\n",
    "        \n",
    "        count = review_df.count()\n",
    "        print(f\"Loaded {count:,} reviews\")\n",
    "        \n",
    "        return review_df\n",
    "    \n",
    "    def load_user_data(self):\n",
    "        \"\"\"Load user data\"\"\"\n",
    "        print(\"Loading user data...\")\n",
    "        \n",
    "        user_df = self.spark.read \\\n",
    "            .schema(self.schemas.user_schema()) \\\n",
    "            .json(f\"{self.data_path}user.json\")\n",
    "        \n",
    "        count = user_df.count()\n",
    "        print(f\"Loaded {count:,} users\")\n",
    "        \n",
    "        return user_df\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS FUNCTIONS - OPTIMIZED FOR BIG DATA\n",
    "# ============================================================================\n",
    "\n",
    "class YelpAnalytics:\n",
    "    \"\"\"Core analytics functions optimized for big data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_selling_products_recent(review_df, business_df, days=90, top_n=10):\n",
    "        \"\"\"\n",
    "        1. Top sản phẩm (doanh nghiệp) bán chạy nhất trong khoảng thời gian gần\n",
    "        \n",
    "        Optimizations:\n",
    "        - Salting to handle data skew\n",
    "        - Two-stage aggregation\n",
    "        - Broadcast join for business info\n",
    "        - Early filtering and limiting\n",
    "        \n",
    "        Args:\n",
    "            review_df: DataFrame chứa dữ liệu review (đã preprocess dates)\n",
    "            business_df: DataFrame chứa dữ liệu business\n",
    "            days: số ngày gần đây cần phân tích\n",
    "            top_n: số lượng top sản phẩm\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame với top sản phẩm bán chạy nhất\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 1: Top {top_n} Selling Products (Last {days} days)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Add salt to handle skew\n",
    "        review_with_salt = review_df.withColumn(\"salt\", (rand() * 10).cast(\"int\"))\n",
    "        \n",
    "        # Filter by date range\n",
    "        cutoff_date = current_date() - lit(days)\n",
    "        recent_reviews = review_with_salt.filter(col(\"review_date\") >= cutoff_date)\n",
    "        \n",
    "        # Stage 1: Salted aggregation\n",
    "        salted_agg = recent_reviews.groupBy(\"business_id\", \"salt\").agg(\n",
    "            count(\"review_id\").alias(\"partial_count\"),\n",
    "            sum(\"stars\").alias(\"partial_sum_stars\"),\n",
    "            count(\"stars\").alias(\"partial_count_stars\")\n",
    "        )\n",
    "        \n",
    "        # Stage 2: Final aggregation\n",
    "        business_stats = salted_agg.groupBy(\"business_id\").agg(\n",
    "            sum(\"partial_count\").alias(\"recent_review_count\"),\n",
    "            (sum(\"partial_sum_stars\") / sum(\"partial_count_stars\")).alias(\"avg_rating\")\n",
    "        )\n",
    "        \n",
    "        # Get top candidates before join\n",
    "        top_candidates = business_stats \\\n",
    "            .orderBy(desc(\"recent_review_count\")) \\\n",
    "            .limit(top_n * 10)\n",
    "        \n",
    "        # Broadcast join with business info\n",
    "        result = top_candidates.join(\n",
    "            broadcast(business_df.select(\n",
    "                \"business_id\", \"name\", \"city\", \"state\", \"categories\"\n",
    "            )),\n",
    "            \"business_id\"\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"categories\",\n",
    "            \"recent_review_count\",\n",
    "            \"avg_rating\"\n",
    "        ).orderBy(desc(\"recent_review_count\")).limit(top_n)\n",
    "        \n",
    "        # Materialize result\n",
    "        result_count = result.count()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} results\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_stores_by_product_count(business_df, top_n=10):\n",
    "        \"\"\"\n",
    "        2. Cửa hàng bán nhiều sản phẩm nhất (dựa trên categories)\n",
    "        \n",
    "        Optimizations:\n",
    "        - Early null filtering\n",
    "        - Minimal column selection\n",
    "        - Efficient string processing\n",
    "        \n",
    "        Args:\n",
    "            business_df: DataFrame chứa dữ liệu business\n",
    "            top_n: số lượng top cửa hàng\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame với top cửa hàng đa dạng nhất\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 2: Top {top_n} Stores by Product Diversity\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Filter and select only needed columns\n",
    "        business_filtered = business_df \\\n",
    "            .filter(col(\"categories\").isNotNull()) \\\n",
    "            .filter(length(col(\"categories\")) > 0) \\\n",
    "            .select(\n",
    "                \"business_id\", \"name\", \"city\", \"state\", \n",
    "                \"categories\", \"review_count\", \"stars\"\n",
    "            )\n",
    "        \n",
    "        # Count categories\n",
    "        result = business_filtered.withColumn(\n",
    "            \"category_count\",\n",
    "            size(split(trim(col(\"categories\")), \"\\\\s*,\\\\s*\"))\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"categories\",\n",
    "            \"category_count\",\n",
    "            \"review_count\",\n",
    "            \"stars\"\n",
    "        ).orderBy(\n",
    "            desc(\"category_count\"), \n",
    "            desc(\"review_count\")\n",
    "        ).limit(top_n)\n",
    "        \n",
    "        result_count = result.count()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} results\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_rated_products(business_df, review_df, min_reviews=50, top_n=10):\n",
    "        \"\"\"\n",
    "        3. Sản phẩm (doanh nghiệp) đánh giá tích cực nhất\n",
    "        \n",
    "        Optimizations:\n",
    "        - Partitioning by business_id\n",
    "        - Strategic caching\n",
    "        - Early filtering by min_reviews\n",
    "        - Broadcast join\n",
    "        \n",
    "        Args:\n",
    "            business_df: DataFrame chứa dữ liệu business\n",
    "            review_df: DataFrame chứa dữ liệu review\n",
    "            min_reviews: số lượng review tối thiểu\n",
    "            top_n: số lượng top sản phẩm\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame với top sản phẩm có rating cao nhất\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 3: Top {top_n} Rated Products (Min {min_reviews} reviews)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Repartition and cache\n",
    "        review_partitioned = review_df \\\n",
    "            .select(\"business_id\", \"review_id\", \"stars\", \"useful\") \\\n",
    "            .repartition(200, \"business_id\") \\\n",
    "            .cache()\n",
    "        \n",
    "        # Aggregate review stats\n",
    "        business_stats = review_partitioned \\\n",
    "            .filter(col(\"stars\").isNotNull()) \\\n",
    "            .groupBy(\"business_id\") \\\n",
    "            .agg(\n",
    "                count(\"review_id\").alias(\"total_reviews\"),\n",
    "                avg(\"stars\").alias(\"avg_review_stars\"),\n",
    "                sum(\"useful\").alias(\"total_useful\")\n",
    "            )\n",
    "        \n",
    "        # Filter by minimum reviews\n",
    "        qualified = business_stats.filter(col(\"total_reviews\") >= min_reviews)\n",
    "        \n",
    "        # Get top candidates\n",
    "        top_candidates = qualified \\\n",
    "            .orderBy(desc(\"avg_review_stars\"), desc(\"total_reviews\")) \\\n",
    "            .limit(top_n * 5)\n",
    "        \n",
    "        # Broadcast join\n",
    "        result = top_candidates.join(\n",
    "            broadcast(business_df.select(\n",
    "                \"business_id\", \"name\", \"city\", \"state\", \"categories\", \"stars\"\n",
    "            )),\n",
    "            \"business_id\"\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"categories\",\n",
    "            \"total_reviews\",\n",
    "            \"avg_review_stars\",\n",
    "            \"total_useful\",\n",
    "            col(\"stars\").alias(\"business_avg_stars\")\n",
    "        ).orderBy(\n",
    "            desc(\"avg_review_stars\"), \n",
    "            desc(\"total_reviews\")\n",
    "        ).limit(top_n)\n",
    "        \n",
    "        result_count = result.count()\n",
    "        \n",
    "        # Cleanup\n",
    "        review_partitioned.unpersist()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} results\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_stores_by_positive_reviews(business_df, review_df, \n",
    "                                       positive_threshold=4, top_n=10):\n",
    "        \"\"\"\n",
    "        4. Cửa hàng nhận nhiều đánh giá tích cực nhất\n",
    "        \n",
    "        Optimizations:\n",
    "        - Single-pass aggregation with conditional logic\n",
    "        - Repartitioning and caching\n",
    "        - Early filtering\n",
    "        - Broadcast join\n",
    "        \n",
    "        Args:\n",
    "            business_df: DataFrame chứa dữ liệu business\n",
    "            review_df: DataFrame chứa dữ liệu review\n",
    "            positive_threshold: ngưỡng sao tích cực (default: 4)\n",
    "            top_n: số lượng top cửa hàng\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame với top cửa hàng có nhiều review tích cực nhất\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analysis 4: Top {top_n} Stores by Positive Reviews (>= {positive_threshold} stars)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Repartition and cache\n",
    "        review_partitioned = review_df \\\n",
    "            .select(\"business_id\", \"review_id\", \"stars\", \"useful\") \\\n",
    "            .repartition(200, \"business_id\") \\\n",
    "            .cache()\n",
    "        \n",
    "        # Single-pass aggregation with conditional logic\n",
    "        review_stats = review_partitioned.groupBy(\"business_id\").agg(\n",
    "            # Count positive reviews\n",
    "            sum(when(col(\"stars\") >= positive_threshold, 1).otherwise(0))\n",
    "                .alias(\"positive_review_count\"),\n",
    "            \n",
    "            # Total review count\n",
    "            count(\"review_id\").alias(\"total_review_count\"),\n",
    "            \n",
    "            # Average stars of positive reviews\n",
    "            avg(when(col(\"stars\") >= positive_threshold, col(\"stars\")))\n",
    "                .alias(\"avg_positive_rating\"),\n",
    "            \n",
    "            # Total useful votes from positive reviews\n",
    "            sum(when(col(\"stars\") >= positive_threshold, col(\"useful\")).otherwise(0))\n",
    "                .alias(\"total_useful_votes\")\n",
    "        )\n",
    "        \n",
    "        # Calculate positive ratio and filter\n",
    "        review_stats_filtered = review_stats \\\n",
    "            .withColumn(\n",
    "                \"positive_ratio\", \n",
    "                col(\"positive_review_count\") / col(\"total_review_count\")\n",
    "            ) \\\n",
    "            .filter(col(\"positive_review_count\") > 0)\n",
    "        \n",
    "        # Get top candidates\n",
    "        top_candidates = review_stats_filtered \\\n",
    "            .orderBy(desc(\"positive_review_count\"), desc(\"positive_ratio\")) \\\n",
    "            .limit(top_n * 3)\n",
    "        \n",
    "        # Broadcast join\n",
    "        result = top_candidates.join(\n",
    "            broadcast(business_df.select(\n",
    "                \"business_id\", \"name\", \"city\", \"state\", \"categories\"\n",
    "            )),\n",
    "            \"business_id\"\n",
    "        ).select(\n",
    "            \"business_id\",\n",
    "            \"name\",\n",
    "            \"city\",\n",
    "            \"state\",\n",
    "            \"categories\",\n",
    "            \"positive_review_count\",\n",
    "            \"total_review_count\",\n",
    "            \"positive_ratio\",\n",
    "            \"avg_positive_rating\",\n",
    "            \"total_useful_votes\"\n",
    "        ).orderBy(\n",
    "            desc(\"positive_review_count\"), \n",
    "            desc(\"positive_ratio\")\n",
    "        ).limit(top_n)\n",
    "        \n",
    "        result_count = result.count()\n",
    "        \n",
    "        # Cleanup\n",
    "        review_partitioned.unpersist()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ Completed in {elapsed:.2f}s - Found {result_count} results\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PIPELINE ORCHESTRATION\n",
    "# ============================================================================\n",
    "\n",
    "class YelpAnalysisPipeline:\n",
    "    \"\"\"\n",
    "    Main pipeline orchestrator\n",
    "    Production-ready with error handling, monitoring, and checkpointing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=\"data/\", output_path=\"output/\"):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.spark = SparkConfig.create_spark_session()\n",
    "        self.data_loader = DataLoader(self.spark, data_path)\n",
    "        self.analytics = YelpAnalytics()\n",
    "        self.results = {}\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATA LOADING PHASE\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.business_df = self.data_loader.load_business_data()\n",
    "        self.review_df = self.data_loader.load_review_data()\n",
    "        # self.user_df = self.data_loader.load_user_data()  # Load if needed\n",
    "        \n",
    "        # Checkpoint business data (reused multiple times)\n",
    "        self.business_df.checkpoint()\n",
    "        \n",
    "        print(\"\\n✓ All data loaded successfully\")\n",
    "    \n",
    "    def run_analysis_1(self, days=90, top_n=10):\n",
    "        \"\"\"Run Analysis 1: Top Selling Products\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.top_selling_products_recent(\n",
    "                self.review_df, self.business_df, days=days, top_n=top_n\n",
    "            )\n",
    "            self.results['top_selling'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 1: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_analysis_2(self, top_n=10):\n",
    "        \"\"\"Run Analysis 2: Top Diverse Stores\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.top_stores_by_product_count(\n",
    "                self.business_df, top_n=top_n\n",
    "            )\n",
    "            self.results['diverse_stores'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 2: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_analysis_3(self, min_reviews=50, top_n=10):\n",
    "        \"\"\"Run Analysis 3: Top Rated Products\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.top_rated_products(\n",
    "                self.business_df, self.review_df, \n",
    "                min_reviews=min_reviews, top_n=top_n\n",
    "            )\n",
    "            self.results['best_rated'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 3: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_analysis_4(self, positive_threshold=4, top_n=10):\n",
    "        \"\"\"Run Analysis 4: Top Stores by Positive Reviews\"\"\"\n",
    "        try:\n",
    "            result = self.analytics.top_stores_by_positive_reviews(\n",
    "                self.business_df, self.review_df,\n",
    "                positive_threshold=positive_threshold, top_n=top_n\n",
    "            )\n",
    "            self.results['most_positive'] = result\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error in Analysis 4: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_all_analyses(self, config=None):\n",
    "        \"\"\"\n",
    "        Run all analyses with custom configuration\n",
    "        \n",
    "        Args:\n",
    "            config: dict with parameters for each analysis\n",
    "        \"\"\"\n",
    "        if config is None:\n",
    "            config = {\n",
    "                'analysis_1': {'days': 90, 'top_n': 10},\n",
    "                'analysis_2': {'top_n': 10},\n",
    "                'analysis_3': {'min_reviews': 50, 'top_n': 10},\n",
    "                'analysis_4': {'positive_threshold': 4, 'top_n': 10}\n",
    "            }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ANALYSIS PHASE - RUNNING ALL ANALYSES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_start = time.time()\n",
    "        \n",
    "        # Run all analyses\n",
    "        self.run_analysis_1(**config['analysis_1'])\n",
    "        self.run_analysis_2(**config['analysis_2'])\n",
    "        self.run_analysis_3(**config['analysis_3'])\n",
    "        self.run_analysis_4(**config['analysis_4'])\n",
    "        \n",
    "        total_elapsed = time.time() - total_start\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"ALL ANALYSES COMPLETED in {total_elapsed:.2f}s\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def display_results(self):\n",
    "        \"\"\"Display all results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"RESULTS PREVIEW\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for name, df in self.results.items():\n",
    "            print(f\"\\n{name.upper().replace('_', ' ')}:\")\n",
    "            print(\"-\" * 60)\n",
    "            df.show(truncate=False)\n",
    "    \n",
    "    def save_results(self, format='parquet', coalesce=True):\n",
    "        \"\"\"\n",
    "        Save results to disk\n",
    "        \n",
    "        Args:\n",
    "            format: output format ('parquet', 'csv', 'json')\n",
    "            coalesce: whether to coalesce to single file\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SAVING RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for name, df in self.results.items():\n",
    "            output_path = f\"{self.output_path}{name}\"\n",
    "            \n",
    "            try:\n",
    "                writer = df.coalesce(1) if coalesce else df\n",
    "                \n",
    "                if format == 'parquet':\n",
    "                    writer.write \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"compression\", \"snappy\") \\\n",
    "                        .parquet(output_path)\n",
    "                elif format == 'csv':\n",
    "                    writer.write \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .csv(output_path)\n",
    "                elif format == 'json':\n",
    "                    writer.write \\\n",
    "                        .mode(\"overwrite\") \\\n",
    "                        .json(output_path)\n",
    "                \n",
    "                print(f\"✓ Saved {name} to {output_path}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error saving {name}: {str(e)}\")\n",
    "    \n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate summary statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nTotal Businesses: {self.business_df.count():,}\")\n",
    "        print(f\"Total Reviews: {self.review_df.count():,}\")\n",
    "        \n",
    "        for name, df in self.results.items():\n",
    "            print(f\"\\n{name.upper().replace('_', ' ')}:\")\n",
    "            print(f\"  Results: {df.count()}\")\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CLEANUP\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Unpersist cached data\n",
    "        self.business_df.unpersist()\n",
    "        \n",
    "        print(\"✓ Resources cleaned up\")\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop Spark session\"\"\"\n",
    "        self.spark.stop()\n",
    "        print(\"✓ Spark session stopped\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 20 + \"YELP BIG DATA ANALYSIS SYSTEM\")\n",
    "    print(\" \" * 25 + \"Optimized for Large-Scale Processing\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = YelpAnalysisPipeline(\n",
    "        data_path=\"data/\",\n",
    "        output_path=\"output/\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        pipeline.load_data()\n",
    "        \n",
    "        # Step 2: Run all analyses\n",
    "        pipeline.run_all_analyses(config={\n",
    "            'analysis_1': {'days': 90, 'top_n': 10},\n",
    "            'analysis_2': {'top_n': 10},\n",
    "            'analysis_3': {'min_reviews': 50, 'top_n': 10},\n",
    "            'analysis_4': {'positive_threshold': 4, 'top_n': 10}\n",
    "        })\n",
    "        \n",
    "        # Step 3: Display results\n",
    "        pipeline.display_results()\n",
    "        \n",
    "        # Step 4: Save results\n",
    "        pipeline.save_results(format='parquet', coalesce=True)\n",
    "        \n",
    "        # Step 5: Generate summary\n",
    "        pipeline.generate_summary_report()\n",
    "        \n",
    "        # Step 6: Cleanup\n",
    "        pipeline.cleanup()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" \" * 25 + \"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Pipeline failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        # Always stop Spark\n",
    "        pipeline.stop()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def run_single_analysis(analysis_number, **kwargs):\n",
    "    \"\"\"\n",
    "    Run a single analysis independently\n",
    "    \n",
    "    Args:\n",
    "        analysis_number: 1, 2, 3, or 4\n",
    "        **kwargs: parameters for the specific analysis\n",
    "    \"\"\"\n",
    "    pipeline = YelpAnalysisPipeline()\n",
    "    \n",
    "    try:\n",
    "        pipeline.load_data()\n",
    "        \n",
    "        if analysis_number == 1:\n",
    "            result = pipeline.run_analysis_1(**kwargs)\n",
    "        elif analysis_number == 2:\n",
    "            result = pipeline.run_analysis_2(**kwargs)\n",
    "        elif analysis_number == 3:\n",
    "            result = pipeline.run_analysis_3(**kwargs)\n",
    "        elif analysis_number == 4:\n",
    "            result = pipeline.run_analysis_4(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Analysis number must be 1, 2, 3, or 4\")\n",
    "        \n",
    "        result.show(truncate=False)\n",
    "        return result\n",
    "    \n",
    "    finally:\n",
    "        pipeline.stop()\n",
    "\n",
    "\n",
    "def run_custom_analysis(business_df, review_df, analysis_func, **kwargs):\n",
    "    \"\"\"\n",
    "    Run custom analysis function\n",
    "    \n",
    "    Args:\n",
    "        business_df: business DataFrame\n",
    "        review_df: review DataFrame\n",
    "        analysis_func: custom analysis function\n",
    "        **kwargs: parameters for the analysis function\n",
    "    \"\"\"\n",
    "    return analysis_func(business_df, review_df, **kwargs)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run full pipeline\n",
    "    main()\n",
    "    \n",
    "    # Or run single analysis:\n",
    "    # run_single_analysis(1, days=90, top_n=10)\n",
    "    # run_single_analysis(2, top_n=15)\n",
    "    # run_single_analysis(3, min_reviews=100, top_n=20)\n",
    "    # run_single_analysis(4, positive_threshold=4, top_n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
